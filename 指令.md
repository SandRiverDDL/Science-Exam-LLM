基于 设计.md为我完成项目，有以下要求：
1.使用config.yaml管理参数
2.目前只有 jinaai/jina-embeddings-v4-text-retrieval-GGUF和jinaai/jina-reranker-v3-GGUF 模型，其他embedding模型后续再添加，也就是说目前只有embedding只有一路
3.faiss使用IndexFlatIP + float16 存储
4.使用BM25辅助embedding找回，但不作为主力检索
5.不同检索方式实现模块化，因为我要通过实验比较不同检索方式的效果
6.我需要先检验Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP能否正常输出文字，然后用它做zero-shot分类，每次把问题和一个回答输入，只输出 0 或 1，0表示回答错误，1表示回答正确，注意KV cache，每次的问题是相同的，zero-shot作为base，跟后续实验做对比

构造一个流程，使得zero shot的qwen3模型，从train.csv读取问题和选项，像sample_submission.csv一样输出文件到output文件夹，同时拿预测结果和train.csv的answer做对比，并输出准确率，
这是单选题，
注意模块化，
prompt可以参考，你也可以改进，取决于你自己
You are a system that outputs ONLY one of A, B, C, D, E.
No explanation. No reasoning. Output must be exactly one letter.

Question:
{question}

Options:
A: ...
B: ...
C: ...
D: ...
E: ...

Answer (A/B/C/D/E):

set PYTHONPATH=%PYTHONPATH%;.\src
$env:PYTHONPATH="$env:PYTHONPATH;.\src"
$env:HUGGINGFACE_HUB_CACHE="D:\4000_projects\1project\LLM\data"

1. 加载训练集、验证集、测试集 CSV

   * 字段包括：question、options(A~E)、context 或背景文本（如果有）、correct_answer（训练集）

2.文本分块（chunking）
文本内容来自 data\raw\articles
对所有原始文档进行 chunk 处理：

chunk_size = 256 tokens
chunk_overlap = 50 tokens

操作方法：

使用 langchain.text_splitter.RecursiveCharacterTextSplitter 或等价自定义 splitter

分割每段文本为 token 级别的 chunks

每个 chunk 附带 metadata：

chunk_id

doc_id

position（chunk 的顺序）

3. 对所有 chunks 生成三种 embedding：

   * Jina-embeddings-v4 (text-retrieval)
   * BGE-m3 或 bge-large-v1.5
   * e5-mistral-instruct
   具体来说multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base，且这两者我已经下载好了，注意Jina-embeddings-v4是主力

4. 保存 embedding 到 FAISS 索引

   * 对每个 embedding 分别创建一个 IndexFlatIP (FP16)
   * 保存 id→chunk_text、id→metadata 映射


使用qwen3的tokenizer而不是你自己编写的，可以参考以下实现
---

def tokenize_chunks(text, tokenizer, chunk_size=256, overlap=50):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    n = len(tokens)
    chunks = []
    start = 0
    position = 0

    while start < n:
        end = min(start + chunk_size, n)
        chunk = tokens[start:end]
        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)

        chunks.append({
            "text": chunk_text,
            "position": position,
        })

        if end == n:
            break
        start = end - overlap
        position += 1

    return chunks

    对某个csv文件分块时，要打印 “正在分块文件：文件名”例如data\raw\articles\3.csv
    embedding部分加上e5 和bge multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base
    我的csv文件总大小为17G，你自己分析，我32G（大约25G）内存空闲是否够用，如果不够用可能需要一个文件一个文件处理

    暂时放弃jina（但不要删除相关代码），因为jina参数太大
    同时引入bge-m3模型作为辅助检索模型，现在的检索主力模型是[multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)
    ✔ 主力模型：multilingual-e5-large-instruct（最稳 → 决定你得分上限）
    ✔ 辅助模型：bge-m3（最佳互补 → 召回提升最大）
    ✔ 第三：gte-multilingual-base

我感觉非常慢，这正常吗，我决定先只启动gte去生成向量，快速验证，并检查以下问题：
你可能在用 CPU（GGUF）或单线程 tokenization → 会把预计速度砍到 1/10 或更低。

没有批量化（batch=1 或极小 batch） → 实际吞吐率急剧下降。

tokenizer 单线程或 Python-level decode/encode 成为瓶颈（尤其用 whitespace 切分或 slow tokenizer）。

I/O 磁盘读取慢（大量小文件） → CPU 等待 I/O。

没有把模型放到 GPU（或显存不足导致频繁 CPU-GPU 交换）。

框架/实现问题：例如用 sentence-transformers 但没有启用 device='cuda' 或没使用 torch.inference_mode()。

并行进程未启用（建议 tokenization 与 model inference 分离并行）


程序出bug，似乎是提示说超出了yield的limit，我没有报错错误报告

当程序 print 正在分块文件：data\raw\articles\1.csv\1.csv，到底在做什么，是在embedding吗？总不能分块分这么久吧
能不能实时显示embedding的进度，比如已经处理了多少个chunk，多少MB文件（估计）？

1.目前只用gte模型，除此之外，参考这个文档进行进行加速，因为每秒100chunks实在太慢了
2.为什么我跑了这么多我看不到embedding向量的输出？

1.[progress] flush=68 chunks=13056 bytes~11.00MB speed=265.5 chunks/s这个速度还是只有265chunks，而且我batchsize定多设定成192，再高就会超出显存，而且192显示的speed没有比64更快，而且我体感感觉64反而打印得更快，虽然显示的速度差不多
2.我希望这一段print控制打印频率，大概5秒打印一次，而且打印会覆盖上一次的打印，而不是每次都打印新的一行，例如这样
def print_and_overwrite(text):
    """
    打印文本，并用 \r 将光标移回行首，用于覆盖上次输出。
    """
    # 1. 打印文本
    # 2. 使用 end='\r' 来代替默认的 '\n'
    # 3. 如果需要清除残留，可以先打印足够多的空格
    print(f"\r{text}", end="")

    [progress] flush=50 chunks=9600 bytes~12.98MB speed=168.3 chunks/sTraceback (most recent call last):
    既然瓶颈在CPU侧，那能否离线将文档全转化为token，然后再输入embedding模型？
    我增大了一倍chunk_size_tokens，但目前的speed*2和之前差不多 265.5 chunks/s 

    [progress] flush=70 chunks=13440 bytes~18.60MB speed=166.0 chunks/s 怎么没有什么区别？


    multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base BGE-m3

我决定新增BAAI/bge-small-en-v1.5模型以快速生成embedding，你自己决定是在bge-m3基础上新增还是直接新增一个索引


问题1：le "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 325, in main   
    result = build_indices_streaming(
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 252, in build_indices_streaming
    flush()
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 209, in flush  
    if use_token_ids and buf_ids and hasattr(bge_small_emb, "embed_from_ids"):
UnboundLocalError: local variable 'buf_ids' referenced before assignment
问题2：CSV迭代异常 data/raw/articles\4.csv\4.csv: field larger than field limit (131072)

我觉得bge small也不小，所以切换到bge-m3模型了，但
1.怎么这么慢，才50chunks每秒
2.Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192)这个错误怎么来的？
Loaded text docs: 0 from data/raw/articles
Found CSV files: 38 from data/raw/articles
`torch_dtype` is deprecated! Use `dtype` instead!
[init] BGE device: cuda
[tokenize] 使用 BGE 的 tokenizer 进行分块（直通 token ids）
正在分块CSV：data\raw\articles\0.csv\0.csv
Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors
[progress] flush=3 chunks=384 bytes~0.08MB speed=51.4 chunks/sTraceback

1.我准备就长期使用bge-small了，还有什么加速embedding的方法吗？
2.我希望实现每隔多少个chunk就生成一次索引（大概十分钟一次？），并设置检查点，支持中断继续

参考以下，本项目可以用来加速embedding吗
SentenceTransformers + FastTokenizer + multi-worker dataloader
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-small-en-v1.5", 
                            device="cuda",
                            trust_remote_code=True)

embeddings = model.encode(
    docs,
    batch_size=4096,          # 发疯一样的大 batch
    device="cuda",
    show_progress_bar=True,
    normalize_embeddings=True,
    convert_to_numpy=True,
    num_workers=8             # tokenizer 多线程

)

我添加了一个新的embedding模型Qwen/Qwen3-Embedding-0.6B, 替我加入代码
我想知道是否可以用它来加速embedding

为什么qwen3 0.6b这么慢，才[progress] flush=3 chunks=384 bytes~0.08MB speed=17.6 chunks/s
是不是因为什么bug？
是不是没有启动flashattention

- 确保在 GPU + FP16：
- embedding_qwen3.device: "cuda"
- embedding_qwen3.dtype: float16
- 开启直通 token-id（避免重复分词）：
- pipeline.embed_from_tokens: true
- pipeline.embed_tokens_model: qwen3
- 看日志应出现： [tokenize] 使用 Qwen3 的 tokenizer 进行分块（直通 token ids）
- 平衡序列长度与分块大小：
- 将 embedding_qwen3.max_length: 256 （或略高于你的 chunk_size_tokens ，例如 256/320），避免 512 的无谓填充。
- 调整批量：
- 起步用 pipeline.batch_size: 256 或 512 ，有 16–24GB 显存可试 1024 ；OOM 就回退。
- 让 PyTorch 更快：
- CUDA 上默认已用 AMP（自动混精）。可额外允许 TF32：在入口脚本加 torch.backends.cuda.matmul.allow_tf32 = True （FP16 已足够的话，这项影响不大）。
- 如果只想“更快更小”：
- 直接用 bge-small-en-v1.5 在 GPU，吞吐通常比 Qwen3 0.6B 更高。

use_fast

University of California-Los Angeles (Los Angeles, CA)
Harry
Smith
2004 7 7

目前我使用bge-small的embedding速度大概为600 chunks每秒，要embedding 17G的数据需要十几小时，帮我分析以下我的程序的速度瓶颈在哪里，可以采取哪些措施