åŸºäº è®¾è®¡.mdä¸ºæˆ‘å®Œæˆé¡¹ç›®ï¼Œæœ‰ä»¥ä¸‹è¦æ±‚ï¼š
1.ä½¿ç”¨config.yamlç®¡ç†å‚æ•°
2.ç›®å‰åªæœ‰ jinaai/jina-embeddings-v4-text-retrieval-GGUFå’Œjinaai/jina-reranker-v3-GGUF æ¨¡å‹ï¼Œå…¶ä»–embeddingæ¨¡å‹åç»­å†æ·»åŠ ï¼Œä¹Ÿå°±æ˜¯è¯´ç›®å‰åªæœ‰embeddingåªæœ‰ä¸€è·¯
3.faissä½¿ç”¨IndexFlatIP + float16 å­˜å‚¨
4.ä½¿ç”¨BM25è¾…åŠ©embeddingæ‰¾å›ï¼Œä½†ä¸ä½œä¸ºä¸»åŠ›æ£€ç´¢
5.ä¸åŒæ£€ç´¢æ–¹å¼å®ç°æ¨¡å—åŒ–ï¼Œå› ä¸ºæˆ‘è¦é€šè¿‡å®éªŒæ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹å¼çš„æ•ˆæœ
6.æˆ‘éœ€è¦å…ˆæ£€éªŒQwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMPèƒ½å¦æ­£å¸¸è¾“å‡ºæ–‡å­—ï¼Œç„¶åç”¨å®ƒåšzero-shotåˆ†ç±»ï¼Œæ¯æ¬¡æŠŠé—®é¢˜å’Œä¸€ä¸ªå›ç­”è¾“å…¥ï¼Œåªè¾“å‡º 0 æˆ– 1ï¼Œ0è¡¨ç¤ºå›ç­”é”™è¯¯ï¼Œ1è¡¨ç¤ºå›ç­”æ­£ç¡®ï¼Œæ³¨æ„KV cacheï¼Œæ¯æ¬¡çš„é—®é¢˜æ˜¯ç›¸åŒçš„ï¼Œzero-shotä½œä¸ºbaseï¼Œè·Ÿåç»­å®éªŒåšå¯¹æ¯”

æ„é€ ä¸€ä¸ªæµç¨‹ï¼Œä½¿å¾—zero shotçš„qwen3æ¨¡å‹ï¼Œä»train.csvè¯»å–é—®é¢˜å’Œé€‰é¡¹ï¼Œåƒsample_submission.csvä¸€æ ·è¾“å‡ºæ–‡ä»¶åˆ°outputæ–‡ä»¶å¤¹ï¼ŒåŒæ—¶æ‹¿é¢„æµ‹ç»“æœå’Œtrain.csvçš„answeråšå¯¹æ¯”ï¼Œå¹¶è¾“å‡ºå‡†ç¡®ç‡ï¼Œ
è¿™æ˜¯å•é€‰é¢˜ï¼Œ
æ³¨æ„æ¨¡å—åŒ–ï¼Œ
promptå¯ä»¥å‚è€ƒï¼Œä½ ä¹Ÿå¯ä»¥æ”¹è¿›ï¼Œå–å†³äºä½ è‡ªå·±
You are a system that outputs ONLY one of A, B, C, D, E.
No explanation. No reasoning. Output must be exactly one letter.

Question:
{question}

Options:
A: ...
B: ...
C: ...
D: ...
E: ...

Answer (A/B/C/D/E):

set PYTHONPATH=%PYTHONPATH%;.\src
$env:PYTHONPATH="$env:PYTHONPATH;.\src"
$env:HUGGINGFACE_HUB_CACHE="D:\4000_projects\1project\LLM\data"

1. åŠ è½½è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›† CSV

   * å­—æ®µåŒ…æ‹¬ï¼šquestionã€options(A~E)ã€context æˆ–èƒŒæ™¯æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰ã€correct_answerï¼ˆè®­ç»ƒé›†ï¼‰

2.æ–‡æœ¬åˆ†å—ï¼ˆchunkingï¼‰
æ–‡æœ¬å†…å®¹æ¥è‡ª data\raw\articles
å¯¹æ‰€æœ‰åŸå§‹æ–‡æ¡£è¿›è¡Œ chunk å¤„ç†ï¼š

chunk_size = 256 tokens
chunk_overlap = 50 tokens

æ“ä½œæ–¹æ³•ï¼š

ä½¿ç”¨ langchain.text_splitter.RecursiveCharacterTextSplitter æˆ–ç­‰ä»·è‡ªå®šä¹‰ splitter

åˆ†å‰²æ¯æ®µæ–‡æœ¬ä¸º token çº§åˆ«çš„ chunks

æ¯ä¸ª chunk é™„å¸¦ metadataï¼š

chunk_id

doc_id

positionï¼ˆchunk çš„é¡ºåºï¼‰

3. å¯¹æ‰€æœ‰ chunks ç”Ÿæˆä¸‰ç§ embeddingï¼š

   * Jina-embeddings-v4 (text-retrieval)
   * BGE-m3 æˆ– bge-large-v1.5
   * e5-mistral-instruct
   å…·ä½“æ¥è¯´multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-baseï¼Œä¸”è¿™ä¸¤è€…æˆ‘å·²ç»ä¸‹è½½å¥½äº†ï¼Œæ³¨æ„Jina-embeddings-v4æ˜¯ä¸»åŠ›

4. ä¿å­˜ embedding åˆ° FAISS ç´¢å¼•

   * å¯¹æ¯ä¸ª embedding åˆ†åˆ«åˆ›å»ºä¸€ä¸ª IndexFlatIP (FP16)
   * ä¿å­˜ idâ†’chunk_textã€idâ†’metadata æ˜ å°„


ä½¿ç”¨qwen3çš„tokenizerè€Œä¸æ˜¯ä½ è‡ªå·±ç¼–å†™çš„ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹å®ç°
---

def tokenize_chunks(text, tokenizer, chunk_size=256, overlap=50):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    n = len(tokens)
    chunks = []
    start = 0
    position = 0

    while start < n:
        end = min(start + chunk_size, n)
        chunk = tokens[start:end]
        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)

        chunks.append({
            "text": chunk_text,
            "position": position,
        })

        if end == n:
            break
        start = end - overlap
        position += 1

    return chunks

    å¯¹æŸä¸ªcsvæ–‡ä»¶åˆ†å—æ—¶ï¼Œè¦æ‰“å° â€œæ­£åœ¨åˆ†å—æ–‡ä»¶ï¼šæ–‡ä»¶åâ€ä¾‹å¦‚data\raw\articles\3.csv
    embeddingéƒ¨åˆ†åŠ ä¸Še5 å’Œbge multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base
    æˆ‘çš„csvæ–‡ä»¶æ€»å¤§å°ä¸º17Gï¼Œä½ è‡ªå·±åˆ†æï¼Œæˆ‘32Gï¼ˆå¤§çº¦25Gï¼‰å†…å­˜ç©ºé—²æ˜¯å¦å¤Ÿç”¨ï¼Œå¦‚æœä¸å¤Ÿç”¨å¯èƒ½éœ€è¦ä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å¤„ç†

    æš‚æ—¶æ”¾å¼ƒjinaï¼ˆä½†ä¸è¦åˆ é™¤ç›¸å…³ä»£ç ï¼‰ï¼Œå› ä¸ºjinaå‚æ•°å¤ªå¤§
    åŒæ—¶å¼•å…¥bge-m3æ¨¡å‹ä½œä¸ºè¾…åŠ©æ£€ç´¢æ¨¡å‹ï¼Œç°åœ¨çš„æ£€ç´¢ä¸»åŠ›æ¨¡å‹æ˜¯[multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)
    âœ” ä¸»åŠ›æ¨¡å‹ï¼šmultilingual-e5-large-instructï¼ˆæœ€ç¨³ â†’ å†³å®šä½ å¾—åˆ†ä¸Šé™ï¼‰
    âœ” è¾…åŠ©æ¨¡å‹ï¼šbge-m3ï¼ˆæœ€ä½³äº’è¡¥ â†’ å¬å›æå‡æœ€å¤§ï¼‰
    âœ” ç¬¬ä¸‰ï¼šgte-multilingual-base

æˆ‘æ„Ÿè§‰éå¸¸æ…¢ï¼Œè¿™æ­£å¸¸å—ï¼Œæˆ‘å†³å®šå…ˆåªå¯åŠ¨gteå»ç”Ÿæˆå‘é‡ï¼Œå¿«é€ŸéªŒè¯ï¼Œå¹¶æ£€æŸ¥ä»¥ä¸‹é—®é¢˜ï¼š
ä½ å¯èƒ½åœ¨ç”¨ CPUï¼ˆGGUFï¼‰æˆ–å•çº¿ç¨‹ tokenization â†’ ä¼šæŠŠé¢„è®¡é€Ÿåº¦ç åˆ° 1/10 æˆ–æ›´ä½ã€‚

æ²¡æœ‰æ‰¹é‡åŒ–ï¼ˆbatch=1 æˆ–æå° batchï¼‰ â†’ å®é™…ååç‡æ€¥å‰§ä¸‹é™ã€‚

tokenizer å•çº¿ç¨‹æˆ– Python-level decode/encode æˆä¸ºç“¶é¢ˆï¼ˆå°¤å…¶ç”¨ whitespace åˆ‡åˆ†æˆ– slow tokenizerï¼‰ã€‚

I/O ç£ç›˜è¯»å–æ…¢ï¼ˆå¤§é‡å°æ–‡ä»¶ï¼‰ â†’ CPU ç­‰å¾… I/Oã€‚

æ²¡æœ‰æŠŠæ¨¡å‹æ”¾åˆ° GPUï¼ˆæˆ–æ˜¾å­˜ä¸è¶³å¯¼è‡´é¢‘ç¹ CPU-GPU äº¤æ¢ï¼‰ã€‚

æ¡†æ¶/å®ç°é—®é¢˜ï¼šä¾‹å¦‚ç”¨ sentence-transformers ä½†æ²¡æœ‰å¯ç”¨ device='cuda' æˆ–æ²¡ä½¿ç”¨ torch.inference_mode()ã€‚

å¹¶è¡Œè¿›ç¨‹æœªå¯ç”¨ï¼ˆå»ºè®® tokenization ä¸ model inference åˆ†ç¦»å¹¶è¡Œï¼‰


ç¨‹åºå‡ºbugï¼Œä¼¼ä¹æ˜¯æç¤ºè¯´è¶…å‡ºäº†yieldçš„limitï¼Œæˆ‘æ²¡æœ‰æŠ¥é”™é”™è¯¯æŠ¥å‘Š

å½“ç¨‹åº print æ­£åœ¨åˆ†å—æ–‡ä»¶ï¼šdata\raw\articles\1.csv\1.csvï¼Œåˆ°åº•åœ¨åšä»€ä¹ˆï¼Œæ˜¯åœ¨embeddingå—ï¼Ÿæ€»ä¸èƒ½åˆ†å—åˆ†è¿™ä¹ˆä¹…å§
èƒ½ä¸èƒ½å®æ—¶æ˜¾ç¤ºembeddingçš„è¿›åº¦ï¼Œæ¯”å¦‚å·²ç»å¤„ç†äº†å¤šå°‘ä¸ªchunkï¼Œå¤šå°‘MBæ–‡ä»¶ï¼ˆä¼°è®¡ï¼‰ï¼Ÿ

1.ç›®å‰åªç”¨gteæ¨¡å‹ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œå‚è€ƒè¿™ä¸ªæ–‡æ¡£è¿›è¡Œè¿›è¡ŒåŠ é€Ÿï¼Œå› ä¸ºæ¯ç§’100chunkså®åœ¨å¤ªæ…¢äº†
2.ä¸ºä»€ä¹ˆæˆ‘è·‘äº†è¿™ä¹ˆå¤šæˆ‘çœ‹ä¸åˆ°embeddingå‘é‡çš„è¾“å‡ºï¼Ÿ

1.[progress] flush=68 chunks=13056 bytes~11.00MB speed=265.5 chunks/sè¿™ä¸ªé€Ÿåº¦è¿˜æ˜¯åªæœ‰265chunksï¼Œè€Œä¸”æˆ‘batchsizeå®šå¤šè®¾å®šæˆ192ï¼Œå†é«˜å°±ä¼šè¶…å‡ºæ˜¾å­˜ï¼Œè€Œä¸”192æ˜¾ç¤ºçš„speedæ²¡æœ‰æ¯”64æ›´å¿«ï¼Œè€Œä¸”æˆ‘ä½“æ„Ÿæ„Ÿè§‰64åè€Œæ‰“å°å¾—æ›´å¿«ï¼Œè™½ç„¶æ˜¾ç¤ºçš„é€Ÿåº¦å·®ä¸å¤š
2.æˆ‘å¸Œæœ›è¿™ä¸€æ®µprintæ§åˆ¶æ‰“å°é¢‘ç‡ï¼Œå¤§æ¦‚5ç§’æ‰“å°ä¸€æ¬¡ï¼Œè€Œä¸”æ‰“å°ä¼šè¦†ç›–ä¸Šä¸€æ¬¡çš„æ‰“å°ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½æ‰“å°æ–°çš„ä¸€è¡Œï¼Œä¾‹å¦‚è¿™æ ·
def print_and_overwrite(text):
    """
    æ‰“å°æ–‡æœ¬ï¼Œå¹¶ç”¨ \r å°†å…‰æ ‡ç§»å›è¡Œé¦–ï¼Œç”¨äºè¦†ç›–ä¸Šæ¬¡è¾“å‡ºã€‚
    """
    # 1. æ‰“å°æ–‡æœ¬
    # 2. ä½¿ç”¨ end='\r' æ¥ä»£æ›¿é»˜è®¤çš„ '\n'
    # 3. å¦‚æœéœ€è¦æ¸…é™¤æ®‹ç•™ï¼Œå¯ä»¥å…ˆæ‰“å°è¶³å¤Ÿå¤šçš„ç©ºæ ¼
    print(f"\r{text}", end="")

    [progress] flush=50 chunks=9600 bytes~12.98MB speed=168.3 chunks/sTraceback (most recent call last):
    æ—¢ç„¶ç“¶é¢ˆåœ¨CPUä¾§ï¼Œé‚£èƒ½å¦ç¦»çº¿å°†æ–‡æ¡£å…¨è½¬åŒ–ä¸ºtokenï¼Œç„¶åå†è¾“å…¥embeddingæ¨¡å‹ï¼Ÿ
    æˆ‘å¢å¤§äº†ä¸€å€chunk_size_tokensï¼Œä½†ç›®å‰çš„speed*2å’Œä¹‹å‰å·®ä¸å¤š 265.5 chunks/s 

    [progress] flush=70 chunks=13440 bytes~18.60MB speed=166.0 chunks/s æ€ä¹ˆæ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ


    multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base BGE-m3

æˆ‘å†³å®šæ–°å¢BAAI/bge-small-en-v1.5æ¨¡å‹ä»¥å¿«é€Ÿç”Ÿæˆembeddingï¼Œä½ è‡ªå·±å†³å®šæ˜¯åœ¨bge-m3åŸºç¡€ä¸Šæ–°å¢è¿˜æ˜¯ç›´æ¥æ–°å¢ä¸€ä¸ªç´¢å¼•


é—®é¢˜1ï¼šle "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 325, in main   
    result = build_indices_streaming(
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 252, in build_indices_streaming
    flush()
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 209, in flush  
    if use_token_ids and buf_ids and hasattr(bge_small_emb, "embed_from_ids"):
UnboundLocalError: local variable 'buf_ids' referenced before assignment
é—®é¢˜2ï¼šCSVè¿­ä»£å¼‚å¸¸ data/raw/articles\4.csv\4.csv: field larger than field limit (131072)

æˆ‘è§‰å¾—bge smallä¹Ÿä¸å°ï¼Œæ‰€ä»¥åˆ‡æ¢åˆ°bge-m3æ¨¡å‹äº†ï¼Œä½†
1.æ€ä¹ˆè¿™ä¹ˆæ…¢ï¼Œæ‰50chunksæ¯ç§’
2.Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192)è¿™ä¸ªé”™è¯¯æ€ä¹ˆæ¥çš„ï¼Ÿ
Loaded text docs: 0 from data/raw/articles
Found CSV files: 38 from data/raw/articles
`torch_dtype` is deprecated! Use `dtype` instead!
[init] BGE device: cuda
[tokenize] ä½¿ç”¨ BGE çš„ tokenizer è¿›è¡Œåˆ†å—ï¼ˆç›´é€š token idsï¼‰
æ­£åœ¨åˆ†å—CSVï¼šdata\raw\articles\0.csv\0.csv
Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors
[progress] flush=3 chunks=384 bytes~0.08MB speed=51.4 chunks/sTraceback

1.æˆ‘å‡†å¤‡å°±é•¿æœŸä½¿ç”¨bge-smalläº†ï¼Œè¿˜æœ‰ä»€ä¹ˆåŠ é€Ÿembeddingçš„æ–¹æ³•å—ï¼Ÿ
2.æˆ‘å¸Œæœ›å®ç°æ¯éš”å¤šå°‘ä¸ªchunkå°±ç”Ÿæˆä¸€æ¬¡ç´¢å¼•ï¼ˆå¤§æ¦‚ååˆ†é’Ÿä¸€æ¬¡ï¼Ÿï¼‰ï¼Œå¹¶è®¾ç½®æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒä¸­æ–­ç»§ç»­

å‚è€ƒä»¥ä¸‹ï¼Œæœ¬é¡¹ç›®å¯ä»¥ç”¨æ¥åŠ é€Ÿembeddingå—
SentenceTransformers + FastTokenizer + multi-worker dataloader
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-small-en-v1.5", 
                            device="cuda",
                            trust_remote_code=True)

embeddings = model.encode(
    docs,
    batch_size=4096,          # å‘ç–¯ä¸€æ ·çš„å¤§ batch
    device="cuda",
    show_progress_bar=True,
    normalize_embeddings=True,
    convert_to_numpy=True,
    num_workers=8             # tokenizer å¤šçº¿ç¨‹

)

æˆ‘æ·»åŠ äº†ä¸€ä¸ªæ–°çš„embeddingæ¨¡å‹Qwen/Qwen3-Embedding-0.6B, æ›¿æˆ‘åŠ å…¥ä»£ç 
æˆ‘æƒ³çŸ¥é“æ˜¯å¦å¯ä»¥ç”¨å®ƒæ¥åŠ é€Ÿembedding

ä¸ºä»€ä¹ˆqwen3 0.6bè¿™ä¹ˆæ…¢ï¼Œæ‰[progress] flush=3 chunks=384 bytes~0.08MB speed=17.6 chunks/s
æ˜¯ä¸æ˜¯å› ä¸ºä»€ä¹ˆbugï¼Ÿ
æ˜¯ä¸æ˜¯æ²¡æœ‰å¯åŠ¨flashattention

- ç¡®ä¿åœ¨ GPU + FP16ï¼š
- embedding_qwen3.device: "cuda"
- embedding_qwen3.dtype: float16
- å¼€å¯ç›´é€š token-idï¼ˆé¿å…é‡å¤åˆ†è¯ï¼‰ï¼š
- pipeline.embed_from_tokens: true
- pipeline.embed_tokens_model: qwen3
- çœ‹æ—¥å¿—åº”å‡ºç°ï¼š [tokenize] ä½¿ç”¨ Qwen3 çš„ tokenizer è¿›è¡Œåˆ†å—ï¼ˆç›´é€š token idsï¼‰
- å¹³è¡¡åºåˆ—é•¿åº¦ä¸åˆ†å—å¤§å°ï¼š
- å°† embedding_qwen3.max_length: 256 ï¼ˆæˆ–ç•¥é«˜äºä½ çš„ chunk_size_tokens ï¼Œä¾‹å¦‚ 256/320ï¼‰ï¼Œé¿å… 512 çš„æ— è°“å¡«å……ã€‚
- è°ƒæ•´æ‰¹é‡ï¼š
- èµ·æ­¥ç”¨ pipeline.batch_size: 256 æˆ– 512 ï¼Œæœ‰ 16â€“24GB æ˜¾å­˜å¯è¯• 1024 ï¼›OOM å°±å›é€€ã€‚
- è®© PyTorch æ›´å¿«ï¼š
- CUDA ä¸Šé»˜è®¤å·²ç”¨ AMPï¼ˆè‡ªåŠ¨æ··ç²¾ï¼‰ã€‚å¯é¢å¤–å…è®¸ TF32ï¼šåœ¨å…¥å£è„šæœ¬åŠ  torch.backends.cuda.matmul.allow_tf32 = True ï¼ˆFP16 å·²è¶³å¤Ÿçš„è¯ï¼Œè¿™é¡¹å½±å“ä¸å¤§ï¼‰ã€‚
- å¦‚æœåªæƒ³â€œæ›´å¿«æ›´å°â€ï¼š
- ç›´æ¥ç”¨ bge-small-en-v1.5 åœ¨ GPUï¼Œååé€šå¸¸æ¯” Qwen3 0.6B æ›´é«˜ã€‚

use_fast

University of California-Los Angeles (Los Angeles, CA)
Harry
Smith
2004 7 7

ç›®å‰æˆ‘ä½¿ç”¨bge-smallçš„embeddingé€Ÿåº¦å¤§æ¦‚ä¸º600 chunksæ¯ç§’ï¼Œè¦embedding 17Gçš„æ•°æ®éœ€è¦åå‡ å°æ—¶ï¼Œå¸®æˆ‘åˆ†æä»¥ä¸‹æˆ‘çš„ç¨‹åºçš„é€Ÿåº¦ç“¶é¢ˆåœ¨å“ªé‡Œï¼Œå¯ä»¥é‡‡å–å“ªäº›æªæ–½

å®æ–½è¿™äº›ä¼˜åŒ–,ä½†éœ€è¦æ³¨æ„æˆ‘æ˜¯windowsç³»ç»Ÿï¼Œä½¿ç”¨å¤šè¿›ç¨‹è§£ç å¯èƒ½ä¼šå¡ä¸»ï¼Œ
å¦å¤–åœ¨build_chunks_and_indices.pyä¸­å¯ä»¥å°è¯•é‡æ„ï¼Œè¿™ä¸ªæ–‡ä»¶å¤ªå¤§äº†

1.build_chunks_and_indices.pyçš„æ–‡ä»¶è¿˜æ˜¯å¤ªå¤§äº†å¯èƒ½éœ€è¦é‡æ„
2.[progress] flush=22 chunks=45056 bytes~52.19MB speed=854.2 chunks/s emb=51% io=2%
è¿™ä¸ªé€Ÿåº¦å’Œæ—¶é—´å æ¯”æ€ä¹ˆæ ·ï¼Ÿ emb + ioåŠ èµ·æ¥ä¹Ÿæ‰53%
3.ä½ ç¡®å®šembedding_builder.pyå’Œindex_manager.pyé€‚åˆæ”¾åœ¨pipelineç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯ indexæˆ–è€…retrievelç›®å½•ä¸‹
4.æˆ‘å‡†å¤‡æš‚æ—¶æ”¹æˆä¸€åˆ†é’Ÿä¿å­˜ä¸€æ¬¡ï¼Œç„¶åï¼ˆ1ï¼‰æ£€æŸ¥ä¸­æ–­åèƒ½å¦çœŸçš„å†æ¬¡åŠ è½½
ï¼ˆ2ï¼‰å¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œåœ¨ä¿å­˜çš„faisså‘é‡é‡Œæ£€ç´¢0.csvé‡Œçš„textå†…å®¹ï¼Œçœ‹çœ‹ä¿å­˜çš„å‘é‡æ˜¯å¦çœŸçš„æœ‰æ•ˆ

1.csv0æ•°æ®çš„åœ¨ 'data/raw/articles/0.csv/0.csv',ä½ çš„è„šæœ¬ä¸­çš„åˆ—åä¼¼ä¹æœ‰é—®é¢˜
[æ ·æœ¬ 7] æŸ¥è¯¢æ–‡æœ¬ (å‰100å­—): 0X or 0-X ("zero/oh ex") may refer to:...
[æ£€ç´¢ç»“æœ] Top-5:
  [1] è·ç¦»=166672.0000 | chunk_id=0.8036060333251953 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [2] è·ç¦»=44025.0000 | chunk_id=0.7438042759895325 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [3] è·ç¦»=165743.0000 | chunk_id=0.681257426738739 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [4] è·ç¦»=44508.0000 | chunk_id=0.662455677986145 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [5] è·ç¦»=152025.0000 | chunk_id=0.6491485238075256 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
è¿™ç§æ£€ç´¢ç»“æœæ­£å¸¸å—
2.[progress] flush=6 chunks=12288 bytes~12.74MB speed=864.7 chunks/s emb=54% io=3%
æ–­ç‚¹é‡ç»­ä¹‹åï¼Œchunkså’Œbytesåªæ˜¾ç¤ºçš„æ˜¯æœ¬æ¬¡æ‰§è¡Œçš„embeddingçš„chunksï¼Œæˆ‘å¸Œæœ›è¿˜è¦æ˜¾ç¤ºçš„æ˜¯ç´¯è®¡çš„chunkså’Œbytes

[checkpoint] saved at chunks=38912 -> data/faiss/checkpoints/embedding_progress.json
D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/build_chunks_and_indices.py

D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/verify_index.py

æˆ‘çœ‹åˆ°embedding_progress.jsonä¸­"chunk_id": 38912, "flush_count": 19,ä½†bge_small_fp16_ip.faissæ–‡ä»¶æœ‰200MBï¼Œè¿™æ˜¯ä¸æ˜¯æ•°æ®ä¸Šä¸ä¸€è‡´äº†
æˆ‘å†æ¬¡è¿è¡Œembeddingæ—¶ï¼Œä¼¼ä¹åˆ†å—é€Ÿåº¦åªå‰©600chunksæ¯ç§’äº†ï¼Œä¸ºä»€ä¹ˆè¿˜å˜æ…¢äº†ï¼Ÿ
æ—¢ç„¶æˆ‘å·²ç»é‡‡ç”¨tokenizeræ·å¾„äº†ï¼Œé‚£æˆ‘é¢„å…ˆtokenizeræ‰€æœ‰csvæ•°æ®ï¼Œé‚£æ˜¯ä¸æ˜¯ä¼šæ›´å¿«ï¼Ÿ

æˆ‘éœ€è¦ä½ åšå‡ºæ”¹åŠ¨ï¼Œæ¯ä¸ªchunkå‰éƒ½è¦åŠ ä¸Šè¯¥chunkæ‰€å±æ–‡æ¡£çš„title
å¦‚ä½•æ‹¼æ ‡é¢˜ï¼Ÿç”¨ä»€ä¹ˆç¬¦å·æœ€åˆé€‚ï¼Ÿ
âš¡ æœ€æ¨èï¼ˆä¸šç•Œå…¬è®¤æœ€ä½³ï¼‰ï¼š
æ ¼å¼ï¼š"{title}\n\n{chunk_text}"

ä¹Ÿå°±æ˜¯ï¼š

æ ‡é¢˜
<ç©ºè¡Œ>
chunk å†…å®¹

ä¸ºä»€ä¹ˆç”¨æ¢è¡Œï¼Ÿ

Tokenizer å¯¹æ¢è¡Œæœ‰å¤©ç„¶åˆ†éš”æ•ˆæœ

ä¸å¼•å…¥é¢å¤–æ„ä¹‰ç¬¦å·ï¼ˆé¿å…è¯­ä¹‰è¯¯å¯¼ï¼‰

æ¨¡å‹ä¼šè®¤ä¸ºâ€œæ¢è¡Œ = æ–°æ®µè½â€

é«˜è´¨é‡ embedding å¸¸ç”¨æ ¼å¼ï¼ˆOpenAI, Cohere, Voyageï¼‰

ç›®å‰æˆ‘å·²ç»ç¡®å®šä½¿ç”¨ å¤ç”¨token idsçš„trickäº†ï¼Œå› ä¸ºå¤šè·¯embeddingçš„è€—æ—¶å¤ªé•¿ï¼Œæˆ‘ä¸èƒ½æ¥å—ã€‚å¦å¤–æˆ‘è¦æ±‚ä½ ä½¿ç”¨ä»¥ä¸‹trick
1.

æ€»æµç¨‹
ã€é˜¶æ®µ 0ï¼šé¢„å¤„ç†ã€‘
- æ–‡æ¡£çº§ embeddingï¼ˆdoc-embï¼‰
- æ–‡æ¡£çº§å®ä½“æå–ï¼ˆdoc-entityï¼‰
- åŠ¨æ€ chunking + chunk embeddingï¼ˆchunk-embï¼‰
- metadata å†™å…¥ parquet

ã€é˜¶æ®µ 1ï¼šquery å¤„ç†ã€‘
- query embedding
- query entityï¼ˆNERï¼‰

ã€é˜¶æ®µ 2ï¼šæ–‡æ¡£çº§å¬å›ï¼ˆfastï¼‰ã€‘
top_docs = ANN(doc_emb, top 1000)

ã€é˜¶æ®µ 3ï¼šæ–‡æ¡£å®ä½“é‡æ’åºï¼ˆcheapï¼‰ã€‘
for doc in top_docs:
    doc.score = 0.8 * embedding_score + 0.2 * entity_overlap

select doc_top_k = top 500â€“800

ã€é˜¶æ®µ 4ï¼šchunk-level å¬å›ã€‘
ä»è¿™ 500â€“800 æ–‡æ¡£çš„ chunk ä¸­ ANN å¬å› 1000â€“2000 chunkï¼ˆç”± bge-small å¤„ç†ï¼‰

ã€é˜¶æ®µ 5ï¼šäºŒé˜¶æ®µç²¾æ£€ã€‘
ç”¨ Jina V4 è®¡ç®— query ä¸ chunk ç²¾ç¡® embedding ç›¸ä¼¼åº¦
é€‰ top 100â€“200 è¿›å…¥ Reranker / LLM

ã€é˜¶æ®µ 6ï¼šLLM rerank + answerã€‘





## 0) æ•°æ®æ¸…æ´—ï¼ˆå¿…åšï¼Œé«˜å½±å“ï¼‰ä»¥åŠé¢„Tokenization 

ç›®æ ‡ï¼šå»æ‰åƒåœ¾ã€ç»Ÿä¸€é£æ ¼ã€å‡å°‘æ— ç”¨ chunksã€‚

**è¦ç‚¹ / æ“ä½œï¼ˆä¼˜å…ˆçº§é«˜ï¼‰**

1. **è¯­è¨€æ£€æµ‹**ï¼ˆåªä¿ç•™ç›®æ ‡è¯­è¨€enï¼‰

   * ç”¨ `langdetect` æˆ– `fasttext`ã€‚
2. **åªä¿ç•™ ASCII**ï¼ˆå» emojiã€ç½•è§ Unicodeï¼‰

   * regex: `re.sub(r'[^\p{Han}\p{Hiragana}\p{Katakana}\p{Hangul}\p{ASCII}]', '', text)` ï¼ˆéœ€ `regex` åº“ï¼‰
3. **ç§»é™¤ URL / HTML / æ§åˆ¶å­—ç¬¦**

   * URL regexã€`BeautifulSoup` å» htmlã€‚
4. **Unicode æ ‡å‡†åŒ–**ã€‚
*   **ä»£ç **ï¼š`import unicodedata; text = unicodedata.normalize('NFKC', text)`
*   **ä½œç”¨**ï¼šè§£å†³å…¨è§’/åŠè§’å­—ç¬¦æ··ä¹±ï¼ˆå¦‚ `ï¼‘ï¼’ï¼“` vs `123`ï¼Œ`ï¼¡` vs `A`ï¼‰ï¼Œä»¥åŠä¸åŒçš„ç©ºæ ¼ç¬¦å·ã€‚è¿™å¯¹äº Embedding å¯¹é½éå¸¸é‡è¦ã€‚
5. å°†å…¨éƒ¨æ–‡æ¡£textä½¿ç”¨bge_small tokenizeré¢„Tokenization 

6. **çŸ­æ–‡æœ¬è¿‡æ»¤**ï¼ˆå¤§å¤šæ•°å™ªéŸ³å‘ç”ŸäºæçŸ­æ–‡æœ¬ï¼‰

   * `min_text_tokens = 32`ï¼ˆæ¨èï¼‰



---

## 1) æ ‡é¢˜å¤„ç†ï¼ˆTitle cleaningï¼‰â€”â€”ä½ ç‰¹åˆ«å…³å¿ƒçš„éƒ¨åˆ†

**ç›®æ ‡**ï¼šåªæ‹¼æ¥â€œé«˜ä¿¡å™ªæ¯”â€æ ‡é¢˜ï¼Œå¹¶ä¸”é™åˆ¶é•¿åº¦ï¼Œé¿å…æ±¡æŸ“ embeddingã€‚

**æ­¥éª¤**

1. æ ‡é¢˜å™ªå£°æ£€æµ‹ï¼ˆæ ¸å¿ƒï¼‰** â€” å†³å®šæ˜¯å¦æ‹¼æ¥æ ‡é¢˜

   * åˆ¤å®šè§„åˆ™ï¼ˆç¤ºä¾‹ï¼‰ï¼š

     ```python
     def is_good_title(t):
         t=t.strip()
         if not t or len(t)<3 or len(t)>120: return False
         alnum_ratio = sum(c.isalnum() for c in t)/max(1,len(t))
         if alnum_ratio>0.8: return False
         if " " not in t and len(t)>6: return False
         if re.search(r'(file_|doc_|id_|^v\d+\.)', t.lower()): return False
         return True
     ```
   <!-- * é¢å¤–ï¼šç»Ÿè®¡ title ä¸­çš„è‹±æ–‡å•è¯æ¯”ä¸­æ–‡å­—ç¬¦æ¯”ç‡ï¼Œä½äºé˜ˆå€¼å°±åˆ¤ä¸ºåƒåœ¾ã€‚ -->
2. å»åœç”¨è¯ï¼ˆå¯é€‰ï¼Œä»…å½“ title å¾ˆé•¿æ—¶ï¼‰

   * English stopword: NLTK/stopwords
   * Chinese: è‡ªå®šä¹‰åœç”¨è¯è¡¨
   * ç¤ºä¾‹ï¼ˆPythonï¼‰ï¼š

     ```python
     from nltk.corpus import stopwords
     stops = set(stopwords.words('english'))
     def shorten_title(t, max_words=8):
         words=t.split()
         words=[w for w in words if w.lower() not in stops]
         return " ".join(words[:max_words])
     ```
3. é™é•¿ï¼ˆtokençº§ï¼‰ï¼š

   * `title_max_tokens = 8~32`ï¼ˆæ¨è 8-16 è‹±æ–‡å•è¯ / 16â€“32 token è§†æ¨¡å‹ï¼‰
   * ç”¨ tokenizer æˆªæ–­ï¼š`title_tokens = tokenizer.encode(title)[:title_max_tokens]`
4. æ­£åˆ™æ¸…æ´—ï¼ˆç§»é™¤å¸¸è§å™ªéŸ³ï¼‰ï¼š

   * remove serials: `re.sub(r'\b[A-Z0-9_-]{6,}\b','',title)`
   * remove leading/trailing punctuation: `title.strip(" -_:;.,")`

**æ‹¼æ¥å»ºè®®**

* å¦‚æœ `is_good_title` ä¸º Trueï¼š

  ```
  chunk_text_final = title_clean + "\n\n" + entities_line + "\n\n" + chunk_body
  ```
* å¦åˆ™ï¼Œä¸æ‹¼ titleã€‚

---
å¤„ç†åæŠŠç»“æœå†™åˆ° Parquetï¼ˆåˆ—ï¼šdoc_id, title, doc_ids, length_tokensï¼‰ä»¥ä¾¿åç»­é‡ç”¨ã€‚æ¯éš”å¤§æ¦‚256MBç”Ÿæˆä¸€æ¬¡ 1.Parquet   2.Parquet
å®ç°ä»¥ä¸Šè¦æ±‚ï¼Œä»£ç å¯ä»¥è€ƒè™‘æ”¾åœ¨processingæ–‡ä»¶å¤¹ä¸‹ï¼Œå¦‚æœä½ æœ‰éœ€è¦ç”Ÿæˆæ–°çš„æ–‡ä»¶å¤¹ä¹Ÿå¯ä»¥

æˆ‘å·²ç»å®‰è£…äº†beautifulsoup4 regex langdetectï¼Œä¸è¦ä½ è‡ªå·±å»å®ç°ï¼Œç”¨regexå»ç§»é™¤url
æ ‡é¢˜å¤„ç†å»ºè®®é‡Œæœ‰ä¸€ä¸ªæ˜¯é”™çš„ï¼Œä»–æåˆ°ç›´æ¥ç”¨å­—ç¬¦ä¼°è®¡tokenæ•°ï¼Œæˆ‘ä¸éœ€è¦ï¼Œç›´æ¥encodeå¹¶æ ‡é¢˜çš„idsï¼Œåç»­ç›´æ¥è€ƒè™‘åˆå¹¶
åŸºäºä¸¤ä¸ªå»ºè®®æ–‡ä»¶ï¼Œä¿®æ”¹ä»£ç ï¼Œæ³¨æ„ï¼Œæˆ‘å‡†å¤‡è¿›ä¸€æ­¥å°†åŠ¨æ€chunkingå’Œé¢„tokenizeç»“åˆï¼Œåœ¨ä¸€ä¸ªpiplineä¸­å®Œæˆä»csvæ–‡ä»¶ åˆ° chunksï¼Œå…·ä½“æ€ä¹ˆåšæˆ‘ä¼šåç»­å‘Šè¯‰ä½ ï¼Œä½†ä½ è€ƒè™‘é¢„ç•™æ¥å£ä½ç½®

å°†chunkingçš„æµç¨‹å¹¶å…¥ preprocess_dataï¼Œæ³¨æ„é‡‡ç”¨çˆ¶æ–‡æ¡£ç´¢å¼•ï¼Œå­æ–‡æ¡£chunksize 128ï¼Œæ— overlapï¼Œçˆ¶æ–‡æ¡£512
meta dataé‡Œéœ€è¦å­˜å‚¨
rerank_text	String: å·²æ‹¼æ¥ Titleã€‚ Reranker è¾“å…¥ã€‚ Reranker éœ€è¦æ–‡æœ¬æ¥åš Cross-Attention å¯¹æ¯”ã€‚
doc_id
start_idx, end_idx çˆ¶æ–‡æ¡£åœ¨åŸæ–‡æ¡£idsä¸­çš„èµ·å§‹å’Œç»ˆæ­¢index
chunk_id	Int/String	å‘é‡ç´¢å¼• ID	ä¸»é”®ã€‚
chunk_len	Int	å­ Chunk é•¿åº¦
è¿™æ„å‘³ç€ç¡®å®éœ€è¦æŠŠæ¸…æ´—ç»“æœç»“æœå†™åˆ° Parquetï¼ˆåˆ—ï¼šdoc_id, title, doc_ids, length_tokensï¼‰
è¿™æ ·åç»­æ‰èƒ½æ ¹æ® doc_id, start_idx, end_idx å¿«é€Ÿå®šä½åˆ°çˆ¶æ–‡æ¡£

1.è¿™ä¸ªç»ˆç«¯æç¤ºçš„è¶…å‡ºtokenæ•°ä¼šæœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
2.æˆ‘å¥½å¥‡ä½ åœ¨partqueé‡Œå­˜å‚¨idsæ˜¯ä»¥ä»€ä¹ˆæ ¼å¼ï¼Œæˆ‘å¬è¯´ np.uint16å¯ä»¥å‹ç¼©ç©ºé—´ï¼Œè¿˜å¯ä»¥æŒ‡å®šå‹ç¼©ç®—æ³•ZSTDï¼š é€Ÿåº¦è¾ƒå¿«ï¼Œå‹ç¼©ç‡æœ€é«˜ï¼ˆæ¨èï¼‰ã€‚
3.æˆ‘éœ€è¦ä½ åœ¨preprocess_dataå®ç° æ–­ç‚¹é‡ç»­æœºåˆ¶

å·²å¤„ç† 3,000 æ¡æ–‡æ¡£ï¼Œä¿ç•™ 526 æ¡è¿™ç§ä¿¡æ¯ï¼Œåä¸€å¤©è¦†ç›–å‰ä¸€æ¡ï¼Œä¸ç„¶å¤ªé•¿äº†
ä½ çš„tokenizeé€»è¾‘å¥½åƒæœ‰é—®é¢˜å•Šï¼ŒToken indices sequence length is longer than the specified maximum sequence length for this model (5947 > 512). Running this sequence through the model will result in indexing errors
æˆ–è€…è¯´è¿™åªæ˜¯ä¸€ä¸ªè­¦å‘Š
æ–­ç‚¹é‡ç»­æ—¶ï¼Œè®°å¾— â€œä¿ç•™ 3,381 æ¡â€è¿™ä¸ªæ•°é‡è¦å’Œä¹‹å‰çš„ç´¯åŠ 

å†™ä¸€ä¸ªè„šæœ¬ï¼ŒæŸ¥çœ‹ç°æœ‰çš„ docs_1.parquet chunks_1.parqueté‡Œé¢å­˜çš„æ˜¯ä»€ä¹ˆæ•°æ®ï¼Œå«åš check_parquet_data
æˆ‘å‘ç°äº†ä¸€ä¸ªå¾ˆä¸¥é‡çš„é—®é¢˜ï¼Œä¼¼ä¹ä½ æ–­ç‚¹é‡ç»­ä¹‹åä¼šé‡æ–°å¼€å§‹å†™å…¥chunks_1.parquetï¼Œè€Œä¸”é‡ç»­ä¹‹åè¿˜æ˜¯ä»0å¼€å§‹è®¡ç®— â€œä¿ç•™ x æ¡â€

ä½ é€šè¿‡ len(tokens)*4è¿™ç§æ–¹æ³•ä¼°è®¡å®Œå…¨æ˜¯é”™çš„ï¼Œé¦–å…ˆæˆ‘æ”¹æˆäº† uint16åå æ®ä¸¤å­—èŠ‚ï¼Œå…¶æ¬¡è¿˜æœ‰ zstdå‹ç¼©ï¼Œ
ç°åœ¨æˆ‘è®¾å®šå†™å…¥çš„é˜ˆå€¼æ˜¯ 1G
ç„¶åä½ éœ€è¦æ ¹æ®å®é™…æ–‡ä»¶å¤§å°å’Œå¤„ç†æ–‡æ¡£è¡Œæ•°æ¥ä¼°ç®—æ–‡ä»¶å¤§å°ï¼Œä¸€ä¸ªchunks.parquetçº¦ä¸º82MBï¼Œä¸€ä¸ªdocs.parquetçº¦ä¸º95MB

æ€ä¹ˆè¿1Gchunk docéƒ½è¾“å‡ºä¸äº†å°±å†…å­˜ä¸å¤Ÿäº†ï¼Œæ˜¯ä¸æ˜¯æ„å‘³ç€1Gå¤ªå¤§åº”è¯¥æ”¹æˆ512MB,è¿˜æ˜¯è¯´ä½ çš„ç¨‹åºæˆ–è®¡ç®—å…¬å¼å“ªé‡Œæœ‰é—®é¢˜
æ³¨ï¼šratioæˆ‘åæ¥å¾®è°ƒäº†ä¸€ç‚¹ï¼Œä½†åº”è¯¥ä¸å½±å“


## 5) Embeddingï¼ˆç”Ÿæˆå‘é‡ï¼‰

**åŸåˆ™**

* ä½¿ç”¨ HF backend + `embed_from_tokens` å¯è·å¾—æœ€é«˜é€Ÿåº¦ï¼ˆå‰æï¼šæ¨¡å‹æ”¯æŒç›´æ¥ id è¾“å…¥ï¼‰
* dtypeï¼š`float16`ï¼Œdevice=`cuda`
* batch_sizeï¼šå°½é‡å¤§ï¼ˆå—æ˜¾å­˜é™åˆ¶ï¼Œå¸¸è§ 1024~4096ï¼‰
* ç”Ÿæˆå¹¶ä¿å­˜ï¼š`id -> vector` å†™å…¥ FAISSï¼ˆæŒ‰æ‰¹å†™å…¥ï¼‰

---

## 6) FAISS / Indexï¼ˆæ£€ç´¢ï¼‰

**ç»“æ„**

* FAISS index: å­˜å‘é‡ + id
* metadata store (Parquet/SQLite/duckDB): id -> chunk_text, title

**Index å»ºè®®**

* IVF-FLAT æˆ– HNSW + å­˜å‚¨åŸå‘é‡ï¼ˆä¾è§„æ¨¡ï¼‰
* å»º index æ—¶å½’ä¸€åŒ–ï¼ˆL2/cosineï¼‰: `index = faiss.IndexFlatIP(...)` with normalized vectors
* å†™å…¥æ—¶ batch commit + periodic persist
æˆ‘ä¹‹å‰ä¹Ÿæœ‰embeddingä»£ç ï¼Œä¸è¿‡é‚£ä¸ªä»£ç å¯èƒ½è¦åºŸå¼ƒäº†ï¼Œç°åœ¨æˆ‘åªç”¨bge smallè¿›è¡Œembeddingï¼Œè€Œä¸”ç›´æ¥ä»parquetè¯»å–chunk idsè¿›è¡Œembedding

faissç´¢å¼•åªå­˜å‚¨chunk_idå’Œembeddingï¼Œå…¶ä»–metadataé€šè¿‡chunkidä»chunksçš„parquetæ–‡ä»¶ä¸­è¯»å–
faissç´¢å¼•é€šè¿‡LZ4 ç®—æ³•å‹ç¼©

1.è¿™æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Œå¤„ç†äº†400ä¸‡å‘é‡åçªç„¶ä¸­æ­¢äº† 
2.æˆ‘æƒ³åœ¨â€œå·²å¤„ç† 4,009,984 chunks | å·²åµŒå…¥ 4,009,984 å‘é‡ | é€Ÿåº¦ 637 vec/sâ€æ‰“å°ä¿¡æ¯ä¸­ï¼ŒæŒ‰ hh:mm:ssæ˜¾ç¤ºå½“å‰æ—¶é—´
3.è€Œä¸”é€Ÿåº¦æ€ä¹ˆè¶Šæ¥è¶Šæ…¢äº†ï¼Œä»1500 vec/såˆ°637vecs/s

è¯·å›ç­”.1.ä¿®æ”¹ä»£ç ä¹‹åï¼Œæˆ‘è¿˜èƒ½æ–­ç‚¹å†ç»­å—ï¼Œè¿˜æ˜¯è¦å½»åº•é‡æ–°å¼€å§‹ 2.æ¯60sè‡ªåŠ¨ä¿å­˜é—´éš”å¤ªçŸ­ï¼Œæ”¹ä¸º5åˆ†é’Ÿä¿å­˜ä¸€æ¬¡ 3.è¯·å›ç­”å‘é‡æ˜¯ä»¥fp16å½¢å¼å­˜å‚¨å—ï¼Ÿè¿˜æ˜¯fp32

æ€»å…±åº”è¯¥ç”±ä¸‰åƒä¸‡å‘é‡ï¼Œé‚£æˆ‘åº”è¯¥ä½¿ç”¨ä»€ä¹ˆç´¢å¼•ç»“æ„ ï¼Œæˆ‘æƒ³ä½¿ç”¨IVFScalarQuantizerï¼Œä½¿ç”¨fp16å‚¨å­˜å‘é‡

æˆ‘å‘ç°ä¸€ä¸ªå¾ˆä¸¥é‡çš„é—®é¢˜ï¼Œ1.csvåƒè¿™æ ·çš„æ–‡ä»¶é‡Œå‚¨å­˜çš„wikiæ•°æ®å¥½åƒå¯èƒ½ä¸æ˜¯ç§‘å­¦ä¸»é¢˜çš„ï¼Œå¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œè¯»å–è¿™ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥è°ƒç”¨build_chunks_and_indices.pyé‡Œçš„æ¸…æ´—æ–¹æ³•ï¼Œç­›é€‰å‡ºé•¿åº¦å¤§äº32å­—ç¬¦çš„ï¼Œæ‰“å°å‡ºå‰20è¡Œ

æˆ‘æ–°åŠ äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä¾‹å¦‚'data\raw\archive\0_to_25000.parquet'ï¼Œç”¨åŒæ ·çš„æ–¹æ³•å¤„ç†ï¼Œå¯èƒ½åªæ˜¯è¯»å–çš„æ–¹æ³•ä¸ä¸€æ ·ï¼Œæ³¨æ„parquetåŠ›æ•°æ®çš„åˆ—åæˆ‘ä¸ç¡®å®š

ç”±äºä¹‹å‰çš„csvæ–‡æ¡£æ•°æ®é‡Œå¾ˆå¤šä¸æ˜¯ç§‘å­¦ä¸»é¢˜çš„ï¼Œæ‰€ä»¥æˆ‘æ¢äº†ä¸€ä¸ªå¤§æ¦‚å‡ ç™¾MBçš„parquetæ ¼å¼çš„æ•°æ®é›†ï¼Œä¾‹å¦‚ 'data\raw\archive\0_to_25000.parquet',æ–‡ä»¶åŒ…å«çš„æ‰€æœ‰åˆ—: ['text', 'url', 'title'],ä½†æˆ‘å¸Œæœ›ä½ èƒ½ä¿ç•™å¯¹csvæ–‡ä»¶çš„è¯»å–èƒ½åŠ›é˜²æ­¢æœªæ¥æˆ‘åˆåˆ‡æ¢æ•°æ®é›†äº†ï¼Œå³å®ç°ä¸»è¦ä»£ç é€»è¾‘å’Œè¯»å–æ–‡ä»¶å½¢å¼çš„è§£è€¦
æ³¨æ„ï¼Œç”±äºæ•°æ®é‡å¤§å¤§å‡å°‘ï¼Œæˆ‘ä¸å†é‡‡ç”¨ å¤ç”¨idsçš„æŠ€å·§ï¼Œè¿™æ„å‘³ç€æ–°ç”Ÿæˆçš„ chunk.parquet é‡Œ
ç”±äºè¦ä½¿ç”¨å¤šä¸ªembedding modelè¿›è¡Œensembleï¼Œæ„å‘³ç€è¦å®ç°chunkå’Œtokenizerçš„è§£è€¦ï¼Œå¯ä»¥é‡‡å–å¦‚ä¸‹æ–¹å¼ï¼Œtokenizeré‡‡ç”¨ tiktoken
çˆ¶ 512 / å­ 128 token çš„å®ç°æ–¹æ³•
ğŸŸ© æ–¹æ³• 1ï¼ˆæ¨èï¼‰ï¼šç”¨â€œå¥å­åˆ—è¡¨ + æ»‘åŠ¨çª—å£â€å®ç° token é™åˆ¶ï¼ˆåŠ¨æ€çš„ï¼‰

æµç¨‹ï¼š

Step 1ï¼šå…ˆæŠŠæ–‡æ¡£æŒ‰å¥å­åˆ‡ï¼ˆåŸºäºå­—ç¬¦ï¼‰ï¼Œå¾—åˆ°ï¼š
sentence[i]  # æ¯ä¸ªå¥å­ç”¨ char_start/char_end å®šä¹‰

Step 2ï¼šåŠ¨æ€ç»„åˆå¥å­ç›´åˆ°è¾¾åˆ°çº¦ 128 tokensï¼Œå½¢æˆå­ chunk

ä½ å¯ä»¥è¿™æ ·åšï¼š

current_chunk = ""
for sent in sentences:
    if tokenizer(current_chunk + sent).len() > 128:
        yield current_chunk
        current_chunk = sent
    else:
        current_chunk += " " + sent

Step 3ï¼šçˆ¶ chunk = æ‹¼æ¥ N ä¸ªå­ chunkï¼Œç›´åˆ°çº¦ 512 tokens
æ³¨æ„

æ‰€æœ‰ chunk éƒ½ç”¨ å­—ç¬¦ offset ç®¡ç†æ–‡æœ¬ï¼Œä¸ä¾èµ– token offsetã€‚

embedding æ¨¡å‹ tokenize æ—¶è‡ªåŠ¨é€‚é…ã€‚

æœ‰é—®é¢˜å•Š
1.ä½ ç¡®å®šä½¿ç”¨äº†text_cleaner.pyå’Œtitle_cleaner.pyå®šä¹‰çš„æ–¹æ³•æ¥æ¸…æ™° text titleå—
2.parquetæ–‡ä»¶çš„æ•°æ®é‡Œï¼Œæ¯è¡Œæ•°æ®ä¼¼ä¹æ²¡æœ‰idï¼Œè¿™è¦æ±‚ä½ è‡ªè¡Œç”Ÿæˆid
3.æˆ‘è®¤ä¸ºchunkæ•°æ®é‡Œä¸ç”¨å­˜ rerank_textï¼Œè€Œä¸”è¦å­˜ title
4.ä½ ç¡®å®šä½ ä¿å­˜äº†æ¸…æ´—è¿‡äº†çš„docå—ï¼Œå¯¹åŸparquetæ•°æ®æ¸…æ´—ä¹‹åï¼ŒåŠ ä¸Šdoc_idï¼ˆå¦‚æœåŸæ¥æ²¡æœ‰idçš„è¯ï¼‰ï¼Œå†å®Œå…¨å¯ä»¥ä¸€èµ·å‚¨å­˜æˆä¸€ä¸ªæ–‡ä»¶ï¼ˆå› ä¸ºæ€»å…±æ‰å‡ ç™¾MBï¼‰

å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...
âœ… æ–‡æ¡£å·²ä¿å­˜: data/processed\documents_cleaned.parquet
   å¤§å°: 315.52 MB | è®°å½•æ•°: 130,926
âœ… Chunkså·²ä¿å­˜: data/processed\chunks.parquet
   å¤§å°: 23.67 MB | è®°å½•æ•°: 1,281,646
âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: data/processed\preprocess_stats.json

============================================================
å¤„ç†ç»Ÿè®¡:
  æ€»æ–‡æ¡£æ•°: 131,049
  ä¿ç•™æ–‡æ¡£: 130,926 (99.9%)
  æ€»chunks: 1,281,646
  å¹³å‡chunks/æ–‡æ¡£: 9.8

è¿‡æ»¤ç»Ÿè®¡:
  è¿‡çŸ­æ–‡æœ¬: 123
  æ ‡é¢˜è´¨é‡: 33



å‡†å¤‡è¿›è¡Œembeddingä»¥åŠæ„å»ºIndexFlatIPç´¢å¼•ï¼Œå› ä¸ºchunkåªæœ‰ä¸€ç™¾ä¸‡ä¸”æµ‹è¯•é›†ä¸­queryæ•°é‡å°‘ï¼Œæ‰€ä»¥ä½¿ç”¨IndexFlatIPå–å¾—æœ€å¤§ç²¾åº¦
* ä½¿ç”¨ HF backend + `embed_from_tokens` å¯è·å¾—æœ€é«˜é€Ÿåº¦ï¼ˆå‰æï¼šæ¨¡å‹æ”¯æŒç›´æ¥ id è¾“å…¥ï¼‰
* dtypeï¼š`float16`ï¼Œdevice=`cuda`
* batch_sizeï¼šå°½é‡å¤§ï¼ˆå—æ˜¾å­˜é™åˆ¶ï¼Œå¸¸è§ 1024~4096ï¼‰
* ç”Ÿæˆå¹¶ä¿å­˜ï¼š`id -> vector` å†™å…¥ FAISSï¼ˆæŒ‰æ‰¹å†™å…¥ï¼‰

---

## 6) FAISS / Indexï¼ˆæ£€ç´¢ï¼‰

**ç»“æ„**

* FAISS index: å­˜å‘é‡ + id
* metadata store (Parquet/SQLite/duckDB): id -> chunk_text, title

**Index å»ºè®®**

* faissä½¿ç”¨IndexFlatIPç´¢å¼•
* å»º index æ—¶å½’ä¸€åŒ–ï¼ˆL2/cosineï¼‰: `index = faiss.IndexFlatIP(...)` with normalized vectors
* å†™å…¥æ—¶ batch commit + periodic persist
faissç´¢å¼•åªå­˜å‚¨chunk_idå’Œembeddingï¼Œå…¶ä»–metadataé€šè¿‡chunkidä»chunksçš„parquetæ–‡ä»¶ä¸­è¯»å–
faissç´¢å¼•é€šè¿‡LZ4 ç®—æ³•å‹ç¼©
ä½ å¯ä»¥å‚è€ƒsrc\pipeline\build_embeddings.pyï¼Œå¤§éƒ¨åˆ†ä»£ç éƒ½åº”è¯¥æ˜¯ç›¸åŒçš„ï¼Œä¸»è¦æœ‰ä»¥ä¸‹åŒºåˆ«
1.faissä½¿ç”¨IndexFlatIPç´¢å¼•
2.embeddingæ¨¡å‹è¯»å–å¾—ä¸å†æ˜¯idsï¼Œè€Œæ˜¯ä»chunkä¸­æŸ¥è¯¢åˆ°å¼€å§‹ç»“æŸçš„å­—ç¬¦ç´¢å¼•åï¼Œä»documents_cleaned.parquetè¯»å–text
3.ç›®å‰å…ˆé‡‡ç”¨ qwen3 0.6B embedding æ¨¡å‹è€Œä¸æ˜¯bge smallï¼Œåç»­å¯èƒ½æ·»åŠ æ–°çš„æ¨¡å‹ï¼Œå»ºè®®æ¨¡å‹è·å–å’Œå¤„ç†è§£è€¦ï¼Œæ–¹ä¾¿åç»­ç›´æ¥æ›¿æ¢æ¨¡å‹

1.æˆ‘å‘ç°ä½ è¿™äº›æ¨¡å‹å‚æ•°ä¸æ˜¯è¯»å–yamlæ–‡ä»¶çš„ä¸å¯¹å§ï¼Œè¿™äº›å‚æ•°åº”è¯¥ä»yamlæ–‡ä»¶è¯»ï¼Œè€Œæˆ‘æ§åˆ¶çš„æ˜¯é€‰æ‹©å“ªä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚æˆ‘ä¼ å…¥qwen3ï¼Œä½ å°±æ˜¯ä½¿ç”¨ config[embedding_qwen3]çš„å‚æ•°

1.yamlä¸­å‚æ•°çš„ç»“æ„å‘ç”Ÿäº†å˜åŒ–ï¼Œä¾‹å¦‚qwenç°åœ¨æ˜¯è¿™æ ·çš„
embedding:
  qwen3:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    max_length: 168
    device: null   # è‡ªåŠ¨é€‰æ‹© cuda/cpuï¼ˆå»ºè®®åœ¨æœ‰ GPU æ—¶è®¾ä¸º "cuda"ï¼‰
    dtype: float16 # åœ¨ CUDA ä¸Šç”¨ FP16 æå‡ååï¼›CPU å°†å›é€€ FP32
    index_path: "data/faiss/qwen3_fp16_ip.faiss"
æˆ‘å»ºè®®å°†æ‰€æœ‰æ¶‰åŠmodelçš„å‚æ•°æ”¾å…¥yamlè€Œä¸æ˜¯ç¡¬ç¼–ç 
2.è§£å†³æŠ¥é”™

embedingè¿‡ç¨‹printä¿¡æ¯ï¼Œä¾‹å¦‚ 
æ¯ç§’å¤„ç†å¤šå°‘chunksï¼Œ
æ€»å…±å¤„ç†å¤šå°‘chunks/chunksæ€»æ•°(çº¦128ä¸‡),
å·²ç»å¤„ç†çš„æ—¶é—´ï¼Œ
é¢„è®¡è¿˜éœ€è¦å¤šå°‘æ—¶é—´
æ³¨æ„ \r é˜²æ­¢æ‰“å°å¤ªå¤šè¡Œ
è¿˜æœ‰æ¯æ¬¡å†™å…¥ä¹Ÿè¦æ‰“å°ä¿¡æ¯ï¼Œæ³¨æ„åˆ«è¢« \rè¦†ç›–äº†ï¼Œå†™å…¥è®°å¾—æ ‡æ³¨æ—¶é—´

embeddingå¤ªæ…¢äº†ï¼Œæˆ‘æ€€ç–‘ä½ åœ¨é€æ¡å¤„ç†ï¼Œå‚è€ƒä»¥ä¸‹ä»£ç è¿›è¡ŒåŠ é€Ÿ
ä¸‰è€…ç»„åˆåçš„é€Ÿåº¦å¤§æ¦‚å¦‚ä½•ï¼Ÿ

å…¸å‹æ•ˆæœï¼š

æ–¹æ³•	ç›¸å¯¹é€Ÿåº¦
åŸå§‹ Python for-loop	1Ã—
Arrow-based æ‹¼æ¥	3â€“6Ã—
tokenizer multiprocessing / Fast tokenizer	4â€“10Ã—
batched pipelineï¼ˆé¿å…é€æ¡å¤„ç†ï¼‰	5â€“15Ã—
ä¸‰è€…å åŠ 	30ï½80Ã— æå‡ï¼ˆçœŸå®å¯è¾¾ï¼‰

å¦‚æœä½ æœ‰å‡ ç™¾ä¸‡æ–‡æœ¬ï¼Œè¿™ä¸ªå·®è·æ˜¯éå¸¸å·¨å¤§çš„ã€‚

ğŸ§© ç»™ä½ çœ‹ä¸€ä¸ªæœ€ç»ˆçš„æ•´åˆç¤ºä¾‹ï¼ˆæ ¸å¿ƒç»“æ„ï¼‰
import pyarrow as pa
import pyarrow.parquet as pq
from multiprocessing import Pool
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("your-model", use_fast=True)

def tokenize_batch(batch_texts):
    out = tokenizer(batch_texts, padding=False, truncation=False)
    return out["input_ids"]

def process_batches(all_texts, batch_size=2048):
    batches = []
    for i in range(0, len(all_texts), batch_size):
        batches.append(all_texts[i:i+batch_size])
    return batches

all_batches = process_batches(doc_texts)

with Pool(16) as p:
    tokenized_batches = p.map(tokenize_batch, all_batches)

tables = []
for batch_input_ids in tokenized_batches:
    arr = pa.array(batch_input_ids, type=pa.list_(pa.int32()))
    table = pa.Table.from_arrays([arr], names=["input_ids"])
    tables.append(table)

final_table = pa.concat_tables(tables)
pq.write_table(final_table, "tokens.parquet")


åˆ†æç»“æœ:
==================================================
å¹³å‡CPUä½¿ç”¨ç‡: 7.1%
å¹³å‡GPUä½¿ç”¨ç‡: 99.3%

ğŸŸ¢ ç»“è®º: GPUç“¶é¢ˆ
   CPUå‡†å¤‡æ•°æ®é€Ÿåº¦å¤Ÿå¿«
   å»ºè®®: å¢å¤§batch_sizeã€ä½¿ç”¨æ›´å¤§æ¨¡å‹
ä¼šä¸ä¼šæ˜¯å› ä¸ºCPUæ˜¯å•è¿›ç¨‹å¯¼è‡´åˆ©ç”¨ç‡ä¸Šä¸å»



æˆ‘å°†ä»£ç è½¬ç§»åˆ°äº†linuxæœåŠ¡å™¨ä¸Šï¼Œå¹¶å®‰è£…äº†  faiss-gpuå’Œ flash-attnï¼ŒåŒæ—¶å¯ä»¥å¤šçº¿ç¨‹ï¼Œä½ é€šè¿‡yamlçš„use_linuxå‚æ•°æ§åˆ¶æ˜¯å¦å¯¼å…¥å’Œå¯åŠ¨
æ³¨æ„ï¼Œqwen3æ˜¯æ”¯æŒ flash-attnçš„
åœ¨faissçš„ç”Ÿæˆé˜¶æ®µï¼Œfaiss-gpuçš„ä»£ç ä¼¼ä¹å¹¶æ²¡æœ‰å’Œfaiss-cpuæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œä½†åœ¨æ£€ç´¢æ—¶ï¼Œä¸¤è€…ä¼šæœ‰ä¸€å®šåŒºåˆ«


æˆ‘éœ€è¦å¼•å…¥infgrad/Jasper-Token-Compression-600Mæ¨¡å‹ï¼Œç”¨äºembeddingï¼Œå‚æ•°æ”¾åœ¨yamlçš„embeddingéƒ¨åˆ†ä¸­
ä½¿ç”¨ä¾‹å­å¯ä»¥å‚è€ƒ/root/autodl-tmp/Science-Exam-LLM/src/download/test.pyï¼Œä¸è¿‡ä¸ºäº†ç²¾åº¦æˆ‘ä»¬åªè¿›è¡Œencodeï¼Œä¸è¿›è¡Œå‹ç¼©
å¥½åƒJasperä¹Ÿæ˜¯æ”¯æŒflash attnçš„

ä¸ºä»€ä¹ˆqwen3_fp16_ip.faiss.lz4æ¯”qwen3_fp16_ip.faisså¤§è¿™ä¹ˆå¤šï¼Ÿä»–ä»¬éš¾é“ä¸éƒ½æ˜¯ç´¢å¼•æ–‡ä»¶å—ï¼Ÿéš¾é“qwen3_fp16_ip.faiss.lz4ä¸æ˜¯ï¼Ÿ
drwx---rwx 2 root root   48 Nov 23 12:03 checkpoints
-rw-r--r-- 1 root root  57M Nov 23 13:25 qwen3_fp16_ip.faiss
-rw-r--r-- 1 root root 4.9G Nov 23 13:25 qwen3_fp16_ip.faiss.lz4

ä¼šä¸ä¼šæœ‰ä¸€ç§å¯èƒ½qwen3_fp16_ip.faissåªæ˜¯ç”¨æ¥å‚¨å­˜ä¸­é—´æ–‡ä»¶çš„ï¼Œqwen3_fp16_ip.faiss.lz4å‚¨å­˜çš„æ‰æ˜¯æ‰€æœ‰ç´¢å¼•æ–‡ä»¶ï¼Ÿ

qwen3_fp16_ip.faiss.lz4æœ‰4.8Gå¤§æ¦‚è¿™ä¸ªæ‰æ˜¯å‚¨å­˜äº†ç´¢å¼•çš„æ–‡ä»¶ï¼Œ128ç©chunksçš„ç´¢å¼•æœ¬æ¥å°±åº”è¯¥æœ‰å¥½å‡ Gï¼Œ

D:\4000_projects\1project\LLM\data\faiss\qwen3_fp16_ip.faiss.lz4
å¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬åœ¨src/scriptç›®å½•ä¸‹ï¼Œè§£å‹ç´¢å¼•æ–‡ä»¶å¹¶å¯¹å…¶è¿›è¡Œæ£€æŸ¥ï¼Œçœ‹æ˜¯å¦æœ‰æ•ˆï¼Œä¾‹å¦‚å’Œchunks.parquetå¯¹æ¯”ï¼Œçœ‹chunk_idæœ‰æ²¡æœ‰å…¨è¦†ç›–

æ£€æŸ¥data\faiss\qwen3_fp16_ip.faissæ˜¯å¦æ˜¯ qwen3_fp16_ip_chunk_ids.jsonï¼Œå› ä¸ºæˆ‘å‚¨å­˜æ—¶å¯èƒ½å‘½åé”™è¯¯äº†ï¼Œä½†æˆ‘ä¹Ÿä¸ç¡®å®šåˆ°åº•æ˜¯ä¸æ˜¯ï¼Œå†™ä¸€ä¸ªè„šæœ¬åœ¨src/scriptç›®å½•ä¸‹ï¼Œåˆ¤æ–­qwen3_fp16_ip.faissåˆ°åº•æ˜¯ ä¸€ä¸ªç´¢å¼•ï¼Œè¿˜æ˜¯æ˜ å°„æ–‡ä»¶

è¿™ä¸ªæ˜ å°„æ–‡ä»¶æ˜¯ä¸æ˜¯æœ‰é—®é¢˜å•Šï¼Œä¸ºä»€ä¹ˆä¸æ˜¯æ˜ å°„data\processed\chunks.parquetï¼Œè€Œæ˜¯æ˜ å°„\data\raw\archive\25000_to_50000.parquetè¿™ç§åŸå§‹æ•°æ®æ–‡ä»¶

æˆ‘çš„æ˜ å°„æ–‡ä»¶å¥½åƒæ²¡æœ‰å†™é”™ï¼Œchunk idå¥½åƒå°±æ˜¯è¿™æ ·åˆ†çš„
[
  "0_to_25000.parquet:row:0:chunk:0",
  "0_to_25000.parquet:row:0:chunk:1",
  "0_to_25000.parquet:row:1:chunk:0",
  "0_to_25000.parquet:row:1:chunk:1",
  å¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œè¯»å–D:\4000_projects\1project\LLM\data\processed\chunks.parquetï¼Œå‰åè¡Œæ•°æ®å¹¶æ‰“å°

                             chunk_id                    doc_id              title  child_start  child_end  parent_start  parent_end  chunk_len
0  0_to_25000.parquet:row:0:chunk:0  0_to_25000.parquet:row:0       Becurtovirus            0        479             0         911        121
1  0_to_25000.parquet:row:0:chunk:1  0_to_25000.parquet:row:0       Becurtovirus          479        911             0         911         93
2  0_to_25000.parquet:row:1:chunk:0  0_to_25000.parquet:row:1       Cyprinivirus            0        403             0        1062        103
3  0_to_25000.parquet:row:1:chunk:1  0_to_25000.parquet:row:1       Cyprinivirus          403        891             0        1062 
parent_startå›ºå®šä¸º0ï¼Œparent end å‡ ä¹ä»¥å‰ï¼Œè¿™ä¸ªåˆ†å—é€»è¾‘æœ‰é—®é¢˜å•Šï¼Œæˆ‘çš„parentåº”è¯¥æ˜¯å·®ä¸å¤š512tokenæ‰å¯¹å•Šï¼Œch

ä¸ºæˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œåä¸ºcheck_retrieve,å¤§æ¦‚æ€è·¯å°±æ˜¯ä»chunk parquetéšæœºè¯»å–å‡ºäº”ä¸ªchunkçš„å†…å®¹ï¼ˆéœ€è¦æ‹¿idxå»data\processed\documents_cleaned.parquetæ‰¾textï¼‰ï¼Œç„¶åæ‹¿qwen3 embeddingç„¶åæ£€ç´¢ï¼Œé€‰å‡ºtop5ï¼ŒæŸ¥æ‰¾å¯¹åº”textï¼Œæ‰“å°å‡ºæ¥ï¼Œç»“æœä¸Šç†è®ºä¸Štop1å°±æ˜¯chunkæœ¬èº«ï¼Œå…¶ä»–å¤§æ¦‚ç‡ä¹Ÿå±äºåŒä¸€doc

check_retrieve_simple.pyå¡åœ¨Fetching 2 files:   0%|                                                                           | 0/2 [00:00<?, ?it/s]äº†




æ£€ç´¢é˜¶æ®µï¼Œå°†Qwen3 0.6B embeddingå’ŒBM25å„æœ200æ¡ï¼Œç„¶å
**Paragraph Boosting**

* å¦‚æœå¤šä¸ª chunk æ¥è‡ªåŒä¸€ paragraph/source, apply small bonus: `score += 0.05 * count_same_paragraph`
betaï¼ˆæ®µè½å¢å¼ºç³»æ•°ï¼‰ï¼š0.05 ~ 0.15ï¼ˆè¶Šå¤§è¶Šæ¿€è¿›ï¼‰
å†é€šè¿‡RRFèåˆæ£€ç´¢ç»“æœ k=60

ä½¿ç”¨

**MMRï¼ˆå»é‡ï¼‰**

* é€‰å– topN æ—¶ç”¨ MMRï¼ˆÎ»=0.6~0.8ï¼‰
Î»ï¼ˆrelevance vs diversity æƒè¡¡ï¼‰: 0.6 ~ 0.8ï¼ˆè¶Šå 1 è¶Šé‡ç›¸å…³æ€§ï¼‰

reranker topN è¾“å…¥ï¼šN=80~200

æœ€ç»ˆé€‰ç»™ LLM çš„ chunk æ•° K_finalï¼š3~8ï¼ˆé€šå¸¸ 3~5 æœ€å¸¸ç”¨ï¼‰
æ£€ç´¢ï¼ˆRecallï¼‰

Dense: top 300~500  Qwen3 0.6B embedding

BM25: top 200  BM25

åˆå¹¶å paragraph boostingï¼šæŒ‡æ•°å‹ boosting

ğŸ”¥ èåˆï¼ˆRRFï¼‰

RRF k = 30~50

ğŸ”¥ MMRï¼ˆä¸€å®šåœ¨ RRF ä¹‹åï¼‰

Î» = 0.75~0.85

topN = 300ï¼ˆæä¾›ç»™ rerankerï¼‰

ğŸ”¥ Rerankerï¼ˆCross-Encoderï¼‰

è¾“å…¥ 200~300 æ¡

ä½¿ç”¨æœ€å¼ºçš„ cross-encoderï¼ˆå¦‚ BGE-Reranker-large æˆ– Jina Reranker v2ï¼‰

ğŸ”¥ è¾“å‡ºç»™ LLM

æœ€ç»ˆé€‰ 3~5 ä¸ª chunkï¼ˆæ ¹æ® reranker åˆ†æ•°ï¼‰


src\retrieval\config.yamlä¸åº”è¯¥é‡æ–°åˆ›å»ºï¼Œæ–°çš„å‚æ•°åº”è¯¥æ”¾è¿›é¡¹ç›®ç›®å½•config.yamlï¼Œ
ä¸ºä»€ä¹ˆsrc\retrieval\dense_retrieval.pyä¸å»å¤ç”¨src\embedding\embedding_qwen.pyä»£ç 
è®°ä½ï¼Œæˆ‘ç”¨çš„æ˜¯Qwen/Qwen3-Embedding-0.6Bè€Œä¸æ˜¯ Qwen2
ä»config.yamlè¯»å–å¿…è¦å‚æ•°ï¼Œå¹¶å°†å‚æ•° ä¾‹å¦‚ RRFçš„k paragraph boostingç­‰å‚æ•°æš´éœ²åœ¨config.yaml

åœ¨test_retrieval_pipeline.pyè„šæœ¬ä¸­å¯ç”¨ rerankerï¼Œæ³¨æ„
rerankeréœ€è¦é‡å†™ï¼Œå› ä¸ºæˆ‘æ¢æ¨¡å‹äº†ï¼Œä¸å†ä½¿ç”¨ggufé‡åŒ–ï¼Œè€Œæ˜¯jinaai/jina-reranker-v3,å…¶å¼•ç”¨æ–¹å¼å’Œggufæ¨¡å‹ä¸åŒï¼Œå› æ­¤ä½ éœ€è¦é‡å†™src\rerank\jina_reranker.py
from transformers import AutoModel

model = AutoModel.from_pretrained(
    'jinaai/jina-reranker-v3',
    dtype="auto",
    trust_remote_code=True,
)
model.eval()

Rank documents:

query = "What are the health benefits of green tea?"
documents = [
    "Green tea contains antioxidants called catechins that may help reduce inflammation and protect cells from damage.",
    "El precio del cafÃ© ha aumentado un 20% este aÃ±o debido a problemas en la cadena de suministro.",
]

# Rerank documents
results = model.rerank(query, documents)

# Results are sorted by relevance score (highest first)
for result in results:
    print(f"Score: {result['relevance_score']:.4f}")
    print(f"Document: {result['document'][:100]}...")
    print()
    
ä½ çš„rerankeréƒ¨åˆ†è‚¯å®šæœ‰é—®é¢˜ï¼Œä¸€æ¬¡queryè¦è·‘å‡ åç§’ç„¶åçˆ†æ˜¾å­˜ï¼Œè¿™æ‰0.6bæ¨¡å‹

config = Config()
        reranker_config = config.get("reranker")
        self.device = reranker_config.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = reranker_config.get("batch_size", 16)
        self.use_parent_chunk = reranker_config.get("use_parent_chunk", True)


 cfg = Config()
        # Load model
        self.model = AutoModel.from_pretrained(
            model_id,
            dtype=dtype,
            trust_remote_code=trust_remote_code,
            batch_size=8
        )æŠŠå‚æ•°æš´éœ²åˆ° yaml çš„rerankerä¸‹
ä½ çš„test_retrieval_pipeline.pyæ˜¯ä¸æ˜¯æ²¡æœ‰å¤ç”¨å·²æœ‰çš„æ£€ç´¢pipelineï¼Ÿ
åˆ†ææ€§èƒ½ç“¶é¢ˆjina_reranker.py


1.BM25ä¸å†é‡‡ç”¨jiebaåˆ†è¯å™¨ï¼Œè€Œæ˜¯ä½¿ç”¨
qwen:
  model_id: "ISTA-DASLab/Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP"
ä¹Ÿå°±æ˜¯æœ€åLLMçš„tokenizer
2.åˆå‡ºbugäº†

1.æˆ‘å‘ç°ISTA-DASLab/Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMPç”¨çš„tokenizerå’ŒQwen/Qwen3-Embedding-0.6Bæ˜¯åŒä¸€ä¸ªï¼Œæ‰€ä»¥å¯ä»¥æŠŠQwen3-Embedding-0.6Bçš„tokenizerç»™BM25ç”¨
2.è¿™æ˜¯ä»€ä¹ˆé—®é¢˜

å…ˆåŠ è½½Qwen3-Embedding-0.6Bï¼ŒBM25çš„tokenizerç›´æ¥ä»Qwen3-Embedding-0.6Bå¼•å…¥ï¼Œä¸è¦å†æ¬¡åŠ è½½äº†

ä½ æ˜¯ä¸æ˜¯æ¯æ¬¡éƒ½æ˜¯ä¸´æ—¶ç”¨bm25åˆ†è¯ï¼Œè¿™äº›ç»“æœåº”è¯¥è¦æŒä¹…åŒ–æ”¾å…¥data\processed
è€Œä¸”æˆ‘éœ€è¦ä½ æŠŠbm25çš„é€»è¾‘ä»src\retrieval\retrieval_pipeline.pyåˆ†ç¦»åˆ°src\retrieval\bm25.pyï¼Œè§£è€¦
å¤§æ¦‚è¦å­˜å‚¨è¿™äº›æ•°æ®
æ•°æ®	å¿…é¡»ï¼Ÿ	ä½œç”¨
tokenized_chunks	âœ”	ç”¨äºç»Ÿè®¡ TF å’Œ doc_len
chunk_len âœ”	BM25 çš„
avgdl	âœ”	BM25 é•¿åº¦å½’ä¸€åŒ–
idf_table (DF)	âœ”	BM25 æƒé‡ï¼ˆIDFï¼‰
vocab / inverted_index	âœ”	é«˜é€ŸæŸ¥æ‰¾å“ªäº› chunk åŒ…å«æŸè¯
è¿™äº›æ•°æ®å¯ä»¥æ ¹æ®chunks.parquet å’Œ documents_cleaned.parquetå¾—å‡ºï¼Œä»chunksçš„indexæŸ¥æ‰¾textå†åˆ†è¯

æˆ‘å‘ç°data\processed\bm25_index.pklæ˜¯pklæ ¼å¼ï¼Œå¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬è½¬åŒ–æˆ parquetæ ¼å¼ï¼Œä¸”ä¿®æ”¹bm25.pyä»£ç ï¼Œè®©ä»–ä»¥åç”Ÿæˆ parquetæ ¼å¼çš„ç´¢å¼•

ä¸ç”¨è€ƒè™‘pklæ ¼å¼ï¼Œæˆ‘ä»¥ååªä¼šä½¿ç”¨parquetæ ¼å¼ï¼Œå¦å¤–æˆ‘å¸Œæœ›bm25æŠŠè¶…å‚æ•°æš´éœ²åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„config.yaml
æˆ‘ç°åœ¨éƒ½ä¸çŸ¥é“ä¿å­˜ç´¢å¼•çš„è·¯å¾„æ—¶å“ªé‡Œå†³å®šçš„


ç°æœ‰çš„æ£€ç´¢pipelineå¤ªæ…¢äº†ï¼ŒæŒ‰å¦‚ä¸‹æµç¨‹ä¿®æ”¹ï¼Œæ³¨æ„æ¨¡å—åŒ–ï¼Œä¸€ä¸ªæ–‡ä»¶ä»£ç æœ€å¥½ä¸è¶…è¿‡300è¡Œ
Step 1ï¼‰åŠ è½½æ¨¡å‹ï¼ˆembeddingï¼ŒLLMï¼Œrerankerï¼‰

ä¸€æ¬¡æ€§åŠ è½½ï¼Œæ•´ä¸ª RAG è¿‡ç¨‹ä¸­ä¸å¸è½½ã€‚

Step 2ï¼‰dense æ£€ç´¢ï¼ˆembedding + Faissï¼‰

Faiss On-Disk Index æˆ– Parquet mmap
â†’ è¿”å› topK chunck_ids

Step 3ï¼‰BM25 æ£€ç´¢ï¼ˆä» parquet è¯»å–å€’æ’è¡¨ï¼‰

é€šè¿‡ term æŸ¥è¯¢å€’æ’ postingsï¼Œè€Œä¸æ˜¯å…¨åŠ è½½ã€‚

Step 4ï¼‰paragraph boostingï¼ŒRRFåˆå¹¶å€™é€‰ topKï¼Œmmrå»é‡åï¼Œå–åˆ†æ•°æœ€é«˜çš„ reranker_input_k =20ä¸ªchunk id

Step 5ï¼‰ç”¨DuckDB ä» Parquet æŸ¥è¯¢ chunk æ–‡æœ¬

æŒ‰ doc_id æ‰¹é‡è¯»å–å¯¹åº”æ–‡æœ¬
æ–‡æœ¬ä¸å¸¸é©»å†…å­˜

Step 6ï¼‰reranker

åªå¯¹ top 20 æ–‡æœ¬åš rerank
ç„¶å LLM ç”Ÿæˆç­”æ¡ˆã€‚
â‘  Dense Embedding + Faiss ANNï¼ˆOn-Disk / mmapï¼‰ â†’ topK_dense

å­˜æˆ .faissindex

mmap only

è¿è¡Œæ—¶å†…å­˜å ç”¨éå¸¸å°

â‘¡ BM25 å€’æ’è¡¨ï¼ˆå‹ç¼© pkl / mmapï¼‰ â†’ topK_bm25

ç´¢å¼•ç»“æ„ï¼š

idf_table: dict[str, float]

postings: dict[str, list[int]]

chunk_len: list[int]

å…¨éƒ¨åºåˆ—åŒ–ï¼ˆç»Ÿä¸€ pkl æˆ–å¤šä¸ª pklï¼‰

è¿è¡Œæ—¶ï¼š

ä¸€æ¬¡æ€§ mmap åˆ°å†…å­˜

BM25 è®¡ç®—åªä½¿ç”¨å€’æ’ listï¼ˆæå¿«ï¼‰

â‘¢ Reranker éœ€è¦ chunk æ–‡æœ¬ â†’ DuckDB è¯»å– parquet

åªåœ¨è¿™ä¸€æ­¥ä½¿ç”¨ DuckDBï¼Œéå¸¸åˆç†ï¼š

chunk_id â†’ SELECT text/title/parent chunk

ä¸åŠ è½½æ•´ä¸ªæ–‡ä»¶

æŸ¥è¯¢æ¯«ç§’çº§

æˆ‘å¯¹BM25æŒä¹…åŒ–çš„ç´¢å¼•çš„æ–‡ä»¶æ ¼å¼åšå‡ºæ”¹å˜
æ–‡ä»¶	å†…å®¹	å»ºè®®æ ¼å¼	ç†ç”±
chunk_len	æ¯ä¸ª chunk é•¿åº¦	parquet	ç®€å•æ ‡é‡ï¼Œåˆ—å¼å‹ç¼©æå¥½
idf_table	token â†’ idf	parquet æˆ– pkl	å°å­—å…¸ï¼Œç”¨ parquet/pkl éƒ½ ok
inverted_index	token â†’ postings	pkl âœ”	postings æ˜¯ listï¼Œä¸è§„åˆ™ç»“æ„
tokenized_chunks	æ¯ä¸ª chunk çš„ token åˆ—è¡¨	pkl âœ”ï¼ˆå¼ºçƒˆï¼‰	åˆ—è¡¨åµŒå¥—ç»“æ„
metadata	chunk_id, start, end, doc_id	parquet	ç»“æ„åŒ–
è¿™å¯èƒ½éœ€è¦ä½ å†™ä¸€ä¸ªè„šæœ¬è¯»å–D:\4000_projects\1project\LLM\data\processed\bm25_indexç›®å½•ä¸‹çš„æ–‡ä»¶è¿›è¡Œè½¬åŒ–ï¼Œä¸”ä¿®æ”¹BM25 save indexçš„é€»è¾‘

æˆ‘åˆåšå‡ºæ”¹å˜äº†inverted_index tokenized_chunksæˆ‘å†³å®šè¿˜æ˜¯ç”¨parquetå­˜å‚¨ï¼Œå› ä¸ºparquetæ”¯æŒéƒ¨åˆ†è¯»å–ï¼Œç°åœ¨ä¸éœ€è¦ä½ å†™convertè„šæœ¬ï¼Œä½†éœ€è¦ä½ æŠŠBM25é‡Œçš„å­˜å‚¨å’Œè¯»å–ä»£ç æ”¹äº†
æ–‡ä»¶	å†…å®¹	æ˜¯å¦éœ€è¦ä¸€æ¬¡æ€§è¯»å…¥å†…å­˜ï¼Ÿ	åŸå› 
idf_table	token â†’ idf(float)	âœ” å¿…é¡»å…¨é‡è¯»å…¥ï¼ˆå¾ˆå°ï¼‰	åªå‡ ä¸‡ token çš„å­—å…¸ï¼Œå‡  MB
chunk_len / avgdl	æ¯ä¸ª chunk çš„ token é•¿åº¦	âœ” å¿…é¡»å…¨é‡è¯»å…¥ï¼ˆè¾ƒå°ï¼‰	BM25 å…¬å¼å¿…é¡»ç”¨é•¿åº¦
tokenized_chunks	æ¯ä¸ª chunk çš„ token list	âŒ ä¸éœ€è¦å…¨è¯»ï¼ˆå¯ mmap éƒ¨åˆ†è¯»ï¼‰	åªå¯¹ topK chunk ç®— f(t,D)
inverted_index	token â†’ posting åˆ—è¡¨ï¼ˆchunk_idsï¼‰	âŒ æŒ‰ token æŸ¥è¯¢å³å¯ï¼Œä¸è¦è¯»å…¨	åªå¯¹ query token ç”¨åˆ°
metadata	chunk â†’ å†…å®¹ï¼ˆç”¨äºæœ€ç»ˆè¾“å‡ºï¼‰	âŒ æŒ‰ chunk_id æŸ¥è¯¢	è¾“å‡ºç­”æ¡ˆæ—¶æ‰ç”¨
ç¤ºä¾‹ï¼ˆæŒ‰ token æŸ¥å€’æ’ï¼‰ï¼š
import pyarrow.dataset as ds
ds_index = ds.dataset("inverted_index.parquet")
table = ds_index.to_table(filter=ds.field("token") == "apple")

å¯¹src\retrieval\retrieval_pipeline.pyåšå‡ºä»¥ä¸‹ä¿®æ”¹
1.queryä»¥batchå½¢å¼ä¼ å…¥
2.æ£€ç´¢çš„æ¯ä¸ªæ­¥éª¤ï¼Œéƒ½è¦åˆ†åˆ«è®¡æ—¶ï¼Œä¾‹å¦‚ denseæ£€ç´¢å’ŒBM25æ£€ç´¢ï¼Œä»¥åŠrerankerï¼Œæ‰“å°å‡º è€—æ—¶ä»¥åŠå¤„ç†çš„queryæ•°é‡
3.æ³¨æ„ ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¦ torch.no_grad() æ¨¡å¼ï¼Œæˆ‘æ€€ç–‘rerankeræ¨¡å‹æ²¡æœ‰é‡‡ç”¨

æ£€ç´¢æ—¶éœ€è¦äºŒæ¬¡åˆ†batchï¼Œéœ€è¦åˆ†åˆ«è®¾è®¡batchsize
æ¨¡å—	æ˜¯å¦æ”¯æŒ batch	æœ€ä½³ batch å¤§å°	å†…å­˜/æ˜¾å­˜è€—è´¹
Qwen Embedding	âœ” æ”¯æŒ	é€šå¸¸ 16â€“128	å æ˜¾å­˜ï¼ˆå¤§ï¼‰
Faiss	âœ” æ”¯æŒ	è¶Šå¤§è¶Šå¥½ï¼ˆé€šè¿‡çŸ©é˜µåŒ–åŠ é€Ÿï¼‰	ä¸­ç­‰
BM25	âŒ æœ¬è´¨ä¸Šä¸æ”¯æŒå¤§ batch	1	å  CPU å†…å­˜ï¼ˆå°ï¼‰
Rerankerï¼ˆcross-encoderï¼‰	âœ” æ”¯æŒ	æå° batchï¼š1â€“8	æ˜¾å­˜å ç”¨éå¸¸å¤§
å¦å¤–æˆ‘åœ¨config.yamlä¸­å°†rerankerçš„å‚æ•°å½’äºretrievalï¼Œè¿™æ„å‘³ç€rerankerå‚æ•°è¯»å–éœ€è¦ä½œå‡ºä¿®æ”¹
æ€»çš„æ¥è¯´ï¼Œå°±æ˜¯æ•´ä¸ªpipelineã€qwen embeddingã€rerankeræœ‰ä¸åŒçš„batchsize
ä¸»è¦æ˜¯å¯¹embeddingçš„ä¿®æ”¹ï¼Œå› ä¸ºrerankeræœ¬æ¥å°±æ˜¯ä¸€ä¸ªqueryä¸€ä¸ªqueryå¤„ç†çš„
retrieval:
  # Dense Retrievalé…ç½®
  query_encoder:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    batch_size: 32
    max_length: 168
    device: null   # è‡ªåŠ¨é€‰æ‹© cuda/cpuï¼ˆå»ºè®®åœ¨æœ‰ GPU æ—¶è®¾ä¸º "cuda"ï¼‰
    dtype: float16 # åœ¨ CUDA ä¸Šç”¨ FP16 æå‡ååï¼›CPU å°†å›é€€ FP32

  # Dense ANN æ£€ç´¢
  dense:
    batch_size: 64
    top_k: 600             # Denseæ£€ç´¢çš„top-kï¼ˆå¯¹128ä¸‡åº“æ¨è600ï¼‰
ç°åœ¨embeddingçš„å‚æ•°ä¸ä»embedding:
  qwen3:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    max_length: 168
    device: null   # è‡ªåŠ¨é€‰æ‹© cuda/cpuï¼ˆå»ºè®®åœ¨æœ‰ GPU æ—¶è®¾ä¸º "cuda"ï¼‰
    dtype: float16 # åœ¨ CUDA ä¸Šç”¨ FP16 æå‡ååï¼›CPU å°†å›é€€ FP32
    index_path: "data/faiss/qwen3_fp16_ip.faiss"
    è¯»å–ï¼Œè€Œæ˜¯ä»æ£€ç´¢éƒ¨åˆ†çš„querty encoderè¯»å–ï¼Œä½ å¯ä»¥è‡ªå·±æ”¹åŠ¨ä½¿å¾—æ›´åŠ åˆç†
    å¤–éƒ¨è‡ªè¡Œå†³å®šqueryæœ‰å¤šå°‘ï¼Œå†…éƒ¨è‡ªå·±åˆ’åˆ†è‡ªå·±çš„ï¼Œæ³¨æ„é²æ£’æ€§ï¼Œå¤„ç†queryå°äºbatchsizeçš„æƒ…å†µ

ä½ æ€ä¹ˆæŠŠembedding:
  qwen3:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    index_path: "data/faiss/qwen3_fp16_ip.faiss"
çš„ä¸€äº›å‚æ•°ç»™åˆ é™¤äº†ï¼Œè¿™æ˜¯å¤„ç†æ‰€æœ‰chunksç”Ÿæˆç´¢å¼•çš„æ¨¡å‹ï¼Œå’Œquery encoderä¸å…±äº«å‚æ•°ï¼Œè™½ç„¶æˆ‘ç”¨çš„éƒ½æ˜¯qwen3 embeddingï¼Œä½†æ˜¯é€»è¾‘ä¸Šä¸å…±äº«ï¼Œæœªæ¥ä¹Ÿå¯èƒ½æœ‰ä¸åŒçš„

å¯¹src\script\retrieval\test_retrieval_pipeline.pyåšå‡ºæ”¹è¿›ï¼Œç°åœ¨æˆ‘éœ€è¦èƒ½å¤Ÿæ§åˆ¶ä¼ å…¥queryçš„æ•°é‡ï¼Œæˆ‘è¦æµ‹è¯•batchåŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼Œä»¥åŠæ•´ä¸ªpipelineçš„é€Ÿåº¦ï¼Œå¯èƒ½å¯ä»¥ç®€å•çš„ç”¨ç°æœ‰queryå¤åˆ¶ï¼Œä½†æˆ‘ä¹Ÿä¸ç¡®å®šä¼šä¸ä¼šæœ‰ç¼“å­˜ä»€ä¹ˆçš„å¹²æ‰°ï¼Œä½ è‡ªå·±åˆ¤æ–­

æˆ‘å‘ç°å¯¹æ•´ä¸ªchunksåšBM25éå¸¸æ…¢ï¼Œæ‰€ä»¥æˆ‘å‡†å¤‡BM25 ä¸è¦å¯¹æ•´ä¸ªåº“è·‘ï¼Œåªå¯¹ Dense çš„ topK å†åš re-score

è¿™æ˜¯æœ€å¼ºçš„ trickï¼š

dense_top600 â†’ å†åš BM25 é‡æ‰“åˆ†ï¼ˆrerankï¼‰
è¿™æ · BM25 åªè·‘ 600 æ¡ chunkï¼Œéå¸¸å¿«ã€‚
è¿™æ„å‘³ç€inverted_index.parquet	å€’æ’ç´¢å¼•	æœ€å¤§æ–‡ä»¶ï¼Œé€Ÿåº¦æœ€æ…¢	ä¸¢å¼ƒ (èŠ‚çœç©ºé—´)
å€’æ’ç´¢å¼•ä¸å†éœ€è¦äº†

paragraph boostingä»ç„¶è¢«éœ€è¦ï¼Œåœ¨RRFèåˆä¹‹åï¼Œè¾“å…¥MMRä¹‹å‰
RRFå’ŒMMR paragraph boostingä¹Ÿéœ€è¦è®¡æ—¶

æ³¨æ„ç°åœ¨rerankerçš„è¾“å…¥æ•°é‡ç”±  
mmr:
    lambda: 0.8             # ç›¸å…³æ€§æƒé‡ï¼ˆ0.8è¡¨ç¤ºå¼ºè°ƒç›¸å…³æ€§ï¼Œè½»åº¦å»é‡ï¼‰
    top_k: 20 
çš„top_kæ§åˆ¶ï¼Œæˆ‘å·²ç»æ”¹å¥½äº†ï¼Œåªæ˜¯é€šçŸ¥ä½ 
å¦å¤–è¿è¡Œsrc\script\retrieval\test_retrieval_pipeline.pyæ—¶ï¼Œæœ‰å…³äºrerankerçš„warning 

ä½ çš„ä¿®æ”¹æ–¹å¼é”™è¯¯ï¼Œæˆ‘çš„rerankerå·²ç»å½’åˆ°retirevalä¸‹äº†

BM25å¤ªæ…¢äº†ï¼Œä½ è‚¯å®šæ˜¯å»æ•´ä¸ªè¯»å– tokenized chunks å’Œinverted_index.parquetäº†ï¼Œå‚è€ƒå»ºè®®æ”¹è¿›ï¼Œç”±ä½ å†³å®šæ˜¯ç›´æ¥ä¼ å…¥textåœ¨çº¿åˆ†è¯ï¼Œè¿˜æ˜¯æŒ‰idä»ç£ç›˜è¯»å–tokenized chunks

src\script\retrieval\test_retrieval_pipeline.pyæŠ¥é”™äº†

æˆ‘éœ€è¦ä½ ä»¥
qwen:
  model_id: "ISTA-DASLab/Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP"
  device_map: "auto"
  max_new_tokens: 1           # åˆ†ç±»ä»…éœ€ 1 ä¸ª tokenï¼ˆ0 æˆ– 1ï¼‰
  gen_temperature: 0.0        # å…³é—­é‡‡æ ·
  do_sample: false
  trust_remote_code: true
åšä¸€ä¸ªç­”æ¡ˆç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥zeroshotå›ç­”ï¼Œä¹Ÿå¯ä»¥æå–hidden state è¿æ¥MLPè¿›ä¸€æ­¥LORAï¼Œå…·ä½“ä»£ç æ”¾åœ¨modelingé‡Œã€‚
é¦–å…ˆä»è¯»å–D:\4000_projects\1project\LLM\data\raw\kaggle-llm-science-exam\test.csvé—®é¢˜åŠå›ç­”ï¼Œå°†queryä¼ åˆ°æ£€ç´¢çš„piplineé‡Œï¼Œæ‹¼æ¥æˆcontextï¼Œæ³¨æ„æ˜¯æ‰¹å¤„ç†çš„ï¼Œå…ˆç”Ÿæˆæ‰€æœ‰çš„contextï¼Œç„¶ååŠ è½½LLMï¼Œå¼€å§‹æ¨ç†ï¼Œç„¶åç”Ÿæˆzero shotçš„pipelineï¼Œè®©æˆ‘ä½œä¸ºåŸºçº¿ï¼Œpromptå¯ä»¥å‚è€ƒ
# System Message (å¯é€‰ï¼Œå–å†³äºæ¨¡å‹æ˜¯å¦æ”¯æŒ)
SYSTEM_PROMPT = "You are a scientific expert answering multiple-choice questions based strictly on the provided context."

# User Prompt Template
LISTWISE_PROMPT = """
### Instructions:
You are participating in a high-level science exam.
Your task is to answer the following multiple-choice question.
You MUST derive your answer primarily from the provided "Context" sections.
If the answer is not directly in the context, use your scientific knowledge to deduce the most plausible answer consistent with the context.

### Context:
{context}

### Question:
{question}

### Options:
A: {option_a}
B: {option_b}
C: {option_c}
D: {option_d}
E: {option_e}

### Task:
1. Analyze the context and the question.
2. Evaluate each option (A, B, C, D, E) one by one. Explain why it is correct or incorrect based on the context.
3. Select the best single answer.

### Output Format:
Provide your reasoning first, then conclude with the final answer in the format: "Answer: X" (where X is A, B, C, D, or E).
"""

ä¸€ä¸ªé—®é¢˜ç”Ÿæˆç­”æ¡ˆå‡ åˆ†é’Ÿéƒ½æ²¡å‡ºç»“æœï¼Œæˆ‘è®°å¾—ä¹‹å‰zeroshotæ˜¯å¯è¡Œçš„ï¼Œä¼šä¸ä¼šæ˜¯å› ä¸ºè¦è¾“å‡ºçš„tokenå¤©é•¿äº†
æˆ‘å»ºè®®ä¿®æ”¹promptï¼Œå¹¶é™åˆ¶LLMçš„è¾“å…¥ä¸º1ä¸ªtokenï¼Œåªèƒ½è¾“å‡ºç­”æ¡ˆï¼Œä¾‹å¦‚
å¦å¤–æˆ‘è¦æ±‚å‚æ•°æš´éœ²åœ¨config.yamlä¸­ï¼ŒåŒ…æ‹¬éªŒè¯é›†è·¯å¾„
qwen:
  model_id: "ISTA-DASLab/Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP"
  device_map: "auto"
  max_new_tokens: 1           # åˆ†ç±»ä»…éœ€ 1 ä¸ª tokenï¼ˆ0 æˆ– 1ï¼‰
  gen_temperature: 0.0        # å…³é—­é‡‡æ ·
  do_sample: false
  trust_remote_code: true

  1.src\script\generation\zeroshot_pipeline.pyä¸­ï¼Œè¦æ±‚å‚æ•°ç»Ÿä¸€ä»config.yamlè¯»å–ï¼Œè€Œä¸æ˜¯å‘½ä»¤è¡Œ
  2.å¦å¤–æ¨¡å‹åˆ°åº•ä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢ï¼Œæ˜¯å› ä¸ºä½ å­˜åˆ°cpuä¸Šäº†å—
  3.src\modeling\qwen_generator.pyçš„extract_hidden_statesæ€ä¹ˆè¿™ä¹ˆå¤æ‚ï¼Œæˆ‘è®°å¾—æå–éšè—å±‚ï¼Œåªéœ€è¦æ¨¡å‹çš„ä¸€è¡Œè®¾ç½®å°±è¡Œäº†

å‚è€ƒmdä½¿å¾—LLMä»…ä»…è¾“å‡ºä¸€ä¸ª ABCDEä¸­çš„å­—æ¯ï¼Œè€Œä¸æ˜¯<think>è¿™ç§

å› ä¸º512tokençš„çˆ¶chunkå¯¹äºåç»­å¤ªå¤§äº†ï¼Œæˆ‘éœ€è¦è®¾è®¡ä¸€ä¸ªè„šæœ¬ï¼Œä½¿å¾—çˆ¶chunkä¸è¶…è¿‡256tokenï¼Œä»chunks.parquetä¸­è¯»å–æ•°æ®ï¼Œå¹¶ä»documents_cleaned.parquetè¯»å–textï¼Œæ ¹æ®charå¯¹åº”çš„256tokenæ•°ï¼Œé‡æ–°è°ƒæ•´parent çš„start å’Œendç´¢å¼•ï¼Œæ³¨æ„ä¸è¦ç”Ÿç¡¬chunkï¼Œæ³¨æ„è¯­ä¹‰å•å…ƒçš„å®Œæ•´æ€§ï¼Œåˆ†å‰²å¤„
æˆ‘å‘ç°ä½ çš„çˆ¶chunkç”Ÿæˆé€»è¾‘æœ‰é—®é¢˜ï¼Œè€Œä¸”æˆ‘æƒ³è°ƒæ•´çˆ¶chunkçš„å¤§å°äº†ï¼ˆä»512åˆ°256ï¼‰ï¼Œä½†æˆ‘ä¸èƒ½è°ƒèŠ‚å­chunkçš„åˆ’åˆ†ï¼Œå› ä¸ºä¸€æ—¦è°ƒèŠ‚å°±éœ€è¦é‡æ–°embeddingæ—¶é—´å¤ªé•¿ï¼Œæ‰€ä»¥ä½ é¢å¤–å†™ä¸€ä¸ªç”Ÿæˆçˆ¶äº²chunkçš„è„šæœ¬ï¼Œæ”¾åœ¨src\chunkingç›®å½•ä¸‹ï¼Œå¯ä»¥æ ¹æ®ä»¥ä¸‹è§„åˆ™ç”Ÿæˆ
ä¸“ä¸š RAG ç³»ç»Ÿçš„çˆ¶ chunkï¼ˆâ€œParent Documentâ€ï¼‰è¦æ±‚ï¼š

ä¸æ–‡æœ¬åŸæœ¬è¯­ä¹‰é«˜åº¦å¯¹é½

256 tokens å°½é‡å¯¹åº”è‡ªç„¶æ®µè½

ä¸éš child chunk æ»‘åŠ¨å˜åŒ–

1 ä¸ªçˆ¶ chunk è¦†ç›–å¤šä¸ªå­ chunkï¼ˆé€šå¸¸ 3~6 ä¸ªï¼‰

LLM çœ‹åˆ°çš„çˆ¶ chunkæ–‡æœ¬åº”è¯¥è‡ªç„¶ã€ä¸ç¢ã€ä¸é‡å¤

è€Œä½ çš„ç‰ˆæœ¬ï¼š

æ¯ä¸ªå­ chunk éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªå±€éƒ¨çˆ¶ chunk â†’ 20x é‡å¤

åˆ‡å‰²ä¸é¿å¼€å¥å·/æ¢è¡Œ

çˆ¶ chunk ä¸ç­‰äºè¯­ä¹‰å•å…ƒ

ä¸èŠ‚çœä¸Šä¸‹æ–‡ window

ä¸åˆ©äº rerank / LLM reading / context cohesion

æ‰€ä»¥â€”â€”
ğŸ‘‰ ä½ çš„ chunking æ–¹æ³•ä¸èƒ½ç”¨äºçœŸå®çš„ SOTA 2025 Parent Document Retrieverã€‚

âœ… æ­£ç¡®çš„åšæ³•ï¼ˆä½ åº”è¯¥ä¿®æ”¹æˆçš„é€»è¾‘ï¼‰
æ­£ç¡®åˆ’åˆ†æµç¨‹åº”è¯¥æ˜¯ä¸¤å±‚åˆ†ç¦»ï¼š
ç¬¬ä¸€å±‚ï¼šæŒ‰è¯­ä¹‰å•ä½åˆ‡çˆ¶ 256 tokensï¼‰

ç‹¬ç«‹äºå­ chunk è¿è¡Œï¼

è§„åˆ™ï¼š

ä½¿ç”¨æ–­å¥å™¨ï¼ˆæˆ– tokenizer åæ ‡ç‚¹ï¼‰

èšåˆå¥å­ç›´åˆ°çº¦ 512 tokens

åˆ‡å‰²ç‚¹å¿…é¡»é è¿‘å¥å·ã€æ¢è¡Œç­‰

çˆ¶ chunk æ•°é‡åº”è¿œè¿œå°‘äºå­ chunk
ç®—å‡ºäº†æ¯ä¸€ä¸ªçˆ¶chunkçš„å­—ç¬¦ç´¢å¼•èŒƒå›´ï¼Œå†ä»å¯¹åº”docé‡Œçš„chunksé‡ŒæŸ¥ï¼Œè°ƒèŠ‚chunksé‡Œçš„ç´¢å¼•


Blingfire åšå¥å­åˆ‡åˆ†ï¼ˆæ¨èç”¨äºä½ çš„ä»»åŠ¡ï¼‰,è€Œä¸æ˜¯for punctuation in ['.', '?', '!', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', ';', 'ï¼›']:
ğŸ”¥ ä¼˜ç‚¹

æå¿«ï¼ˆæ¯” spaCy å¿« 10ï½50 å€ï¼‰

çº¯ C++ æ ¸å¿ƒï¼ŒPython è°ƒç”¨å¼€é”€æå°

å·²ç»å†…ç½® sentence tokenizerï¼ˆä¸éœ€è¦è‡ªå·±å†™ regexï¼‰

ä¸ä¼šåƒ regex ä¸€æ ·è¯¯åˆ‡ â€œDr.â€ â€œe.g.â€ ç­‰ç¼©å†™

ğŸ“Œ ç¤ºä¾‹ä»£ç ï¼ˆä½ å¯ä»¥ç›´æ¥æ”¾è¿› ParentChunkerï¼‰
import blingfire

def split_sentences_blingfire(text: str):
    # ä½¿ç”¨ Blingfire å†…ç½®çš„å¥å­åˆ‡åˆ†å™¨
    # æ¯ä¸ªå¥å­ä¸€è¡Œ
    s = blingfire.text_to_sentences(text)
    return [sent.strip() for sent in s.split("\n") if sent.strip()]

é¦–å…ˆçˆ¶chunksåº”è¯¥ä¿å­˜ä¸ºparquet
å…¶æ¬¡ï¼Œæˆ‘è¦çš„ä¸æ˜¯ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œè€Œæ˜¯ç›´æ¥ä¿®æ”¹chunksé‡Œçš„çˆ¶chunkå­—ç¬¦ç´¢å¼•
è¦å°å¿ƒï¼Œè¿™é‡Œé¢çš„ç´¢å¼•æ—¶åŸºäºdata\processed\documents_cleaned.parqueté‡Œé¢çš„textçš„ï¼Œè¿™æ„å‘³ç€ä¸ºäº†ä¸€è‡´æ€§ï¼Œåœ¨ç”Ÿæˆçˆ¶chunkçš„è¿‡ç¨‹ä¸­ï¼Œä¸èƒ½å¯¹textä¿®æ”¹
åœ¨åˆ¤æ–­çˆ¶chunkå’Œå­chunkæ˜¯å¦åŒ…å«æ—¶ï¼Œå¯ä»¥å‚è€ƒ
hildâ†’parent æ˜ å°„æ—¶çš„æ›´ç¨³å¥ç®—æ³•ï¼ˆæ›¿ä»£ä¸­å¿ƒç‚¹æ³•ï¼‰

ä½ ç°åœ¨ç”¨ child_center ä¸æœ€å¤§ overlapï¼Œå¦‚æœä½ æ‹…å¿ƒè¾¹ç•Œï¼Œæ”¹ä¸ºï¼š

def map_child_to_parent(parent_chunks, child_positions):
    mapping = []
    for cstart, cend in child_positions:
        # é¦–é€‰ï¼šå®Œå…¨åŒ…å«
        found = False
        for pidx, p in enumerate(parent_chunks):
            if p['start_char'] <= cstart and cend <= p['end_char']:
                mapping.append(pidx); found=True; break
        if found: continue
        # æ¬¡é€‰ï¼šæœ€å¤§ overlap
        best, best_ov = None, 0
        for pidx, p in enumerate(parent_chunks):
            overlap = max(0, min(cend, p['end_char']) - max(cstart, p['start_char']))
            if overlap > best_ov:
                best_ov = overlap; best = pidx
        mapping.append(best if best is not None else -1)
    return mapping
æœ€åç»Ÿè®¡çˆ¶å­chunkçš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ•°é‡ï¼Œå¤šå°‘æ¯”ä¾‹å­chunkå®Œå…¨è¢«åŒ…å«ï¼Œå¹³å‡æ¯ä¸ªçˆ¶chunkåŒ…å«å‡ ä¸ªå­chunkè¿™ç§

å­chunkè¿›è¡Œåˆ‡åˆ†çš„æ—¶å€™æ²¡æœ‰ä½¿ç”¨tokenizerï¼Œæ‰€ä»¥


è¿›è¡Œæ•°æ®å¤„ç†ï¼Œå¯¹ data\raw\6000_train_examples.csvæŒ‰ç…§ 0.9è®­ç»ƒé›†ï¼Œ0.1éªŒè¯é›†åˆ’åˆ†ï¼ŒåŒæ—¶åŠ ä¸Šdata\raw\kaggle-llm-science-exam\test.csvä½œä¸ºéªŒè¯é›†