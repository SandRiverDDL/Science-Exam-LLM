åŸºäº è®¾è®¡.mdä¸ºæˆ‘å®Œæˆé¡¹ç›®ï¼Œæœ‰ä»¥ä¸‹è¦æ±‚ï¼š
1.ä½¿ç”¨config.yamlç®¡ç†å‚æ•°
2.ç›®å‰åªæœ‰ jinaai/jina-embeddings-v4-text-retrieval-GGUFå’Œjinaai/jina-reranker-v3-GGUF æ¨¡å‹ï¼Œå…¶ä»–embeddingæ¨¡å‹åç»­å†æ·»åŠ ï¼Œä¹Ÿå°±æ˜¯è¯´ç›®å‰åªæœ‰embeddingåªæœ‰ä¸€è·¯
3.faissä½¿ç”¨IndexFlatIP + float16 å­˜å‚¨
4.ä½¿ç”¨BM25è¾…åŠ©embeddingæ‰¾å›ï¼Œä½†ä¸ä½œä¸ºä¸»åŠ›æ£€ç´¢
5.ä¸åŒæ£€ç´¢æ–¹å¼å®ç°æ¨¡å—åŒ–ï¼Œå› ä¸ºæˆ‘è¦é€šè¿‡å®éªŒæ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹å¼çš„æ•ˆæœ
6.æˆ‘éœ€è¦å…ˆæ£€éªŒQwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMPèƒ½å¦æ­£å¸¸è¾“å‡ºæ–‡å­—ï¼Œç„¶åç”¨å®ƒåšzero-shotåˆ†ç±»ï¼Œæ¯æ¬¡æŠŠé—®é¢˜å’Œä¸€ä¸ªå›ç­”è¾“å…¥ï¼Œåªè¾“å‡º 0 æˆ– 1ï¼Œ0è¡¨ç¤ºå›ç­”é”™è¯¯ï¼Œ1è¡¨ç¤ºå›ç­”æ­£ç¡®ï¼Œæ³¨æ„KV cacheï¼Œæ¯æ¬¡çš„é—®é¢˜æ˜¯ç›¸åŒçš„ï¼Œzero-shotä½œä¸ºbaseï¼Œè·Ÿåç»­å®éªŒåšå¯¹æ¯”

æ„é€ ä¸€ä¸ªæµç¨‹ï¼Œä½¿å¾—zero shotçš„qwen3æ¨¡å‹ï¼Œä»train.csvè¯»å–é—®é¢˜å’Œé€‰é¡¹ï¼Œåƒsample_submission.csvä¸€æ ·è¾“å‡ºæ–‡ä»¶åˆ°outputæ–‡ä»¶å¤¹ï¼ŒåŒæ—¶æ‹¿é¢„æµ‹ç»“æœå’Œtrain.csvçš„answeråšå¯¹æ¯”ï¼Œå¹¶è¾“å‡ºå‡†ç¡®ç‡ï¼Œ
è¿™æ˜¯å•é€‰é¢˜ï¼Œ
æ³¨æ„æ¨¡å—åŒ–ï¼Œ
promptå¯ä»¥å‚è€ƒï¼Œä½ ä¹Ÿå¯ä»¥æ”¹è¿›ï¼Œå–å†³äºä½ è‡ªå·±
You are a system that outputs ONLY one of A, B, C, D, E.
No explanation. No reasoning. Output must be exactly one letter.

Question:
{question}

Options:
A: ...
B: ...
C: ...
D: ...
E: ...

Answer (A/B/C/D/E):

set PYTHONPATH=%PYTHONPATH%;.\src
$env:PYTHONPATH="$env:PYTHONPATH;.\src"
$env:HUGGINGFACE_HUB_CACHE="D:\4000_projects\1project\LLM\data"

1. åŠ è½½è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›† CSV

   * å­—æ®µåŒ…æ‹¬ï¼šquestionã€options(A~E)ã€context æˆ–èƒŒæ™¯æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰ã€correct_answerï¼ˆè®­ç»ƒé›†ï¼‰

2.æ–‡æœ¬åˆ†å—ï¼ˆchunkingï¼‰
æ–‡æœ¬å†…å®¹æ¥è‡ª data\raw\articles
å¯¹æ‰€æœ‰åŸå§‹æ–‡æ¡£è¿›è¡Œ chunk å¤„ç†ï¼š

chunk_size = 256 tokens
chunk_overlap = 50 tokens

æ“ä½œæ–¹æ³•ï¼š

ä½¿ç”¨ langchain.text_splitter.RecursiveCharacterTextSplitter æˆ–ç­‰ä»·è‡ªå®šä¹‰ splitter

åˆ†å‰²æ¯æ®µæ–‡æœ¬ä¸º token çº§åˆ«çš„ chunks

æ¯ä¸ª chunk é™„å¸¦ metadataï¼š

chunk_id

doc_id

positionï¼ˆchunk çš„é¡ºåºï¼‰

3. å¯¹æ‰€æœ‰ chunks ç”Ÿæˆä¸‰ç§ embeddingï¼š

   * Jina-embeddings-v4 (text-retrieval)
   * BGE-m3 æˆ– bge-large-v1.5
   * e5-mistral-instruct
   å…·ä½“æ¥è¯´multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-baseï¼Œä¸”è¿™ä¸¤è€…æˆ‘å·²ç»ä¸‹è½½å¥½äº†ï¼Œæ³¨æ„Jina-embeddings-v4æ˜¯ä¸»åŠ›

4. ä¿å­˜ embedding åˆ° FAISS ç´¢å¼•

   * å¯¹æ¯ä¸ª embedding åˆ†åˆ«åˆ›å»ºä¸€ä¸ª IndexFlatIP (FP16)
   * ä¿å­˜ idâ†’chunk_textã€idâ†’metadata æ˜ å°„


ä½¿ç”¨qwen3çš„tokenizerè€Œä¸æ˜¯ä½ è‡ªå·±ç¼–å†™çš„ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹å®ç°
---

def tokenize_chunks(text, tokenizer, chunk_size=256, overlap=50):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    n = len(tokens)
    chunks = []
    start = 0
    position = 0

    while start < n:
        end = min(start + chunk_size, n)
        chunk = tokens[start:end]
        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)

        chunks.append({
            "text": chunk_text,
            "position": position,
        })

        if end == n:
            break
        start = end - overlap
        position += 1

    return chunks

    å¯¹æŸä¸ªcsvæ–‡ä»¶åˆ†å—æ—¶ï¼Œè¦æ‰“å° â€œæ­£åœ¨åˆ†å—æ–‡ä»¶ï¼šæ–‡ä»¶åâ€ä¾‹å¦‚data\raw\articles\3.csv
    embeddingéƒ¨åˆ†åŠ ä¸Še5 å’Œbge multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base
    æˆ‘çš„csvæ–‡ä»¶æ€»å¤§å°ä¸º17Gï¼Œä½ è‡ªå·±åˆ†æï¼Œæˆ‘32Gï¼ˆå¤§çº¦25Gï¼‰å†…å­˜ç©ºé—²æ˜¯å¦å¤Ÿç”¨ï¼Œå¦‚æœä¸å¤Ÿç”¨å¯èƒ½éœ€è¦ä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å¤„ç†

    æš‚æ—¶æ”¾å¼ƒjinaï¼ˆä½†ä¸è¦åˆ é™¤ç›¸å…³ä»£ç ï¼‰ï¼Œå› ä¸ºjinaå‚æ•°å¤ªå¤§
    åŒæ—¶å¼•å…¥bge-m3æ¨¡å‹ä½œä¸ºè¾…åŠ©æ£€ç´¢æ¨¡å‹ï¼Œç°åœ¨çš„æ£€ç´¢ä¸»åŠ›æ¨¡å‹æ˜¯[multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)
    âœ” ä¸»åŠ›æ¨¡å‹ï¼šmultilingual-e5-large-instructï¼ˆæœ€ç¨³ â†’ å†³å®šä½ å¾—åˆ†ä¸Šé™ï¼‰
    âœ” è¾…åŠ©æ¨¡å‹ï¼šbge-m3ï¼ˆæœ€ä½³äº’è¡¥ â†’ å¬å›æå‡æœ€å¤§ï¼‰
    âœ” ç¬¬ä¸‰ï¼šgte-multilingual-base

æˆ‘æ„Ÿè§‰éå¸¸æ…¢ï¼Œè¿™æ­£å¸¸å—ï¼Œæˆ‘å†³å®šå…ˆåªå¯åŠ¨gteå»ç”Ÿæˆå‘é‡ï¼Œå¿«é€ŸéªŒè¯ï¼Œå¹¶æ£€æŸ¥ä»¥ä¸‹é—®é¢˜ï¼š
ä½ å¯èƒ½åœ¨ç”¨ CPUï¼ˆGGUFï¼‰æˆ–å•çº¿ç¨‹ tokenization â†’ ä¼šæŠŠé¢„è®¡é€Ÿåº¦ç åˆ° 1/10 æˆ–æ›´ä½ã€‚

æ²¡æœ‰æ‰¹é‡åŒ–ï¼ˆbatch=1 æˆ–æå° batchï¼‰ â†’ å®é™…ååç‡æ€¥å‰§ä¸‹é™ã€‚

tokenizer å•çº¿ç¨‹æˆ– Python-level decode/encode æˆä¸ºç“¶é¢ˆï¼ˆå°¤å…¶ç”¨ whitespace åˆ‡åˆ†æˆ– slow tokenizerï¼‰ã€‚

I/O ç£ç›˜è¯»å–æ…¢ï¼ˆå¤§é‡å°æ–‡ä»¶ï¼‰ â†’ CPU ç­‰å¾… I/Oã€‚

æ²¡æœ‰æŠŠæ¨¡å‹æ”¾åˆ° GPUï¼ˆæˆ–æ˜¾å­˜ä¸è¶³å¯¼è‡´é¢‘ç¹ CPU-GPU äº¤æ¢ï¼‰ã€‚

æ¡†æ¶/å®ç°é—®é¢˜ï¼šä¾‹å¦‚ç”¨ sentence-transformers ä½†æ²¡æœ‰å¯ç”¨ device='cuda' æˆ–æ²¡ä½¿ç”¨ torch.inference_mode()ã€‚

å¹¶è¡Œè¿›ç¨‹æœªå¯ç”¨ï¼ˆå»ºè®® tokenization ä¸ model inference åˆ†ç¦»å¹¶è¡Œï¼‰


ç¨‹åºå‡ºbugï¼Œä¼¼ä¹æ˜¯æç¤ºè¯´è¶…å‡ºäº†yieldçš„limitï¼Œæˆ‘æ²¡æœ‰æŠ¥é”™é”™è¯¯æŠ¥å‘Š

å½“ç¨‹åº print æ­£åœ¨åˆ†å—æ–‡ä»¶ï¼šdata\raw\articles\1.csv\1.csvï¼Œåˆ°åº•åœ¨åšä»€ä¹ˆï¼Œæ˜¯åœ¨embeddingå—ï¼Ÿæ€»ä¸èƒ½åˆ†å—åˆ†è¿™ä¹ˆä¹…å§
èƒ½ä¸èƒ½å®æ—¶æ˜¾ç¤ºembeddingçš„è¿›åº¦ï¼Œæ¯”å¦‚å·²ç»å¤„ç†äº†å¤šå°‘ä¸ªchunkï¼Œå¤šå°‘MBæ–‡ä»¶ï¼ˆä¼°è®¡ï¼‰ï¼Ÿ

1.ç›®å‰åªç”¨gteæ¨¡å‹ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œå‚è€ƒè¿™ä¸ªæ–‡æ¡£è¿›è¡Œè¿›è¡ŒåŠ é€Ÿï¼Œå› ä¸ºæ¯ç§’100chunkså®åœ¨å¤ªæ…¢äº†
2.ä¸ºä»€ä¹ˆæˆ‘è·‘äº†è¿™ä¹ˆå¤šæˆ‘çœ‹ä¸åˆ°embeddingå‘é‡çš„è¾“å‡ºï¼Ÿ

1.[progress] flush=68 chunks=13056 bytes~11.00MB speed=265.5 chunks/sè¿™ä¸ªé€Ÿåº¦è¿˜æ˜¯åªæœ‰265chunksï¼Œè€Œä¸”æˆ‘batchsizeå®šå¤šè®¾å®šæˆ192ï¼Œå†é«˜å°±ä¼šè¶…å‡ºæ˜¾å­˜ï¼Œè€Œä¸”192æ˜¾ç¤ºçš„speedæ²¡æœ‰æ¯”64æ›´å¿«ï¼Œè€Œä¸”æˆ‘ä½“æ„Ÿæ„Ÿè§‰64åè€Œæ‰“å°å¾—æ›´å¿«ï¼Œè™½ç„¶æ˜¾ç¤ºçš„é€Ÿåº¦å·®ä¸å¤š
2.æˆ‘å¸Œæœ›è¿™ä¸€æ®µprintæ§åˆ¶æ‰“å°é¢‘ç‡ï¼Œå¤§æ¦‚5ç§’æ‰“å°ä¸€æ¬¡ï¼Œè€Œä¸”æ‰“å°ä¼šè¦†ç›–ä¸Šä¸€æ¬¡çš„æ‰“å°ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½æ‰“å°æ–°çš„ä¸€è¡Œï¼Œä¾‹å¦‚è¿™æ ·
def print_and_overwrite(text):
    """
    æ‰“å°æ–‡æœ¬ï¼Œå¹¶ç”¨ \r å°†å…‰æ ‡ç§»å›è¡Œé¦–ï¼Œç”¨äºè¦†ç›–ä¸Šæ¬¡è¾“å‡ºã€‚
    """
    # 1. æ‰“å°æ–‡æœ¬
    # 2. ä½¿ç”¨ end='\r' æ¥ä»£æ›¿é»˜è®¤çš„ '\n'
    # 3. å¦‚æœéœ€è¦æ¸…é™¤æ®‹ç•™ï¼Œå¯ä»¥å…ˆæ‰“å°è¶³å¤Ÿå¤šçš„ç©ºæ ¼
    print(f"\r{text}", end="")

    [progress] flush=50 chunks=9600 bytes~12.98MB speed=168.3 chunks/sTraceback (most recent call last):
    æ—¢ç„¶ç“¶é¢ˆåœ¨CPUä¾§ï¼Œé‚£èƒ½å¦ç¦»çº¿å°†æ–‡æ¡£å…¨è½¬åŒ–ä¸ºtokenï¼Œç„¶åå†è¾“å…¥embeddingæ¨¡å‹ï¼Ÿ
    æˆ‘å¢å¤§äº†ä¸€å€chunk_size_tokensï¼Œä½†ç›®å‰çš„speed*2å’Œä¹‹å‰å·®ä¸å¤š 265.5 chunks/s 

    [progress] flush=70 chunks=13440 bytes~18.60MB speed=166.0 chunks/s æ€ä¹ˆæ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ


    multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base BGE-m3

æˆ‘å†³å®šæ–°å¢BAAI/bge-small-en-v1.5æ¨¡å‹ä»¥å¿«é€Ÿç”Ÿæˆembeddingï¼Œä½ è‡ªå·±å†³å®šæ˜¯åœ¨bge-m3åŸºç¡€ä¸Šæ–°å¢è¿˜æ˜¯ç›´æ¥æ–°å¢ä¸€ä¸ªç´¢å¼•


é—®é¢˜1ï¼šle "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 325, in main   
    result = build_indices_streaming(
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 252, in build_indices_streaming
    flush()
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 209, in flush  
    if use_token_ids and buf_ids and hasattr(bge_small_emb, "embed_from_ids"):
UnboundLocalError: local variable 'buf_ids' referenced before assignment
é—®é¢˜2ï¼šCSVè¿­ä»£å¼‚å¸¸ data/raw/articles\4.csv\4.csv: field larger than field limit (131072)

æˆ‘è§‰å¾—bge smallä¹Ÿä¸å°ï¼Œæ‰€ä»¥åˆ‡æ¢åˆ°bge-m3æ¨¡å‹äº†ï¼Œä½†
1.æ€ä¹ˆè¿™ä¹ˆæ…¢ï¼Œæ‰50chunksæ¯ç§’
2.Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192)è¿™ä¸ªé”™è¯¯æ€ä¹ˆæ¥çš„ï¼Ÿ
Loaded text docs: 0 from data/raw/articles
Found CSV files: 38 from data/raw/articles
`torch_dtype` is deprecated! Use `dtype` instead!
[init] BGE device: cuda
[tokenize] ä½¿ç”¨ BGE çš„ tokenizer è¿›è¡Œåˆ†å—ï¼ˆç›´é€š token idsï¼‰
æ­£åœ¨åˆ†å—CSVï¼šdata\raw\articles\0.csv\0.csv
Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors
[progress] flush=3 chunks=384 bytes~0.08MB speed=51.4 chunks/sTraceback

1.æˆ‘å‡†å¤‡å°±é•¿æœŸä½¿ç”¨bge-smalläº†ï¼Œè¿˜æœ‰ä»€ä¹ˆåŠ é€Ÿembeddingçš„æ–¹æ³•å—ï¼Ÿ
2.æˆ‘å¸Œæœ›å®ç°æ¯éš”å¤šå°‘ä¸ªchunkå°±ç”Ÿæˆä¸€æ¬¡ç´¢å¼•ï¼ˆå¤§æ¦‚ååˆ†é’Ÿä¸€æ¬¡ï¼Ÿï¼‰ï¼Œå¹¶è®¾ç½®æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒä¸­æ–­ç»§ç»­

å‚è€ƒä»¥ä¸‹ï¼Œæœ¬é¡¹ç›®å¯ä»¥ç”¨æ¥åŠ é€Ÿembeddingå—
SentenceTransformers + FastTokenizer + multi-worker dataloader
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-small-en-v1.5", 
                            device="cuda",
                            trust_remote_code=True)

embeddings = model.encode(
    docs,
    batch_size=4096,          # å‘ç–¯ä¸€æ ·çš„å¤§ batch
    device="cuda",
    show_progress_bar=True,
    normalize_embeddings=True,
    convert_to_numpy=True,
    num_workers=8             # tokenizer å¤šçº¿ç¨‹

)

æˆ‘æ·»åŠ äº†ä¸€ä¸ªæ–°çš„embeddingæ¨¡å‹Qwen/Qwen3-Embedding-0.6B, æ›¿æˆ‘åŠ å…¥ä»£ç 
æˆ‘æƒ³çŸ¥é“æ˜¯å¦å¯ä»¥ç”¨å®ƒæ¥åŠ é€Ÿembedding

ä¸ºä»€ä¹ˆqwen3 0.6bè¿™ä¹ˆæ…¢ï¼Œæ‰[progress] flush=3 chunks=384 bytes~0.08MB speed=17.6 chunks/s
æ˜¯ä¸æ˜¯å› ä¸ºä»€ä¹ˆbugï¼Ÿ
æ˜¯ä¸æ˜¯æ²¡æœ‰å¯åŠ¨flashattention

- ç¡®ä¿åœ¨ GPU + FP16ï¼š
- embedding_qwen3.device: "cuda"
- embedding_qwen3.dtype: float16
- å¼€å¯ç›´é€š token-idï¼ˆé¿å…é‡å¤åˆ†è¯ï¼‰ï¼š
- pipeline.embed_from_tokens: true
- pipeline.embed_tokens_model: qwen3
- çœ‹æ—¥å¿—åº”å‡ºç°ï¼š [tokenize] ä½¿ç”¨ Qwen3 çš„ tokenizer è¿›è¡Œåˆ†å—ï¼ˆç›´é€š token idsï¼‰
- å¹³è¡¡åºåˆ—é•¿åº¦ä¸åˆ†å—å¤§å°ï¼š
- å°† embedding_qwen3.max_length: 256 ï¼ˆæˆ–ç•¥é«˜äºä½ çš„ chunk_size_tokens ï¼Œä¾‹å¦‚ 256/320ï¼‰ï¼Œé¿å… 512 çš„æ— è°“å¡«å……ã€‚
- è°ƒæ•´æ‰¹é‡ï¼š
- èµ·æ­¥ç”¨ pipeline.batch_size: 256 æˆ– 512 ï¼Œæœ‰ 16â€“24GB æ˜¾å­˜å¯è¯• 1024 ï¼›OOM å°±å›é€€ã€‚
- è®© PyTorch æ›´å¿«ï¼š
- CUDA ä¸Šé»˜è®¤å·²ç”¨ AMPï¼ˆè‡ªåŠ¨æ··ç²¾ï¼‰ã€‚å¯é¢å¤–å…è®¸ TF32ï¼šåœ¨å…¥å£è„šæœ¬åŠ  torch.backends.cuda.matmul.allow_tf32 = True ï¼ˆFP16 å·²è¶³å¤Ÿçš„è¯ï¼Œè¿™é¡¹å½±å“ä¸å¤§ï¼‰ã€‚
- å¦‚æœåªæƒ³â€œæ›´å¿«æ›´å°â€ï¼š
- ç›´æ¥ç”¨ bge-small-en-v1.5 åœ¨ GPUï¼Œååé€šå¸¸æ¯” Qwen3 0.6B æ›´é«˜ã€‚

use_fast

University of California-Los Angeles (Los Angeles, CA)
Harry
Smith
2004 7 7

ç›®å‰æˆ‘ä½¿ç”¨bge-smallçš„embeddingé€Ÿåº¦å¤§æ¦‚ä¸º600 chunksæ¯ç§’ï¼Œè¦embedding 17Gçš„æ•°æ®éœ€è¦åå‡ å°æ—¶ï¼Œå¸®æˆ‘åˆ†æä»¥ä¸‹æˆ‘çš„ç¨‹åºçš„é€Ÿåº¦ç“¶é¢ˆåœ¨å“ªé‡Œï¼Œå¯ä»¥é‡‡å–å“ªäº›æªæ–½

å®æ–½è¿™äº›ä¼˜åŒ–,ä½†éœ€è¦æ³¨æ„æˆ‘æ˜¯windowsç³»ç»Ÿï¼Œä½¿ç”¨å¤šè¿›ç¨‹è§£ç å¯èƒ½ä¼šå¡ä¸»ï¼Œ
å¦å¤–åœ¨build_chunks_and_indices.pyä¸­å¯ä»¥å°è¯•é‡æ„ï¼Œè¿™ä¸ªæ–‡ä»¶å¤ªå¤§äº†

1.build_chunks_and_indices.pyçš„æ–‡ä»¶è¿˜æ˜¯å¤ªå¤§äº†å¯èƒ½éœ€è¦é‡æ„
2.[progress] flush=22 chunks=45056 bytes~52.19MB speed=854.2 chunks/s emb=51% io=2%
è¿™ä¸ªé€Ÿåº¦å’Œæ—¶é—´å æ¯”æ€ä¹ˆæ ·ï¼Ÿ emb + ioåŠ èµ·æ¥ä¹Ÿæ‰53%
3.ä½ ç¡®å®šembedding_builder.pyå’Œindex_manager.pyé€‚åˆæ”¾åœ¨pipelineç›®å½•ä¸‹ï¼Œè€Œä¸æ˜¯ indexæˆ–è€…retrievelç›®å½•ä¸‹
4.æˆ‘å‡†å¤‡æš‚æ—¶æ”¹æˆä¸€åˆ†é’Ÿä¿å­˜ä¸€æ¬¡ï¼Œç„¶åï¼ˆ1ï¼‰æ£€æŸ¥ä¸­æ–­åèƒ½å¦çœŸçš„å†æ¬¡åŠ è½½
ï¼ˆ2ï¼‰å¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œåœ¨ä¿å­˜çš„faisså‘é‡é‡Œæ£€ç´¢0.csvé‡Œçš„textå†…å®¹ï¼Œçœ‹çœ‹ä¿å­˜çš„å‘é‡æ˜¯å¦çœŸçš„æœ‰æ•ˆ

1.csv0æ•°æ®çš„åœ¨ 'data/raw/articles/0.csv/0.csv',ä½ çš„è„šæœ¬ä¸­çš„åˆ—åä¼¼ä¹æœ‰é—®é¢˜
[æ ·æœ¬ 7] æŸ¥è¯¢æ–‡æœ¬ (å‰100å­—): 0X or 0-X ("zero/oh ex") may refer to:...
[æ£€ç´¢ç»“æœ] Top-5:
  [1] è·ç¦»=166672.0000 | chunk_id=0.8036060333251953 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [2] è·ç¦»=44025.0000 | chunk_id=0.7438042759895325 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [3] è·ç¦»=165743.0000 | chunk_id=0.681257426738739 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [4] è·ç¦»=44508.0000 | chunk_id=0.662455677986145 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [5] è·ç¦»=152025.0000 | chunk_id=0.6491485238075256 | doc_id=0.csv:row:0
      æ–‡æœ¬ç‰‡æ®µ: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
è¿™ç§æ£€ç´¢ç»“æœæ­£å¸¸å—
2.[progress] flush=6 chunks=12288 bytes~12.74MB speed=864.7 chunks/s emb=54% io=3%
æ–­ç‚¹é‡ç»­ä¹‹åï¼Œchunkså’Œbytesåªæ˜¾ç¤ºçš„æ˜¯æœ¬æ¬¡æ‰§è¡Œçš„embeddingçš„chunksï¼Œæˆ‘å¸Œæœ›è¿˜è¦æ˜¾ç¤ºçš„æ˜¯ç´¯è®¡çš„chunkså’Œbytes

[checkpoint] saved at chunks=38912 -> data/faiss/checkpoints/embedding_progress.json
D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/build_chunks_and_indices.py

D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/verify_index.py

æˆ‘çœ‹åˆ°embedding_progress.jsonä¸­"chunk_id": 38912, "flush_count": 19,ä½†bge_small_fp16_ip.faissæ–‡ä»¶æœ‰200MBï¼Œè¿™æ˜¯ä¸æ˜¯æ•°æ®ä¸Šä¸ä¸€è‡´äº†
æˆ‘å†æ¬¡è¿è¡Œembeddingæ—¶ï¼Œä¼¼ä¹åˆ†å—é€Ÿåº¦åªå‰©600chunksæ¯ç§’äº†ï¼Œä¸ºä»€ä¹ˆè¿˜å˜æ…¢äº†ï¼Ÿ
æ—¢ç„¶æˆ‘å·²ç»é‡‡ç”¨tokenizeræ·å¾„äº†ï¼Œé‚£æˆ‘é¢„å…ˆtokenizeræ‰€æœ‰csvæ•°æ®ï¼Œé‚£æ˜¯ä¸æ˜¯ä¼šæ›´å¿«ï¼Ÿ

æˆ‘éœ€è¦ä½ åšå‡ºæ”¹åŠ¨ï¼Œæ¯ä¸ªchunkå‰éƒ½è¦åŠ ä¸Šè¯¥chunkæ‰€å±æ–‡æ¡£çš„title
å¦‚ä½•æ‹¼æ ‡é¢˜ï¼Ÿç”¨ä»€ä¹ˆç¬¦å·æœ€åˆé€‚ï¼Ÿ
âš¡ æœ€æ¨èï¼ˆä¸šç•Œå…¬è®¤æœ€ä½³ï¼‰ï¼š
æ ¼å¼ï¼š"{title}\n\n{chunk_text}"

ä¹Ÿå°±æ˜¯ï¼š

æ ‡é¢˜
<ç©ºè¡Œ>
chunk å†…å®¹

ä¸ºä»€ä¹ˆç”¨æ¢è¡Œï¼Ÿ

Tokenizer å¯¹æ¢è¡Œæœ‰å¤©ç„¶åˆ†éš”æ•ˆæœ

ä¸å¼•å…¥é¢å¤–æ„ä¹‰ç¬¦å·ï¼ˆé¿å…è¯­ä¹‰è¯¯å¯¼ï¼‰

æ¨¡å‹ä¼šè®¤ä¸ºâ€œæ¢è¡Œ = æ–°æ®µè½â€

é«˜è´¨é‡ embedding å¸¸ç”¨æ ¼å¼ï¼ˆOpenAI, Cohere, Voyageï¼‰

ç›®å‰æˆ‘å·²ç»ç¡®å®šä½¿ç”¨ å¤ç”¨token idsçš„trickäº†ï¼Œå› ä¸ºå¤šè·¯embeddingçš„è€—æ—¶å¤ªé•¿ï¼Œæˆ‘ä¸èƒ½æ¥å—ã€‚å¦å¤–æˆ‘è¦æ±‚ä½ ä½¿ç”¨ä»¥ä¸‹trick
1.

æ€»æµç¨‹
ã€é˜¶æ®µ 0ï¼šé¢„å¤„ç†ã€‘
- æ–‡æ¡£çº§ embeddingï¼ˆdoc-embï¼‰
- æ–‡æ¡£çº§å®ä½“æå–ï¼ˆdoc-entityï¼‰
- åŠ¨æ€ chunking + chunk embeddingï¼ˆchunk-embï¼‰
- metadata å†™å…¥ parquet

ã€é˜¶æ®µ 1ï¼šquery å¤„ç†ã€‘
- query embedding
- query entityï¼ˆNERï¼‰

ã€é˜¶æ®µ 2ï¼šæ–‡æ¡£çº§å¬å›ï¼ˆfastï¼‰ã€‘
top_docs = ANN(doc_emb, top 1000)

ã€é˜¶æ®µ 3ï¼šæ–‡æ¡£å®ä½“é‡æ’åºï¼ˆcheapï¼‰ã€‘
for doc in top_docs:
    doc.score = 0.8 * embedding_score + 0.2 * entity_overlap

select doc_top_k = top 500â€“800

ã€é˜¶æ®µ 4ï¼šchunk-level å¬å›ã€‘
ä»è¿™ 500â€“800 æ–‡æ¡£çš„ chunk ä¸­ ANN å¬å› 1000â€“2000 chunkï¼ˆç”± bge-small å¤„ç†ï¼‰

ã€é˜¶æ®µ 5ï¼šäºŒé˜¶æ®µç²¾æ£€ã€‘
ç”¨ Jina V4 è®¡ç®— query ä¸ chunk ç²¾ç¡® embedding ç›¸ä¼¼åº¦
é€‰ top 100â€“200 è¿›å…¥ Reranker / LLM

ã€é˜¶æ®µ 6ï¼šLLM rerank + answerã€‘





## 0) æ•°æ®æ¸…æ´—ï¼ˆå¿…åšï¼Œé«˜å½±å“ï¼‰ä»¥åŠé¢„Tokenization 

ç›®æ ‡ï¼šå»æ‰åƒåœ¾ã€ç»Ÿä¸€é£æ ¼ã€å‡å°‘æ— ç”¨ chunksã€‚

**è¦ç‚¹ / æ“ä½œï¼ˆä¼˜å…ˆçº§é«˜ï¼‰**

1. **è¯­è¨€æ£€æµ‹**ï¼ˆåªä¿ç•™ç›®æ ‡è¯­è¨€enï¼‰

   * ç”¨ `langdetect` æˆ– `fasttext`ã€‚
2. **åªä¿ç•™ ASCII**ï¼ˆå» emojiã€ç½•è§ Unicodeï¼‰

   * regex: `re.sub(r'[^\p{Han}\p{Hiragana}\p{Katakana}\p{Hangul}\p{ASCII}]', '', text)` ï¼ˆéœ€ `regex` åº“ï¼‰
3. **ç§»é™¤ URL / HTML / æ§åˆ¶å­—ç¬¦**

   * URL regexã€`BeautifulSoup` å» htmlã€‚
4. **Unicode æ ‡å‡†åŒ–**ã€‚
*   **ä»£ç **ï¼š`import unicodedata; text = unicodedata.normalize('NFKC', text)`
*   **ä½œç”¨**ï¼šè§£å†³å…¨è§’/åŠè§’å­—ç¬¦æ··ä¹±ï¼ˆå¦‚ `ï¼‘ï¼’ï¼“` vs `123`ï¼Œ`ï¼¡` vs `A`ï¼‰ï¼Œä»¥åŠä¸åŒçš„ç©ºæ ¼ç¬¦å·ã€‚è¿™å¯¹äº Embedding å¯¹é½éå¸¸é‡è¦ã€‚
5. å°†å…¨éƒ¨æ–‡æ¡£textä½¿ç”¨bge_small tokenizeré¢„Tokenization 

6. **çŸ­æ–‡æœ¬è¿‡æ»¤**ï¼ˆå¤§å¤šæ•°å™ªéŸ³å‘ç”ŸäºæçŸ­æ–‡æœ¬ï¼‰

   * `min_text_tokens = 32`ï¼ˆæ¨èï¼‰



---

## 1) æ ‡é¢˜å¤„ç†ï¼ˆTitle cleaningï¼‰â€”â€”ä½ ç‰¹åˆ«å…³å¿ƒçš„éƒ¨åˆ†

**ç›®æ ‡**ï¼šåªæ‹¼æ¥â€œé«˜ä¿¡å™ªæ¯”â€æ ‡é¢˜ï¼Œå¹¶ä¸”é™åˆ¶é•¿åº¦ï¼Œé¿å…æ±¡æŸ“ embeddingã€‚

**æ­¥éª¤**

1. æ ‡é¢˜å™ªå£°æ£€æµ‹ï¼ˆæ ¸å¿ƒï¼‰** â€” å†³å®šæ˜¯å¦æ‹¼æ¥æ ‡é¢˜

   * åˆ¤å®šè§„åˆ™ï¼ˆç¤ºä¾‹ï¼‰ï¼š

     ```python
     def is_good_title(t):
         t=t.strip()
         if not t or len(t)<3 or len(t)>120: return False
         alnum_ratio = sum(c.isalnum() for c in t)/max(1,len(t))
         if alnum_ratio>0.8: return False
         if " " not in t and len(t)>6: return False
         if re.search(r'(file_|doc_|id_|^v\d+\.)', t.lower()): return False
         return True
     ```
   <!-- * é¢å¤–ï¼šç»Ÿè®¡ title ä¸­çš„è‹±æ–‡å•è¯æ¯”ä¸­æ–‡å­—ç¬¦æ¯”ç‡ï¼Œä½äºé˜ˆå€¼å°±åˆ¤ä¸ºåƒåœ¾ã€‚ -->
2. å»åœç”¨è¯ï¼ˆå¯é€‰ï¼Œä»…å½“ title å¾ˆé•¿æ—¶ï¼‰

   * English stopword: NLTK/stopwords
   * Chinese: è‡ªå®šä¹‰åœç”¨è¯è¡¨
   * ç¤ºä¾‹ï¼ˆPythonï¼‰ï¼š

     ```python
     from nltk.corpus import stopwords
     stops = set(stopwords.words('english'))
     def shorten_title(t, max_words=8):
         words=t.split()
         words=[w for w in words if w.lower() not in stops]
         return " ".join(words[:max_words])
     ```
3. é™é•¿ï¼ˆtokençº§ï¼‰ï¼š

   * `title_max_tokens = 8~32`ï¼ˆæ¨è 8-16 è‹±æ–‡å•è¯ / 16â€“32 token è§†æ¨¡å‹ï¼‰
   * ç”¨ tokenizer æˆªæ–­ï¼š`title_tokens = tokenizer.encode(title)[:title_max_tokens]`
4. æ­£åˆ™æ¸…æ´—ï¼ˆç§»é™¤å¸¸è§å™ªéŸ³ï¼‰ï¼š

   * remove serials: `re.sub(r'\b[A-Z0-9_-]{6,}\b','',title)`
   * remove leading/trailing punctuation: `title.strip(" -_:;.,")`

**æ‹¼æ¥å»ºè®®**

* å¦‚æœ `is_good_title` ä¸º Trueï¼š

  ```
  chunk_text_final = title_clean + "\n\n" + entities_line + "\n\n" + chunk_body
  ```
* å¦åˆ™ï¼Œä¸æ‹¼ titleã€‚

---
å¤„ç†åæŠŠç»“æœå†™åˆ° Parquetï¼ˆåˆ—ï¼šdoc_id, title, doc_ids, length_tokensï¼‰ä»¥ä¾¿åç»­é‡ç”¨ã€‚æ¯éš”å¤§æ¦‚256MBç”Ÿæˆä¸€æ¬¡ 1.Parquet   2.Parquet
å®ç°ä»¥ä¸Šè¦æ±‚ï¼Œä»£ç å¯ä»¥è€ƒè™‘æ”¾åœ¨processingæ–‡ä»¶å¤¹ä¸‹ï¼Œå¦‚æœä½ æœ‰éœ€è¦ç”Ÿæˆæ–°çš„æ–‡ä»¶å¤¹ä¹Ÿå¯ä»¥

æˆ‘å·²ç»å®‰è£…äº†beautifulsoup4 regex langdetectï¼Œä¸è¦ä½ è‡ªå·±å»å®ç°ï¼Œç”¨regexå»ç§»é™¤url
æ ‡é¢˜å¤„ç†å»ºè®®é‡Œæœ‰ä¸€ä¸ªæ˜¯é”™çš„ï¼Œä»–æåˆ°ç›´æ¥ç”¨å­—ç¬¦ä¼°è®¡tokenæ•°ï¼Œæˆ‘ä¸éœ€è¦ï¼Œç›´æ¥encodeå¹¶æ ‡é¢˜çš„idsï¼Œåç»­ç›´æ¥è€ƒè™‘åˆå¹¶
åŸºäºä¸¤ä¸ªå»ºè®®æ–‡ä»¶ï¼Œä¿®æ”¹ä»£ç ï¼Œæ³¨æ„ï¼Œæˆ‘å‡†å¤‡è¿›ä¸€æ­¥å°†åŠ¨æ€chunkingå’Œé¢„tokenizeç»“åˆï¼Œåœ¨ä¸€ä¸ªpiplineä¸­å®Œæˆä»csvæ–‡ä»¶ åˆ° chunksï¼Œå…·ä½“æ€ä¹ˆåšæˆ‘ä¼šåç»­å‘Šè¯‰ä½ ï¼Œä½†ä½ è€ƒè™‘é¢„ç•™æ¥å£ä½ç½®

å°†chunkingçš„æµç¨‹å¹¶å…¥ preprocess_dataï¼Œæ³¨æ„é‡‡ç”¨çˆ¶æ–‡æ¡£ç´¢å¼•ï¼Œå­æ–‡æ¡£chunksize 128ï¼Œæ— overlapï¼Œçˆ¶æ–‡æ¡£512
meta dataé‡Œéœ€è¦å­˜å‚¨
rerank_text	String: å·²æ‹¼æ¥ Titleã€‚ Reranker è¾“å…¥ã€‚ Reranker éœ€è¦æ–‡æœ¬æ¥åš Cross-Attention å¯¹æ¯”ã€‚
doc_id
start_idx, end_idx çˆ¶æ–‡æ¡£åœ¨åŸæ–‡æ¡£idsä¸­çš„èµ·å§‹å’Œç»ˆæ­¢index
chunk_id	Int/String	å‘é‡ç´¢å¼• ID	ä¸»é”®ã€‚
chunk_len	Int	å­ Chunk é•¿åº¦
è¿™æ„å‘³ç€ç¡®å®éœ€è¦æŠŠæ¸…æ´—ç»“æœç»“æœå†™åˆ° Parquetï¼ˆåˆ—ï¼šdoc_id, title, doc_ids, length_tokensï¼‰
è¿™æ ·åç»­æ‰èƒ½æ ¹æ® doc_id, start_idx, end_idx å¿«é€Ÿå®šä½åˆ°çˆ¶æ–‡æ¡£

1.è¿™ä¸ªç»ˆç«¯æç¤ºçš„è¶…å‡ºtokenæ•°ä¼šæœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
2.æˆ‘å¥½å¥‡ä½ åœ¨partqueé‡Œå­˜å‚¨idsæ˜¯ä»¥ä»€ä¹ˆæ ¼å¼ï¼Œæˆ‘å¬è¯´ np.uint16å¯ä»¥å‹ç¼©ç©ºé—´ï¼Œè¿˜å¯ä»¥æŒ‡å®šå‹ç¼©ç®—æ³•ZSTDï¼š é€Ÿåº¦è¾ƒå¿«ï¼Œå‹ç¼©ç‡æœ€é«˜ï¼ˆæ¨èï¼‰ã€‚
3.æˆ‘éœ€è¦ä½ åœ¨preprocess_dataå®ç° æ–­ç‚¹é‡ç»­æœºåˆ¶

å·²å¤„ç† 3,000 æ¡æ–‡æ¡£ï¼Œä¿ç•™ 526 æ¡è¿™ç§ä¿¡æ¯ï¼Œåä¸€å¤©è¦†ç›–å‰ä¸€æ¡ï¼Œä¸ç„¶å¤ªé•¿äº†
ä½ çš„tokenizeé€»è¾‘å¥½åƒæœ‰é—®é¢˜å•Šï¼ŒToken indices sequence length is longer than the specified maximum sequence length for this model (5947 > 512). Running this sequence through the model will result in indexing errors
æˆ–è€…è¯´è¿™åªæ˜¯ä¸€ä¸ªè­¦å‘Š
æ–­ç‚¹é‡ç»­æ—¶ï¼Œè®°å¾— â€œä¿ç•™ 3,381 æ¡â€è¿™ä¸ªæ•°é‡è¦å’Œä¹‹å‰çš„ç´¯åŠ 

å†™ä¸€ä¸ªè„šæœ¬ï¼ŒæŸ¥çœ‹ç°æœ‰çš„ docs_1.parquet chunks_1.parqueté‡Œé¢å­˜çš„æ˜¯ä»€ä¹ˆæ•°æ®ï¼Œå«åš check_parquet_data
æˆ‘å‘ç°äº†ä¸€ä¸ªå¾ˆä¸¥é‡çš„é—®é¢˜ï¼Œä¼¼ä¹ä½ æ–­ç‚¹é‡ç»­ä¹‹åä¼šé‡æ–°å¼€å§‹å†™å…¥chunks_1.parquetï¼Œè€Œä¸”é‡ç»­ä¹‹åè¿˜æ˜¯ä»0å¼€å§‹è®¡ç®— â€œä¿ç•™ x æ¡â€

ä½ é€šè¿‡ len(tokens)*4è¿™ç§æ–¹æ³•ä¼°è®¡å®Œå…¨æ˜¯é”™çš„ï¼Œé¦–å…ˆæˆ‘æ”¹æˆäº† uint16åå æ®ä¸¤å­—èŠ‚ï¼Œå…¶æ¬¡è¿˜æœ‰ zstdå‹ç¼©ï¼Œ
ç°åœ¨æˆ‘è®¾å®šå†™å…¥çš„é˜ˆå€¼æ˜¯ 1G
ç„¶åä½ éœ€è¦æ ¹æ®å®é™…æ–‡ä»¶å¤§å°å’Œå¤„ç†æ–‡æ¡£è¡Œæ•°æ¥ä¼°ç®—æ–‡ä»¶å¤§å°ï¼Œä¸€ä¸ªchunks.parquetçº¦ä¸º82MBï¼Œä¸€ä¸ªdocs.parquetçº¦ä¸º95MB

æ€ä¹ˆè¿1Gchunk docéƒ½è¾“å‡ºä¸äº†å°±å†…å­˜ä¸å¤Ÿäº†ï¼Œæ˜¯ä¸æ˜¯æ„å‘³ç€1Gå¤ªå¤§åº”è¯¥æ”¹æˆ512MB,è¿˜æ˜¯è¯´ä½ çš„ç¨‹åºæˆ–è®¡ç®—å…¬å¼å“ªé‡Œæœ‰é—®é¢˜
æ³¨ï¼šratioæˆ‘åæ¥å¾®è°ƒäº†ä¸€ç‚¹ï¼Œä½†åº”è¯¥ä¸å½±å“


## 5) Embeddingï¼ˆç”Ÿæˆå‘é‡ï¼‰

**åŸåˆ™**

* ä½¿ç”¨ HF backend + `embed_from_tokens` å¯è·å¾—æœ€é«˜é€Ÿåº¦ï¼ˆå‰æï¼šæ¨¡å‹æ”¯æŒç›´æ¥ id è¾“å…¥ï¼‰
* dtypeï¼š`float16`ï¼Œdevice=`cuda`
* batch_sizeï¼šå°½é‡å¤§ï¼ˆå—æ˜¾å­˜é™åˆ¶ï¼Œå¸¸è§ 1024~4096ï¼‰
* ç”Ÿæˆå¹¶ä¿å­˜ï¼š`id -> vector` å†™å…¥ FAISSï¼ˆæŒ‰æ‰¹å†™å…¥ï¼‰

---

## 6) FAISS / Indexï¼ˆæ£€ç´¢ï¼‰

**ç»“æ„**

* FAISS index: å­˜å‘é‡ + id
* metadata store (Parquet/SQLite/duckDB): id -> chunk_text, title

**Index å»ºè®®**

* IVF-FLAT æˆ– HNSW + å­˜å‚¨åŸå‘é‡ï¼ˆä¾è§„æ¨¡ï¼‰
* å»º index æ—¶å½’ä¸€åŒ–ï¼ˆL2/cosineï¼‰: `index = faiss.IndexFlatIP(...)` with normalized vectors
* å†™å…¥æ—¶ batch commit + periodic persist
æˆ‘ä¹‹å‰ä¹Ÿæœ‰embeddingä»£ç ï¼Œä¸è¿‡é‚£ä¸ªä»£ç å¯èƒ½è¦åºŸå¼ƒäº†ï¼Œç°åœ¨æˆ‘åªç”¨bge smallè¿›è¡Œembeddingï¼Œè€Œä¸”ç›´æ¥ä»parquetè¯»å–chunk idsè¿›è¡Œembedding

faissç´¢å¼•åªå­˜å‚¨chunk_idå’Œembeddingï¼Œå…¶ä»–metadataé€šè¿‡chunkidä»chunksçš„parquetæ–‡ä»¶ä¸­è¯»å–
faissç´¢å¼•é€šè¿‡LZ4 ç®—æ³•å‹ç¼©

1.è¿™æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Œå¤„ç†äº†400ä¸‡å‘é‡åçªç„¶ä¸­æ­¢äº† 
2.æˆ‘æƒ³åœ¨â€œå·²å¤„ç† 4,009,984 chunks | å·²åµŒå…¥ 4,009,984 å‘é‡ | é€Ÿåº¦ 637 vec/sâ€æ‰“å°ä¿¡æ¯ä¸­ï¼ŒæŒ‰ hh:mm:ssæ˜¾ç¤ºå½“å‰æ—¶é—´
3.è€Œä¸”é€Ÿåº¦æ€ä¹ˆè¶Šæ¥è¶Šæ…¢äº†ï¼Œä»1500 vec/såˆ°637vecs/s

è¯·å›ç­”.1.ä¿®æ”¹ä»£ç ä¹‹åï¼Œæˆ‘è¿˜èƒ½æ–­ç‚¹å†ç»­å—ï¼Œè¿˜æ˜¯è¦å½»åº•é‡æ–°å¼€å§‹ 2.æ¯60sè‡ªåŠ¨ä¿å­˜é—´éš”å¤ªçŸ­ï¼Œæ”¹ä¸º5åˆ†é’Ÿä¿å­˜ä¸€æ¬¡ 3.è¯·å›ç­”å‘é‡æ˜¯ä»¥fp16å½¢å¼å­˜å‚¨å—ï¼Ÿè¿˜æ˜¯fp32

æ€»å…±åº”è¯¥ç”±ä¸‰åƒä¸‡å‘é‡ï¼Œé‚£æˆ‘åº”è¯¥ä½¿ç”¨ä»€ä¹ˆç´¢å¼•ç»“æ„ ï¼Œæˆ‘æƒ³ä½¿ç”¨IVFScalarQuantizerï¼Œä½¿ç”¨fp16å‚¨å­˜å‘é‡

æˆ‘å‘ç°ä¸€ä¸ªå¾ˆä¸¥é‡çš„é—®é¢˜ï¼Œ1.csvåƒè¿™æ ·çš„æ–‡ä»¶é‡Œå‚¨å­˜çš„wikiæ•°æ®å¥½åƒå¯èƒ½ä¸æ˜¯ç§‘å­¦ä¸»é¢˜çš„ï¼Œå¸®æˆ‘å†™ä¸€ä¸ªè„šæœ¬ï¼Œè¯»å–è¿™ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥è°ƒç”¨build_chunks_and_indices.pyé‡Œçš„æ¸…æ´—æ–¹æ³•ï¼Œç­›é€‰å‡ºé•¿åº¦å¤§äº32å­—ç¬¦çš„ï¼Œæ‰“å°å‡ºå‰20è¡Œ

æˆ‘æ–°åŠ äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä¾‹å¦‚'data\raw\archive\0_to_25000.parquet'ï¼Œç”¨åŒæ ·çš„æ–¹æ³•å¤„ç†ï¼Œå¯èƒ½åªæ˜¯è¯»å–çš„æ–¹æ³•ä¸ä¸€æ ·ï¼Œæ³¨æ„parquetåŠ›æ•°æ®çš„åˆ—åæˆ‘ä¸ç¡®å®š

ç”±äºä¹‹å‰çš„csvæ–‡æ¡£æ•°æ®é‡Œå¾ˆå¤šä¸æ˜¯ç§‘å­¦ä¸»é¢˜çš„ï¼Œæ‰€ä»¥æˆ‘æ¢äº†ä¸€ä¸ªå¤§æ¦‚å‡ ç™¾MBçš„parquetæ ¼å¼çš„æ•°æ®é›†ï¼Œä¾‹å¦‚ 'data\raw\archive\0_to_25000.parquet',æ–‡ä»¶åŒ…å«çš„æ‰€æœ‰åˆ—: ['text', 'url', 'title'],ä½†æˆ‘å¸Œæœ›ä½ èƒ½ä¿ç•™å¯¹csvæ–‡ä»¶çš„è¯»å–èƒ½åŠ›é˜²æ­¢æœªæ¥æˆ‘åˆåˆ‡æ¢æ•°æ®é›†äº†ï¼Œå³å®ç°ä¸»è¦ä»£ç é€»è¾‘å’Œè¯»å–æ–‡ä»¶å½¢å¼çš„è§£è€¦
æ³¨æ„ï¼Œç”±äºæ•°æ®é‡å¤§å¤§å‡å°‘ï¼Œæˆ‘ä¸å†é‡‡ç”¨ å¤ç”¨idsçš„æŠ€å·§ï¼Œè¿™æ„å‘³ç€æ–°ç”Ÿæˆçš„ chunk.parquet é‡Œ
ç”±äºè¦ä½¿ç”¨å¤šä¸ªembedding modelè¿›è¡Œensembleï¼Œæ„å‘³ç€è¦å®ç°chunkå’Œtokenizerçš„è§£è€¦ï¼Œå¯ä»¥é‡‡å–å¦‚ä¸‹æ–¹å¼ï¼Œtokenizeré‡‡ç”¨ tiktoken
çˆ¶ 512 / å­ 128 token çš„å®ç°æ–¹æ³•
ğŸŸ© æ–¹æ³• 1ï¼ˆæ¨èï¼‰ï¼šç”¨â€œå¥å­åˆ—è¡¨ + æ»‘åŠ¨çª—å£â€å®ç° token é™åˆ¶ï¼ˆåŠ¨æ€çš„ï¼‰

æµç¨‹ï¼š

Step 1ï¼šå…ˆæŠŠæ–‡æ¡£æŒ‰å¥å­åˆ‡ï¼ˆåŸºäºå­—ç¬¦ï¼‰ï¼Œå¾—åˆ°ï¼š
sentence[i]  # æ¯ä¸ªå¥å­ç”¨ char_start/char_end å®šä¹‰

Step 2ï¼šåŠ¨æ€ç»„åˆå¥å­ç›´åˆ°è¾¾åˆ°çº¦ 128 tokensï¼Œå½¢æˆå­ chunk

ä½ å¯ä»¥è¿™æ ·åšï¼š

current_chunk = ""
for sent in sentences:
    if tokenizer(current_chunk + sent).len() > 128:
        yield current_chunk
        current_chunk = sent
    else:
        current_chunk += " " + sent

Step 3ï¼šçˆ¶ chunk = æ‹¼æ¥ N ä¸ªå­ chunkï¼Œç›´åˆ°çº¦ 512 tokens
æ³¨æ„

æ‰€æœ‰ chunk éƒ½ç”¨ å­—ç¬¦ offset ç®¡ç†æ–‡æœ¬ï¼Œä¸ä¾èµ– token offsetã€‚

embedding æ¨¡å‹ tokenize æ—¶è‡ªåŠ¨é€‚é…ã€‚

æœ‰é—®é¢˜å•Š
1.ä½ ç¡®å®šä½¿ç”¨äº†text_cleaner.pyå’Œtitle_cleaner.pyå®šä¹‰çš„æ–¹æ³•æ¥æ¸…æ™° text titleå—
2.parquetæ–‡ä»¶çš„æ•°æ®é‡Œï¼Œæ¯è¡Œæ•°æ®ä¼¼ä¹æ²¡æœ‰idï¼Œè¿™è¦æ±‚ä½ è‡ªè¡Œç”Ÿæˆid
3.æˆ‘è®¤ä¸ºchunkæ•°æ®é‡Œä¸ç”¨å­˜ rerank_textï¼Œè€Œä¸”è¦å­˜ title
4.ä½ ç¡®å®šä½ ä¿å­˜äº†æ¸…æ´—è¿‡äº†çš„docå—ï¼Œå¯¹åŸparquetæ•°æ®æ¸…æ´—ä¹‹åï¼ŒåŠ ä¸Šdoc_idï¼ˆå¦‚æœåŸæ¥æ²¡æœ‰idçš„è¯ï¼‰ï¼Œå†å®Œå…¨å¯ä»¥ä¸€èµ·å‚¨å­˜æˆä¸€ä¸ªæ–‡ä»¶ï¼ˆå› ä¸ºæ€»å…±æ‰å‡ ç™¾MBï¼‰

å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...
âœ… æ–‡æ¡£å·²ä¿å­˜: data/processed\documents_cleaned.parquet
   å¤§å°: 315.52 MB | è®°å½•æ•°: 130,926
âœ… Chunkså·²ä¿å­˜: data/processed\chunks.parquet
   å¤§å°: 23.67 MB | è®°å½•æ•°: 1,281,646
âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: data/processed\preprocess_stats.json

============================================================
å¤„ç†ç»Ÿè®¡:
  æ€»æ–‡æ¡£æ•°: 131,049
  ä¿ç•™æ–‡æ¡£: 130,926 (99.9%)
  æ€»chunks: 1,281,646
  å¹³å‡chunks/æ–‡æ¡£: 9.8

è¿‡æ»¤ç»Ÿè®¡:
  è¿‡çŸ­æ–‡æœ¬: 123
  æ ‡é¢˜è´¨é‡: 33



å‡†å¤‡è¿›è¡Œembeddingä»¥åŠæ„å»ºIndexFlatIPç´¢å¼•ï¼Œå› ä¸ºchunkåªæœ‰ä¸€ç™¾ä¸‡ä¸”æµ‹è¯•é›†ä¸­queryæ•°é‡å°‘ï¼Œæ‰€ä»¥ä½¿ç”¨IndexFlatIPå–å¾—æœ€å¤§ç²¾åº¦
* ä½¿ç”¨ HF backend + `embed_from_tokens` å¯è·å¾—æœ€é«˜é€Ÿåº¦ï¼ˆå‰æï¼šæ¨¡å‹æ”¯æŒç›´æ¥ id è¾“å…¥ï¼‰
* dtypeï¼š`float16`ï¼Œdevice=`cuda`
* batch_sizeï¼šå°½é‡å¤§ï¼ˆå—æ˜¾å­˜é™åˆ¶ï¼Œå¸¸è§ 1024~4096ï¼‰
* ç”Ÿæˆå¹¶ä¿å­˜ï¼š`id -> vector` å†™å…¥ FAISSï¼ˆæŒ‰æ‰¹å†™å…¥ï¼‰

---

## 6) FAISS / Indexï¼ˆæ£€ç´¢ï¼‰

**ç»“æ„**

* FAISS index: å­˜å‘é‡ + id
* metadata store (Parquet/SQLite/duckDB): id -> chunk_text, title

**Index å»ºè®®**

* faissä½¿ç”¨IndexFlatIPç´¢å¼•
* å»º index æ—¶å½’ä¸€åŒ–ï¼ˆL2/cosineï¼‰: `index = faiss.IndexFlatIP(...)` with normalized vectors
* å†™å…¥æ—¶ batch commit + periodic persist
faissç´¢å¼•åªå­˜å‚¨chunk_idå’Œembeddingï¼Œå…¶ä»–metadataé€šè¿‡chunkidä»chunksçš„parquetæ–‡ä»¶ä¸­è¯»å–
faissç´¢å¼•é€šè¿‡LZ4 ç®—æ³•å‹ç¼©
ä½ å¯ä»¥å‚è€ƒsrc\pipeline\build_embeddings.pyï¼Œå¤§éƒ¨åˆ†ä»£ç éƒ½åº”è¯¥æ˜¯ç›¸åŒçš„ï¼Œä¸»è¦æœ‰ä»¥ä¸‹åŒºåˆ«
1.faissä½¿ç”¨IndexFlatIPç´¢å¼•
2.embeddingæ¨¡å‹è¯»å–å¾—ä¸å†æ˜¯idsï¼Œè€Œæ˜¯ä»chunkä¸­æŸ¥è¯¢åˆ°å¼€å§‹ç»“æŸçš„å­—ç¬¦ç´¢å¼•åï¼Œä»documents_cleaned.parquetè¯»å–text
3.ç›®å‰å…ˆé‡‡ç”¨ qwen3 0.6B embedding æ¨¡å‹è€Œä¸æ˜¯bge smallï¼Œåç»­å¯èƒ½æ·»åŠ æ–°çš„æ¨¡å‹ï¼Œå»ºè®®æ¨¡å‹è·å–å’Œå¤„ç†è§£è€¦ï¼Œæ–¹ä¾¿åç»­ç›´æ¥æ›¿æ¢æ¨¡å‹

1.æˆ‘å‘ç°ä½ è¿™äº›æ¨¡å‹å‚æ•°ä¸æ˜¯è¯»å–yamlæ–‡ä»¶çš„ä¸å¯¹å§ï¼Œè¿™äº›å‚æ•°åº”è¯¥ä»yamlæ–‡ä»¶è¯»ï¼Œè€Œæˆ‘æ§åˆ¶çš„æ˜¯é€‰æ‹©å“ªä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚æˆ‘ä¼ å…¥qwen3ï¼Œä½ å°±æ˜¯ä½¿ç”¨ config[embedding_qwen3]çš„å‚æ•°

1.yamlä¸­å‚æ•°çš„ç»“æ„å‘ç”Ÿäº†å˜åŒ–ï¼Œä¾‹å¦‚qwenç°åœ¨æ˜¯è¿™æ ·çš„
embedding:
  qwen3:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    max_length: 168
    device: null   # è‡ªåŠ¨é€‰æ‹© cuda/cpuï¼ˆå»ºè®®åœ¨æœ‰ GPU æ—¶è®¾ä¸º "cuda"ï¼‰
    dtype: float16 # åœ¨ CUDA ä¸Šç”¨ FP16 æå‡ååï¼›CPU å°†å›é€€ FP32
    index_path: "data/faiss/qwen3_fp16_ip.faiss"
æˆ‘å»ºè®®å°†æ‰€æœ‰æ¶‰åŠmodelçš„å‚æ•°æ”¾å…¥yamlè€Œä¸æ˜¯ç¡¬ç¼–ç 
2.è§£å†³æŠ¥é”™

embedingè¿‡ç¨‹printä¿¡æ¯ï¼Œä¾‹å¦‚ 
æ¯ç§’å¤„ç†å¤šå°‘chunksï¼Œ
æ€»å…±å¤„ç†å¤šå°‘chunks/chunksæ€»æ•°(çº¦128ä¸‡),
å·²ç»å¤„ç†çš„æ—¶é—´ï¼Œ
é¢„è®¡è¿˜éœ€è¦å¤šå°‘æ—¶é—´
æ³¨æ„ \r é˜²æ­¢æ‰“å°å¤ªå¤šè¡Œ
è¿˜æœ‰æ¯æ¬¡å†™å…¥ä¹Ÿè¦æ‰“å°ä¿¡æ¯ï¼Œæ³¨æ„åˆ«è¢« \rè¦†ç›–äº†ï¼Œå†™å…¥è®°å¾—æ ‡æ³¨æ—¶é—´

embeddingå¤ªæ…¢äº†ï¼Œæˆ‘æ€€ç–‘ä½ åœ¨é€æ¡å¤„ç†ï¼Œå‚è€ƒä»¥ä¸‹ä»£ç è¿›è¡ŒåŠ é€Ÿ
ä¸‰è€…ç»„åˆåçš„é€Ÿåº¦å¤§æ¦‚å¦‚ä½•ï¼Ÿ

å…¸å‹æ•ˆæœï¼š

æ–¹æ³•	ç›¸å¯¹é€Ÿåº¦
åŸå§‹ Python for-loop	1Ã—
Arrow-based æ‹¼æ¥	3â€“6Ã—
tokenizer multiprocessing / Fast tokenizer	4â€“10Ã—
batched pipelineï¼ˆé¿å…é€æ¡å¤„ç†ï¼‰	5â€“15Ã—
ä¸‰è€…å åŠ 	30ï½80Ã— æå‡ï¼ˆçœŸå®å¯è¾¾ï¼‰

å¦‚æœä½ æœ‰å‡ ç™¾ä¸‡æ–‡æœ¬ï¼Œè¿™ä¸ªå·®è·æ˜¯éå¸¸å·¨å¤§çš„ã€‚

ğŸ§© ç»™ä½ çœ‹ä¸€ä¸ªæœ€ç»ˆçš„æ•´åˆç¤ºä¾‹ï¼ˆæ ¸å¿ƒç»“æ„ï¼‰
import pyarrow as pa
import pyarrow.parquet as pq
from multiprocessing import Pool
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("your-model", use_fast=True)

def tokenize_batch(batch_texts):
    out = tokenizer(batch_texts, padding=False, truncation=False)
    return out["input_ids"]

def process_batches(all_texts, batch_size=2048):
    batches = []
    for i in range(0, len(all_texts), batch_size):
        batches.append(all_texts[i:i+batch_size])
    return batches

all_batches = process_batches(doc_texts)

with Pool(16) as p:
    tokenized_batches = p.map(tokenize_batch, all_batches)

tables = []
for batch_input_ids in tokenized_batches:
    arr = pa.array(batch_input_ids, type=pa.list_(pa.int32()))
    table = pa.Table.from_arrays([arr], names=["input_ids"])
    tables.append(table)

final_table = pa.concat_tables(tables)
pq.write_table(final_table, "tokens.parquet")


åˆ†æç»“æœ:
==================================================
å¹³å‡CPUä½¿ç”¨ç‡: 7.1%
å¹³å‡GPUä½¿ç”¨ç‡: 99.3%

ğŸŸ¢ ç»“è®º: GPUç“¶é¢ˆ
   CPUå‡†å¤‡æ•°æ®é€Ÿåº¦å¤Ÿå¿«
   å»ºè®®: å¢å¤§batch_sizeã€ä½¿ç”¨æ›´å¤§æ¨¡å‹
ä¼šä¸ä¼šæ˜¯å› ä¸ºCPUæ˜¯å•è¿›ç¨‹å¯¼è‡´åˆ©ç”¨ç‡ä¸Šä¸å»



æˆ‘å°†ä»£ç è½¬ç§»åˆ°äº†linuxæœåŠ¡å™¨ä¸Šï¼Œå¹¶å®‰è£…äº†  faiss-gpuå’Œ flash-attnï¼ŒåŒæ—¶å¯ä»¥å¤šçº¿ç¨‹ï¼Œä½ é€šè¿‡yamlçš„use_linuxå‚æ•°æ§åˆ¶æ˜¯å¦å¯¼å…¥å’Œå¯åŠ¨
æ³¨æ„ï¼Œqwen3æ˜¯æ”¯æŒ flash-attnçš„
åœ¨faissçš„ç”Ÿæˆé˜¶æ®µï¼Œfaiss-gpuçš„ä»£ç ä¼¼ä¹å¹¶æ²¡æœ‰å’Œfaiss-cpuæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œä½†åœ¨æ£€ç´¢æ—¶ï¼Œä¸¤è€…ä¼šæœ‰ä¸€å®šåŒºåˆ«


æˆ‘éœ€è¦å¼•å…¥infgrad/Jasper-Token-Compression-600Mæ¨¡å‹ï¼Œç”¨äºembeddingï¼Œå‚æ•°æ”¾åœ¨yamlçš„embeddingéƒ¨åˆ†ä¸­
ä½¿ç”¨ä¾‹å­å¯ä»¥å‚è€ƒ/root/autodl-tmp/Science-Exam-LLM/src/download/test.pyï¼Œä¸è¿‡ä¸ºäº†ç²¾åº¦æˆ‘ä»¬åªè¿›è¡Œencodeï¼Œä¸è¿›è¡Œå‹ç¼©
å¥½åƒJasperä¹Ÿæ˜¯æ”¯æŒflash attnçš„

ä¸ºä»€ä¹ˆqwen3_fp16_ip.faiss.lz4æ¯”qwen3_fp16_ip.faisså¤§è¿™ä¹ˆå¤šï¼Ÿä»–ä»¬éš¾é“ä¸éƒ½æ˜¯ç´¢å¼•æ–‡ä»¶å—ï¼Ÿéš¾é“qwen3_fp16_ip.faiss.lz4ä¸æ˜¯ï¼Ÿ
drwx---rwx 2 root root   48 Nov 23 12:03 checkpoints
-rw-r--r-- 1 root root  57M Nov 23 13:25 qwen3_fp16_ip.faiss
-rw-r--r-- 1 root root 4.9G Nov 23 13:25 qwen3_fp16_ip.faiss.lz4

ä¼šä¸ä¼šæœ‰ä¸€ç§å¯èƒ½qwen3_fp16_ip.faissåªæ˜¯ç”¨æ¥å‚¨å­˜ä¸­é—´æ–‡ä»¶çš„ï¼Œqwen3_fp16_ip.faiss.lz4å‚¨å­˜çš„æ‰æ˜¯æ‰€æœ‰ç´¢å¼•æ–‡ä»¶ï¼Ÿ

qwen3_fp16_ip.faiss.lz4æœ‰4.8Gå¤§æ¦‚è¿™ä¸ªæ‰æ˜¯å‚¨å­˜äº†ç´¢å¼•çš„æ–‡ä»¶ï¼Œ128ç©chunksçš„ç´¢å¼•æœ¬æ¥å°±åº”è¯¥æœ‰å¥½å‡ Gï¼Œ