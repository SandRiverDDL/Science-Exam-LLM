基于 设计.md为我完成项目，有以下要求：
1.使用config.yaml管理参数
2.目前只有 jinaai/jina-embeddings-v4-text-retrieval-GGUF和jinaai/jina-reranker-v3-GGUF 模型，其他embedding模型后续再添加，也就是说目前只有embedding只有一路
3.faiss使用IndexFlatIP + float16 存储
4.使用BM25辅助embedding找回，但不作为主力检索
5.不同检索方式实现模块化，因为我要通过实验比较不同检索方式的效果
6.我需要先检验Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP能否正常输出文字，然后用它做zero-shot分类，每次把问题和一个回答输入，只输出 0 或 1，0表示回答错误，1表示回答正确，注意KV cache，每次的问题是相同的，zero-shot作为base，跟后续实验做对比

构造一个流程，使得zero shot的qwen3模型，从train.csv读取问题和选项，像sample_submission.csv一样输出文件到output文件夹，同时拿预测结果和train.csv的answer做对比，并输出准确率，
这是单选题，
注意模块化，
prompt可以参考，你也可以改进，取决于你自己
You are a system that outputs ONLY one of A, B, C, D, E.
No explanation. No reasoning. Output must be exactly one letter.

Question:
{question}

Options:
A: ...
B: ...
C: ...
D: ...
E: ...

Answer (A/B/C/D/E):

set PYTHONPATH=%PYTHONPATH%;.\src
$env:PYTHONPATH="$env:PYTHONPATH;.\src"
$env:HUGGINGFACE_HUB_CACHE="D:\4000_projects\1project\LLM\data"

1. 加载训练集、验证集、测试集 CSV

   * 字段包括：question、options(A~E)、context 或背景文本（如果有）、correct_answer（训练集）

2.文本分块（chunking）
文本内容来自 data\raw\articles
对所有原始文档进行 chunk 处理：

chunk_size = 256 tokens
chunk_overlap = 50 tokens

操作方法：

使用 langchain.text_splitter.RecursiveCharacterTextSplitter 或等价自定义 splitter

分割每段文本为 token 级别的 chunks

每个 chunk 附带 metadata：

chunk_id

doc_id

position（chunk 的顺序）

3. 对所有 chunks 生成三种 embedding：

   * Jina-embeddings-v4 (text-retrieval)
   * BGE-m3 或 bge-large-v1.5
   * e5-mistral-instruct
   具体来说multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base，且这两者我已经下载好了，注意Jina-embeddings-v4是主力

4. 保存 embedding 到 FAISS 索引

   * 对每个 embedding 分别创建一个 IndexFlatIP (FP16)
   * 保存 id→chunk_text、id→metadata 映射


使用qwen3的tokenizer而不是你自己编写的，可以参考以下实现
---

def tokenize_chunks(text, tokenizer, chunk_size=256, overlap=50):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    n = len(tokens)
    chunks = []
    start = 0
    position = 0

    while start < n:
        end = min(start + chunk_size, n)
        chunk = tokens[start:end]
        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)

        chunks.append({
            "text": chunk_text,
            "position": position,
        })

        if end == n:
            break
        start = end - overlap
        position += 1

    return chunks

    对某个csv文件分块时，要打印 “正在分块文件：文件名”例如data\raw\articles\3.csv
    embedding部分加上e5 和bge multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base
    我的csv文件总大小为17G，你自己分析，我32G（大约25G）内存空闲是否够用，如果不够用可能需要一个文件一个文件处理

    暂时放弃jina（但不要删除相关代码），因为jina参数太大
    同时引入bge-m3模型作为辅助检索模型，现在的检索主力模型是[multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)
    ✔ 主力模型：multilingual-e5-large-instruct（最稳 → 决定你得分上限）
    ✔ 辅助模型：bge-m3（最佳互补 → 召回提升最大）
    ✔ 第三：gte-multilingual-base

我感觉非常慢，这正常吗，我决定先只启动gte去生成向量，快速验证，并检查以下问题：
你可能在用 CPU（GGUF）或单线程 tokenization → 会把预计速度砍到 1/10 或更低。

没有批量化（batch=1 或极小 batch） → 实际吞吐率急剧下降。

tokenizer 单线程或 Python-level decode/encode 成为瓶颈（尤其用 whitespace 切分或 slow tokenizer）。

I/O 磁盘读取慢（大量小文件） → CPU 等待 I/O。

没有把模型放到 GPU（或显存不足导致频繁 CPU-GPU 交换）。

框架/实现问题：例如用 sentence-transformers 但没有启用 device='cuda' 或没使用 torch.inference_mode()。

并行进程未启用（建议 tokenization 与 model inference 分离并行）


程序出bug，似乎是提示说超出了yield的limit，我没有报错错误报告

当程序 print 正在分块文件：data\raw\articles\1.csv\1.csv，到底在做什么，是在embedding吗？总不能分块分这么久吧
能不能实时显示embedding的进度，比如已经处理了多少个chunk，多少MB文件（估计）？

1.目前只用gte模型，除此之外，参考这个文档进行进行加速，因为每秒100chunks实在太慢了
2.为什么我跑了这么多我看不到embedding向量的输出？

1.[progress] flush=68 chunks=13056 bytes~11.00MB speed=265.5 chunks/s这个速度还是只有265chunks，而且我batchsize定多设定成192，再高就会超出显存，而且192显示的speed没有比64更快，而且我体感感觉64反而打印得更快，虽然显示的速度差不多
2.我希望这一段print控制打印频率，大概5秒打印一次，而且打印会覆盖上一次的打印，而不是每次都打印新的一行，例如这样
def print_and_overwrite(text):
    """
    打印文本，并用 \r 将光标移回行首，用于覆盖上次输出。
    """
    # 1. 打印文本
    # 2. 使用 end='\r' 来代替默认的 '\n'
    # 3. 如果需要清除残留，可以先打印足够多的空格
    print(f"\r{text}", end="")

    [progress] flush=50 chunks=9600 bytes~12.98MB speed=168.3 chunks/sTraceback (most recent call last):
    既然瓶颈在CPU侧，那能否离线将文档全转化为token，然后再输入embedding模型？
    我增大了一倍chunk_size_tokens，但目前的speed*2和之前差不多 265.5 chunks/s 

    [progress] flush=70 chunks=13440 bytes~18.60MB speed=166.0 chunks/s 怎么没有什么区别？


    multilingual-e5-large-instruct Alibaba-NLP/gte-multilingual-base BGE-m3

我决定新增BAAI/bge-small-en-v1.5模型以快速生成embedding，你自己决定是在bge-m3基础上新增还是直接新增一个索引


问题1：le "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 325, in main   
    result = build_indices_streaming(
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 252, in build_indices_streaming
    flush()
  File "d:\4000_projects\1project\LLM\src\pipeline\build_chunks_and_indices.py", line 209, in flush  
    if use_token_ids and buf_ids and hasattr(bge_small_emb, "embed_from_ids"):
UnboundLocalError: local variable 'buf_ids' referenced before assignment
问题2：CSV迭代异常 data/raw/articles\4.csv\4.csv: field larger than field limit (131072)

我觉得bge small也不小，所以切换到bge-m3模型了，但
1.怎么这么慢，才50chunks每秒
2.Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192)这个错误怎么来的？
Loaded text docs: 0 from data/raw/articles
Found CSV files: 38 from data/raw/articles
`torch_dtype` is deprecated! Use `dtype` instead!
[init] BGE device: cuda
[tokenize] 使用 BGE 的 tokenizer 进行分块（直通 token ids）
正在分块CSV：data\raw\articles\0.csv\0.csv
Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors
[progress] flush=3 chunks=384 bytes~0.08MB speed=51.4 chunks/sTraceback

1.我准备就长期使用bge-small了，还有什么加速embedding的方法吗？
2.我希望实现每隔多少个chunk就生成一次索引（大概十分钟一次？），并设置检查点，支持中断继续

参考以下，本项目可以用来加速embedding吗
SentenceTransformers + FastTokenizer + multi-worker dataloader
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-small-en-v1.5", 
                            device="cuda",
                            trust_remote_code=True)

embeddings = model.encode(
    docs,
    batch_size=4096,          # 发疯一样的大 batch
    device="cuda",
    show_progress_bar=True,
    normalize_embeddings=True,
    convert_to_numpy=True,
    num_workers=8             # tokenizer 多线程

)

我添加了一个新的embedding模型Qwen/Qwen3-Embedding-0.6B, 替我加入代码
我想知道是否可以用它来加速embedding

为什么qwen3 0.6b这么慢，才[progress] flush=3 chunks=384 bytes~0.08MB speed=17.6 chunks/s
是不是因为什么bug？
是不是没有启动flashattention

- 确保在 GPU + FP16：
- embedding_qwen3.device: "cuda"
- embedding_qwen3.dtype: float16
- 开启直通 token-id（避免重复分词）：
- pipeline.embed_from_tokens: true
- pipeline.embed_tokens_model: qwen3
- 看日志应出现： [tokenize] 使用 Qwen3 的 tokenizer 进行分块（直通 token ids）
- 平衡序列长度与分块大小：
- 将 embedding_qwen3.max_length: 256 （或略高于你的 chunk_size_tokens ，例如 256/320），避免 512 的无谓填充。
- 调整批量：
- 起步用 pipeline.batch_size: 256 或 512 ，有 16–24GB 显存可试 1024 ；OOM 就回退。
- 让 PyTorch 更快：
- CUDA 上默认已用 AMP（自动混精）。可额外允许 TF32：在入口脚本加 torch.backends.cuda.matmul.allow_tf32 = True （FP16 已足够的话，这项影响不大）。
- 如果只想“更快更小”：
- 直接用 bge-small-en-v1.5 在 GPU，吞吐通常比 Qwen3 0.6B 更高。

use_fast

University of California-Los Angeles (Los Angeles, CA)
Harry
Smith
2004 7 7

目前我使用bge-small的embedding速度大概为600 chunks每秒，要embedding 17G的数据需要十几小时，帮我分析以下我的程序的速度瓶颈在哪里，可以采取哪些措施

实施这些优化,但需要注意我是windows系统，使用多进程解码可能会卡主，
另外在build_chunks_and_indices.py中可以尝试重构，这个文件太大了

1.build_chunks_and_indices.py的文件还是太大了可能需要重构
2.[progress] flush=22 chunks=45056 bytes~52.19MB speed=854.2 chunks/s emb=51% io=2%
这个速度和时间占比怎么样？ emb + io加起来也才53%
3.你确定embedding_builder.py和index_manager.py适合放在pipeline目录下，而不是 index或者retrievel目录下
4.我准备暂时改成一分钟保存一次，然后（1）检查中断后能否真的再次加载
（2）帮我写一个脚本，在保存的faiss向量里检索0.csv里的text内容，看看保存的向量是否真的有效

1.csv0数据的在 'data/raw/articles/0.csv/0.csv',你的脚本中的列名似乎有问题
[样本 7] 查询文本 (前100字): 0X or 0-X ("zero/oh ex") may refer to:...
[检索结果] Top-5:
  [1] 距离=166672.0000 | chunk_id=0.8036060333251953 | doc_id=0.csv:row:0
      文本片段: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [2] 距离=44025.0000 | chunk_id=0.7438042759895325 | doc_id=0.csv:row:0
      文本片段: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [3] 距离=165743.0000 | chunk_id=0.681257426738739 | doc_id=0.csv:row:0
      文本片段: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [4] 距离=44508.0000 | chunk_id=0.662455677986145 | doc_id=0.csv:row:0
      文本片段: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
  [5] 距离=152025.0000 | chunk_id=0.6491485238075256 | doc_id=0.csv:row:0
      文本片段: 0 ( zero ) is a number representing an empty quantity. adding 0 to any number le...
这种检索结果正常吗
2.[progress] flush=6 chunks=12288 bytes~12.74MB speed=864.7 chunks/s emb=54% io=3%
断点重续之后，chunks和bytes只显示的是本次执行的embedding的chunks，我希望还要显示的是累计的chunks和bytes

[checkpoint] saved at chunks=38912 -> data/faiss/checkpoints/embedding_progress.json
D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/build_chunks_and_indices.py

D:/6000_env/miniconda3/envs/llm/python.exe d:/4000_projects/1project/LLM/src/pipeline/verify_index.py

我看到embedding_progress.json中"chunk_id": 38912, "flush_count": 19,但bge_small_fp16_ip.faiss文件有200MB，这是不是数据上不一致了
我再次运行embedding时，似乎分块速度只剩600chunks每秒了，为什么还变慢了？
既然我已经采用tokenizer捷径了，那我预先tokenizer所有csv数据，那是不是会更快？

我需要你做出改动，每个chunk前都要加上该chunk所属文档的title
如何拼标题？用什么符号最合适？
⚡ 最推荐（业界公认最佳）：
格式："{title}\n\n{chunk_text}"

也就是：

标题
<空行>
chunk 内容

为什么用换行？

Tokenizer 对换行有天然分隔效果

不引入额外意义符号（避免语义误导）

模型会认为“换行 = 新段落”

高质量 embedding 常用格式（OpenAI, Cohere, Voyage）

目前我已经确定使用 复用token ids的trick了，因为多路embedding的耗时太长，我不能接受。另外我要求你使用以下trick
1.

总流程
【阶段 0：预处理】
- 文档级 embedding（doc-emb）
- 文档级实体提取（doc-entity）
- 动态 chunking + chunk embedding（chunk-emb）
- metadata 写入 parquet

【阶段 1：query 处理】
- query embedding
- query entity（NER）

【阶段 2：文档级召回（fast）】
top_docs = ANN(doc_emb, top 1000)

【阶段 3：文档实体重排序（cheap）】
for doc in top_docs:
    doc.score = 0.8 * embedding_score + 0.2 * entity_overlap

select doc_top_k = top 500–800

【阶段 4：chunk-level 召回】
从这 500–800 文档的 chunk 中 ANN 召回 1000–2000 chunk（由 bge-small 处理）

【阶段 5：二阶段精检】
用 Jina V4 计算 query 与 chunk 精确 embedding 相似度
选 top 100–200 进入 Reranker / LLM

【阶段 6：LLM rerank + answer】





## 0) 数据清洗（必做，高影响）以及预Tokenization 

目标：去掉垃圾、统一风格、减少无用 chunks。

**要点 / 操作（优先级高）**

1. **语言检测**（只保留目标语言en）

   * 用 `langdetect` 或 `fasttext`。
2. **只保留 ASCII**（去 emoji、罕见 Unicode）

   * regex: `re.sub(r'[^\p{Han}\p{Hiragana}\p{Katakana}\p{Hangul}\p{ASCII}]', '', text)` （需 `regex` 库）
3. **移除 URL / HTML / 控制字符**

   * URL regex、`BeautifulSoup` 去 html。
4. **Unicode 标准化**。
*   **代码**：`import unicodedata; text = unicodedata.normalize('NFKC', text)`
*   **作用**：解决全角/半角字符混乱（如 `１２３` vs `123`，`Ａ` vs `A`），以及不同的空格符号。这对于 Embedding 对齐非常重要。
5. 将全部文档text使用bge_small tokenizer预Tokenization 

6. **短文本过滤**（大多数噪音发生于极短文本）

   * `min_text_tokens = 32`（推荐）



---

## 1) 标题处理（Title cleaning）——你特别关心的部分

**目标**：只拼接“高信噪比”标题，并且限制长度，避免污染 embedding。

**步骤**

1. 标题噪声检测（核心）** — 决定是否拼接标题

   * 判定规则（示例）：

     ```python
     def is_good_title(t):
         t=t.strip()
         if not t or len(t)<3 or len(t)>120: return False
         alnum_ratio = sum(c.isalnum() for c in t)/max(1,len(t))
         if alnum_ratio>0.8: return False
         if " " not in t and len(t)>6: return False
         if re.search(r'(file_|doc_|id_|^v\d+\.)', t.lower()): return False
         return True
     ```
   <!-- * 额外：统计 title 中的英文单词比中文字符比率，低于阈值就判为垃圾。 -->
2. 去停用词（可选，仅当 title 很长时）

   * English stopword: NLTK/stopwords
   * Chinese: 自定义停用词表
   * 示例（Python）：

     ```python
     from nltk.corpus import stopwords
     stops = set(stopwords.words('english'))
     def shorten_title(t, max_words=8):
         words=t.split()
         words=[w for w in words if w.lower() not in stops]
         return " ".join(words[:max_words])
     ```
3. 限长（token级）：

   * `title_max_tokens = 8~32`（推荐 8-16 英文单词 / 16–32 token 视模型）
   * 用 tokenizer 截断：`title_tokens = tokenizer.encode(title)[:title_max_tokens]`
4. 正则清洗（移除常见噪音）：

   * remove serials: `re.sub(r'\b[A-Z0-9_-]{6,}\b','',title)`
   * remove leading/trailing punctuation: `title.strip(" -_:;.,")`

**拼接建议**

* 如果 `is_good_title` 为 True：

  ```
  chunk_text_final = title_clean + "\n\n" + entities_line + "\n\n" + chunk_body
  ```
* 否则，不拼 title。

---
处理后把结果写到 Parquet（列：doc_id, title, doc_ids, length_tokens）以便后续重用。每隔大概256MB生成一次 1.Parquet   2.Parquet
实现以上要求，代码可以考虑放在processing文件夹下，如果你有需要生成新的文件夹也可以

我已经安装了beautifulsoup4 regex langdetect，不要你自己去实现，用regex去移除url
标题处理建议里有一个是错的，他提到直接用字符估计token数，我不需要，直接encode并标题的ids，后续直接考虑合并
基于两个建议文件，修改代码，注意，我准备进一步将动态chunking和预tokenize结合，在一个pipline中完成从csv文件 到 chunks，具体怎么做我会后续告诉你，但你考虑预留接口位置

将chunking的流程并入 preprocess_data，注意采用父文档索引，子文档chunksize 128，无overlap，父文档512
meta data里需要存储
rerank_text	String: 已拼接 Title。 Reranker 输入。 Reranker 需要文本来做 Cross-Attention 对比。
doc_id
start_idx, end_idx 父文档在原文档ids中的起始和终止index
chunk_id	Int/String	向量索引 ID	主键。
chunk_len	Int	子 Chunk 长度
这意味着确实需要把清洗结果结果写到 Parquet（列：doc_id, title, doc_ids, length_tokens）
这样后续才能根据 doc_id, start_idx, end_idx 快速定位到父文档

1.这个终端提示的超出token数会有什么问题吗？
2.我好奇你在partque里存储ids是以什么格式，我听说 np.uint16可以压缩空间，还可以指定压缩算法ZSTD： 速度较快，压缩率最高（推荐）。
3.我需要你在preprocess_data实现 断点重续机制

已处理 3,000 条文档，保留 526 条这种信息，后一天覆盖前一条，不然太长了
你的tokenize逻辑好像有问题啊，Token indices sequence length is longer than the specified maximum sequence length for this model (5947 > 512). Running this sequence through the model will result in indexing errors
或者说这只是一个警告
断点重续时，记得 “保留 3,381 条”这个数量要和之前的累加

写一个脚本，查看现有的 docs_1.parquet chunks_1.parquet里面存的是什么数据，叫做 check_parquet_data
我发现了一个很严重的问题，似乎你断点重续之后会重新开始写入chunks_1.parquet，而且重续之后还是从0开始计算 “保留 x 条”

你通过 len(tokens)*4这种方法估计完全是错的，首先我改成了 uint16后占据两字节，其次还有 zstd压缩，
现在我设定写入的阈值是 1G
然后你需要根据实际文件大小和处理文档行数来估算文件大小，一个chunks.parquet约为82MB，一个docs.parquet约为95MB

怎么连1Gchunk doc都输出不了就内存不够了，是不是意味着1G太大应该改成512MB,还是说你的程序或计算公式哪里有问题
注：ratio我后来微调了一点，但应该不影响
