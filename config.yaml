# 全局配置
use_linux: false                # 是否运行在Linux服务器上，启用faiss-gpu和flash-attn

data:
  corpus_dir: "data/raw/articles"            # 文档目录（用于构建索引）
  csv_path: "data/raw/6000_train_examples.csv" # 如需读取问答数据可用
pipeline:
  batch_size: 64           # 构建索引时的嵌入批量大小（提升至 8192 以充分利用 GPU）

# FAISS 索引
index:
  type: "flat_ip_fp16"        # 使用 FP16 ScalarQuantizer，实现内积语义 + 节省内存
  save_path: "data/faiss/index_fp16_ip.faiss"
  dim: null                   # 自动根据第一条向量维度确定

# 其他 HF Embedding 模型配置（按需启用）
embedding:
  qwen3:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    max_length: 168
    device: null   # 自动选择 cuda/cpu（建议在有 GPU 时设为 "cuda"）
    dtype: float16 # 在 CUDA 上用 FP16 提升吞吐；CPU 将回退 FP32
    index_path: "data/faiss/qwen3_fp16_ip.faiss"

  jasper:
    model_id: "infgrad/Jasper-Token-Compression-600M"
    max_length: 168
    device: null   # 自动选择 cuda/cpu
    dtype: float16 # 在 CUDA 上用 FP16 提升吞吐
    index_path: "data/faiss/jasper_fp16_ip.faiss"

  # Qwen3 Embedding 模型配置（可选）



# 数据预处理配置
preprocessing:
  target_language: "en"          # 目标语言（en=英文）
  min_text_tokens: 32            # 最小文本token数
  title_max_tokens: 16           # 标题最大token数
  use_stopword_removal: true     # 是否去除停用词
  max_words_after_stopword: 8    # 去停用词后的最大单词数
  parquet_chunk_size_mb: 512    # 每个Parquet文件的大小（MB），默认1GB
  output_dir: "data/processed/parquet"  # 输出目录
  resume: true                   # 断点续跑（开启后自动跳过已处理文档）

# 父文档索引 (Parent Document) Chunking配置
chunking:
  child_size: 128    # 子chunk大小（用于精准检索）
  parent_size: 256   # 父chunk大小（用于LLM上下文）

# 检索参数
retrieval:
  # Dense Retrieval配置
  query_encoder:
    model_id: "Qwen/Qwen3-Embedding-0.6B"
    batch_size: 32
    max_length: 168
    device: null   # 自动选择 cuda/cpu（建议在有 GPU 时设为 "cuda"）
    dtype: float16 # 在 CUDA 上用 FP16 提升吞吐；CPU 将回退 FP32

  # Dense ANN 检索
  dense:
    batch_size: 64
    top_k: 600             # Dense检索的top-k（对128万库推荐600）
  
  # BM25配置
  bm25:
    index_path: "data/processed/bm25_index"  # BM25索引保存路径（parquet格式目录）
    k1: 1.5                # BM25参数k1（调整TF饱和点，值大=更重视TF差异）
    b: 0.75                # BM25参数b（调整长度归一化，0=无长度归一化，1=完全归一化）
  
  # Paragraph Boosting配置（段落指数增强）
  paragraph_boosting:
    alpha: 0.03             # 增强系数（0.02-0.04），公式: score * (1 + alpha * exp(n))
  
  # RRF融合配置
  fusion:
    rrf_k: 30               # RRF参数k，值越小名次越敏感（推荐30-50，大库推荐30）
  
  # MMR重排序配置
  mmr:
    lambda: 0.8             # 相关性权重（0.8表示强调相关性，轻度去重）
    top_k: 20              # MMR返回数量，用于reranker（推荐400-500）

  # Reranker（可选，提示式评分；若不稳定可关闭）
  reranker:
    model_id: "jinaai/jina-reranker-v3"
    batch_size: 4               # 批处理大小（reranker 极小 batch，1-8）
    device: null                 # 自动选择 cuda/cpu（建议在有 GPU 时设为 "cuda"）
    top_k: 3                     # 最终返回给LLM的数量（推荐3-5）

# Qwen3 Zero-shot 分类
qwen:
  model_id: "ISTA-DASLab/Qwen3-8B-Instruct-FPQuant-QAT-MXFP4-TEMP"
  device_map: "auto"
  max_new_tokens: 1           # 分类仅需 1 个 token（0 或 1）
  gen_temperature: 0.0        # 关闭采样
  do_sample: false
  trust_remote_code: true

zero_shot:
  force_output_digits: true   # 强制模型仅输出 0 或 1 的提示词
  use_kv_cache_prefix: true   # 对相同问题复用前缀 KV cache
  question_repeat: true       # 问题相同场景
  test_csv: "data/raw/kaggle-llm-science-exam/test.csv"  # 测试集路径
  val_csv: "data/raw/kaggle-llm-science-exam/sample_submission.csv"  # 验证集路径（可选）
  output: "output/predictions_zeroshot.csv"  # 输出预测路径
  retrieval_batch_size: 8     # 检索批处理大小
  generation_batch_size: 1    # 生成批处理大小
  do_sample: false

kaggle:
  train_csv: "data/raw/kaggle-llm-science-exam/train.csv"
  sample_submission_csv: "data/raw/kaggle-llm-science-exam/sample_submission.csv"
  output_submission: "output/zero_shot_submission.csv"
