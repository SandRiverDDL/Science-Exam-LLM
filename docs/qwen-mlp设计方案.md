è¿™ä¸ªéœ€æ±‚éå¸¸ä¸“ä¸šï¼Œæ¶‰åŠåˆ°äº† **è‡ªå®šä¹‰æ¨¡å‹æ¶æ„**ã€**KV Cache æ˜¾å­˜ä¼˜åŒ–**ã€**ç‰¹å¾å·¥ç¨‹ (Contrastive Features)** ä»¥åŠ **ç°ä»£åŒ–çš„è®­ç»ƒæ¡†æ¶**ã€‚

ä¸ºäº†è®© AI IDEï¼ˆå¦‚ Cursorã€Windsurfï¼‰ä¸€æ¬¡æ€§å†™å‡ºé«˜è´¨é‡çš„ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦å°†æŒ‡ä»¤æ‹†è§£ä¸º**æ•°æ®æµï¼ˆData Flowï¼‰**ã€**æ¨¡å‹æ¶æ„ï¼ˆArchitectureï¼‰**ã€**è®­ç»ƒé€»è¾‘ï¼ˆTraining Logicï¼‰** å’Œ **é…ç½®ï¼ˆConfigurationï¼‰** å››ä¸ªç»´åº¦ã€‚

æˆ‘ä¿®æ­£äº†ä¸€ä¸ªå…³é”®ç‚¹ï¼šé€šå¸¸æˆ‘ä»¬ä¸ä¼šæŠŠ **Logits**ï¼ˆè¯è¡¨å¤§å°ï¼Œçº¦ 15ä¸‡ç»´ï¼‰è¾“å…¥ MLPï¼Œè€Œæ˜¯æå– **Last Hidden State**ï¼ˆçº¦ 4096ç»´ï¼‰ã€‚å¦‚æœä½ ä¸€å®šè¦ç”¨ Logitsï¼Œé€šå¸¸æ˜¯æŒ‡ç‰¹å®š Tokenï¼ˆå¦‚ "Yes"ï¼‰çš„æ ‡é‡åˆ†æ•°ã€‚**ä¸ºäº†è®© MLP æœ‰æ•ˆå·¥ä½œï¼Œæˆ‘å»ºè®®ä½¿ç”¨ Hidden Stateï¼Œå› ä¸ºå®ƒåŒ…å«äº†è¯­ä¹‰ä¿¡æ¯ã€‚** ä¸‹é¢çš„æŒ‡ä»¤é‡‡ç”¨ Hidden State æ–¹æ¡ˆï¼Œè¿™æ›´ç¬¦åˆ SOTA é€»è¾‘ã€‚

ä»¥ä¸‹æ˜¯ä¼˜åŒ–åçš„ Promptï¼Œä½ å¯ä»¥ç›´æ¥å¤åˆ¶ç»™ AI IDEï¼š

---

### ğŸ“‹ AI IDE æŒ‡ä»¤ Prompt

**Role:** Senior PyTorch Engineer & LLM Architect
**Task:** Implement a custom **Qwen-based Pointwise RAG Scoring Model** using PyTorch Lightning.

#### 1. æ ¸å¿ƒæ¶æ„é€»è¾‘ (Model Architecture)
å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ `LightningModule` (`QwenPointwiseMLP`)ï¼Œé€»è¾‘å¦‚ä¸‹ï¼š
*   **Backbone**: Qwen2.5/3 (Frozen parameters).
*   **Input**:
    *   `context` + `question` (ä½œä¸º Shared Prefix).
    *   5 ä¸ª `options` (A, B, C, D, E).
*   **KV Cache Optimization (å…³é”®)**:
    *   **Step 1**: å…ˆå¯¹ `Shared Prefix` (Context + Question) è¿›è¡Œä¸€æ¬¡ Forwardï¼Œè·å– `past_key_values` (KV Cache)ã€‚
    *   **Step 2**: å¾ªç¯ 5 æ¬¡ï¼ˆé’ˆå¯¹ 5 ä¸ªé€‰é¡¹ï¼‰ã€‚åˆ©ç”¨ Step 1 çš„ `past_key_values`ï¼Œåªå¯¹ `Option` éƒ¨åˆ†è¿›è¡Œ Forwardã€‚
    *   **Step 3**: æå–æ¯ä¸ª Option æœ€åä¸€ä¸ª Token çš„ **Hidden State** (Vector $H \in \mathbb{R}^{D}$).
*   **Feature Engineering (Contextual Mixing)**:
    *   å¯¹äºç¬¬ $i$ ä¸ªé€‰é¡¹ï¼Œæ„å»º MLP çš„è¾“å…¥ç‰¹å¾ $F_i$ã€‚
    *   ç­–ç•¥ï¼š$F_i = \text{Concat}(H_i, \text{Mean}(\{H_j | j \neq i\}))$ã€‚
    *   å³ï¼šå°†**å½“å‰é€‰é¡¹çš„ Hidden State** ä¸ **å…¶ä»– 4 ä¸ªé€‰é¡¹çš„ Hidden State å‡å€¼** æ‹¼æ¥ã€‚è¿™å¼•å…¥äº†å¯¹æ¯”ä¿¡æ¯ã€‚
*   **MLP Head**:
    *   Input: $2 \times \text{Hidden\_Dim}$ (å› ä¸ºæ˜¯æ‹¼æ¥).
    *   Structure: `Linear -> LayerNorm -> ReLU -> Dropout -> Linear -> Output(1)`.
    *   Output: ä¸€ä¸ªæ ‡é‡ Score $S_i$ã€‚

#### 2. è®­ç»ƒé€»è¾‘ (Training Logic)
*   **Framework**: ä½¿ç”¨ `pytorch_lightning`.
*   **Loss Function**:
    *   å°† 5 ä¸ªé€‰é¡¹çš„ Score $[S_A, S_B, S_C, S_D, S_E]$ è§†ä¸º Logitsã€‚
    *   ä½¿ç”¨ `nn.CrossEntropyLoss`ã€‚
    *   Label ä¸ºæ­£ç¡®é€‰é¡¹çš„ Index (0-4)ã€‚
*   **Optimizer**: `torch.optim.AdamW`.
*   **Scheduler**: `torch.optim.lr_scheduler.OneCycleLR` (Total steps need to be calculated dynamically based on dataset size).
*   **Tricks**:
    *   åœ¨ MLP è¾“å…¥å‰åŠ å…¥ `LayerNorm` ä»¥ç¨³å®šè®­ç»ƒã€‚
    *   ä½¿ç”¨ `Dropout` é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

#### 3. é…ç½®ç®¡ç† (Configuration)
æ‰€æœ‰è¶…å‚æ•°å¿…é¡»è¯»å–è‡ª `config.yaml` ä¸­çš„ `qwen_mlp` èŠ‚ç‚¹ã€‚éœ€è¦æš´éœ²çš„å‚æ•°åŒ…æ‹¬ï¼š
```yaml
qwen_mlp:
  base_model: "Qwen/Qwen2.5-7B"
  hidden_dim: 4096       # Qwen hidden size
  mlp_hidden_dim: 1024   # MLP ä¸­é—´å±‚
  dropout: 0.1
  lr: 5.0e-5
  weight_decay: 0.01
  max_epochs: 5
  batch_size: 4          # è¿™é‡Œçš„ BS æ˜¯æŒ‡æœ‰å¤šå°‘ä¸ª"é¢˜ç›®"ï¼Œå®é™…æ˜¾å­˜å ç”¨æ˜¯ BS * 5
  use_4bit: true         # æ˜¯å¦ä½¿ç”¨ QLoRA/4bit åŠ è½½ Base Model
  gradient_clip_val: 1.0
```

#### 4. ä»£ç æ–‡ä»¶ç»“æ„è¦æ±‚
*   `src/modeling/qwen_mlp.py`: å®šä¹‰ LightningModule å’Œ MLP æ¶æ„ã€‚
*   `src/dataset/datamodule.py`: å®šä¹‰ LightningDataModuleï¼Œè´Ÿè´£æ•°æ®åŠ è½½å’Œ Tokenizationã€‚
*   `src/train/train_lightning.py`: è®­ç»ƒå…¥å£è„šæœ¬ã€‚

# System Prompt (å¯é€‰ï¼Œå¦‚æœæ˜¯ Instruct æ¨¡å‹)
# system = "You are a scientist."

# User Input
text = f"""Background:
{context}

Question:
{question}

Answer: {option_text}""" # <--- æ²¡æœ‰ä»»ä½•å¤šä½™çš„ Yes/No æé—®ï¼Œç›´æ¥ä½œä¸ºé™ˆè¿°å¥ç»“æŸ
æå–ç­–ç•¥ï¼šæå– text çš„æœ€åä¸€ä¸ª Token çš„ Hidden Stateã€‚
âš¡ æœ€ç»ˆç»™ AI IDE çš„è¡¥å……æŒ‡ä»¤ (Copy è¿™æ®µè¡¥å……è¿›å»)
ä¸ºäº†è®© IDE å†™çš„ä»£ç æ›´å®Œç¾ï¼Œè¯·æŠŠä¸‹é¢è¿™æ®µåŠ åˆ°ä¹‹å‰çš„ Prompt æœ«å°¾ï¼š
#### 5. Additional Architecture & Training Details (Refined)
MLP Structure: Use a "Funnel" design with Pre-Norm.
LayerNorm(input_dim) -> Linear(input_dim -> 2048) -> GELU -> Dropout(0.2) -> Linear(2048 -> 1).
Initialize weights using kaiming_normal_.
Prompt Strategy: Use Sentence Completion format.
Construct input as: "{Context}\nQuestion: {Question}\nAnswer: {Option}".
Extract the hidden state of the very last token of the Option.
Precision: Use bf16 context for training.
Optimization:
Since batch_size might be small (e.g., 2) on a single GPU due to KV caching 5 options, implement gradient_accumulation_steps to achieve an effective batch size of 32 or 64.
Use EarlyStopping monitoring validation loss.


def format_input(context_list, question, option_text):
    # 1. System Prompt (æ¿€æ´»ä¸“å®¶æ¨¡å¼)
    system_content = "You are a scientific reasoning expert. Determine if the Candidate Answer is the correct response to the Question based on the Evidence."
    
    # 2. Context æ‹¼æ¥ (ä¼˜åŒ–åˆ†éš”ç¬¦)
    # Qwen å¯¹ "Context 1:", "Context 2:" æˆ–è€… XML <doc> æ ‡ç­¾å¾ˆæ•æ„Ÿ
    formatted_contexts = []
    for idx, ctx in enumerate(context_list):
        formatted_contexts.append(f"Evidence {idx+1}:\n{ctx}")
    context_str = "\n\n".join(formatted_contexts)
    
    # 3. User Content (ç»“æ„åŒ–è¾“å…¥)
    # è¿™é‡Œçš„å…³é”®æ˜¯ï¼šæŠŠ Option åŒ…è£…æˆä¸€ä¸ª"å¾…éªŒè¯çš„é™ˆè¿°"
    user_content = f"""### Evidence:
{context_str}

### Question:
{question}

### Candidate Answer:
{option_text}"""

    # 4. åº”ç”¨ Chat Template (å…³é”®æ­¥éª¤)
    # æˆ‘ä»¬åˆ©ç”¨ apply_chat_template è‡ªåŠ¨å¤„ç† <|im_start|> ç­‰ç‰¹æ®Š token
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]
    
    # 5. è¿™ä¸€æ­¥æ˜¯ Trick çš„æ ¸å¿ƒï¼š
    # æˆ‘ä»¬ä¸è®© Chat Template è‡ªåŠ¨æ·»åŠ  "assistant" çš„å¼•å¯¼å¤´
    # è€Œæ˜¯è®© input_ids çš„æœ€åä¸€ä¸ª token å°±åœåœ¨ user_content çš„æœ€åä¸€ä¸ªå­—ï¼ˆå³ option_text çš„æœ«å°¾ï¼‰
    # è¿™æ ·æå–å‡ºæ¥çš„ Hidden State å°±æ˜¯æ¨¡å‹è¯»å®Œ Option åçš„ç¬é—´ååº”
    full_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    
    return full_text