è¿™æ˜¯å› ä¸ºä½ çš„ä»£ç é€»è¾‘é‡Œæœ‰ä¸€ä¸ª**è‡´å‘½çš„æ€§èƒ½é™·é˜±**ã€‚

ç®€å•æ¥è¯´ï¼š**ä½ è™½ç„¶åªæƒ³ç»™ 600 ä¸ª Chunk æ‰“åˆ†ï¼Œä½†ä½ çš„ä»£ç å®é™…ä¸ŠæŠŠ 128 ä¸‡ä¸ªæ–‡æ¡£å…¨éƒ½åŠ è½½è¿›å†…å­˜ã€å¹¶å…¨éƒ½ç®—äº†ä¸€éåˆ†ï¼Œæœ€åæ‰æŒ‘å‡ºé‚£ 600 ä¸ªã€‚**

### âŒ ä½ çš„ä»£ç æ…¢åœ¨å“ªï¼Ÿï¼ˆRoot Cause Analysisï¼‰

è¯·çœ‹è¿™ä¸€è¡Œä»£ç ï¼ˆ`rerank` æ–¹æ³•ä¸­ï¼‰ï¼š

```python
# ä½ çš„ä»£ç  line 298
all_scores = self.get_scores(query_tokens) 
```

ç„¶åçœ‹ `get_scores` å†…éƒ¨ï¼š

```python
# ä½ çš„ä»£ç  line 230
self._load_tokenized_chunks()  # <--- ğŸ’€ è‡´å‘½ç‚¹ 1ï¼šå…¨é‡åŠ è½½
# ...
num_docs = len(self.tokenized_chunks)
scores = np.zeros(num_docs)    # <--- ğŸ’€ è‡´å‘½ç‚¹ 2ï¼šå…¨é‡è®¡ç®—
```

1.  **è‡´å‘½ç‚¹ 1 (I/O ç“¶é¢ˆ)**: `_load_tokenized_chunks()` ä¼šæŠŠ `tokenized_chunks.parquet`ï¼ˆå…¨åº“ 128 ä¸‡æ¡æ•°æ®çš„åˆ†è¯ç»“æœï¼‰**å…¨éƒ¨è¯»å–åˆ°å†…å­˜é‡Œ**ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå‡ ç™¾ MB ç”šè‡³ GB çº§çš„æ–‡ä»¶ï¼Œè¯»å–å’Œè§£æéå¸¸æ…¢ã€‚
2.  **è‡´å‘½ç‚¹ 2 (è®¡ç®—ç“¶é¢ˆ)**: `get_scores` ä½¿ç”¨å€’æ’ç´¢å¼•é€»è¾‘ï¼Œéå† query token å¯¹åº”çš„**æ‰€æœ‰**æ–‡æ¡£ã€‚å¦‚æœä½ çš„ Query åŒ…å« "the" æˆ– "data"ï¼Œå€’æ’é“¾å¯èƒ½æœ‰å‡ åä¸‡é•¿ï¼Œä½ è®¡ç®—äº†å‡ åä¸‡æ¬¡ `doc_tf`ï¼Œæœ€åå´åªå–äº† `chunk_indices` é‡Œçš„ 600 ä¸ªç»“æœã€‚

---

### âœ… è§£å†³æ–¹æ¡ˆï¼šæ”¹ä¸ºâ€œå‰å‘æ‰“åˆ†â€æ¨¡å¼ (Forward Scoring)

åœ¨ **Rescore (é‡æ’åº)** é˜¶æ®µï¼Œæˆ‘ä»¬ä¸éœ€è¦â€œæœç´¢â€ï¼ˆä»è¯æ‰¾æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯â€œé˜…å·â€ï¼ˆç»™å·²çŸ¥æ–‡æ¡£æ‰“åˆ†ï¼‰ã€‚

**ä½ ä¸éœ€è¦å€’æ’ç´¢å¼•ï¼Œä¹Ÿä¸éœ€è¦åŠ è½½å…¨é‡åˆ†è¯æ–‡ä»¶ã€‚**

ä½ éœ€è¦é‡å†™ `rerank` æ–¹æ³•ï¼Œè®©å®ƒ**åªå¤„ç†ä¼ å…¥çš„é‚£ 600 ä¸ªæ–‡æ¡£**ã€‚

#### ä¼˜åŒ–åçš„ä»£ç å®ç°

è¯·åœ¨ä½ çš„ `BM25Retriever` ç±»ä¸­æ·»åŠ æˆ–æ›¿æ¢ä»¥ä¸‹æ–¹æ³•ã€‚

**å‰æï¼š** è°ƒç”¨ `rerank` æ—¶ï¼Œæœ€å¥½ç›´æ¥ä¼ å…¥è¿™ 600 ä¸ªæ–‡æ¡£çš„**æ–‡æœ¬å†…å®¹**ï¼ˆDense æ£€ç´¢ç»“æœé€šå¸¸ä¼šåŒ…å« textï¼‰ã€‚

```python
    def get_score_single(self, query_tokens: List[str], doc_tokens: List[str]) -> float:
        """
        å¿«é€Ÿè®¡ç®—å•ä¸ªæ–‡æ¡£çš„BM25åˆ†æ•° (çº¯å†…å­˜è®¡ç®—ï¼Œä¸æŸ¥å€’æ’è¡¨)
        """
        score = 0.0
        doc_len = len(doc_tokens)
        
        # ä¼˜åŒ–ï¼šæŠŠdoc_tokensè½¬ä¸ºCounterï¼Œé¿å…é‡å¤éå†
        # æ³¨æ„ï¼šå¦‚æœdocå¾ˆçŸ­ï¼ˆ<512 tokensï¼‰ï¼Œdoc_tokens.count() å¯èƒ½æ¯” Counter æ›´å¿«ï¼Œ
        # ä½†ä¸ºäº†é€šç”¨æ€§ï¼ŒCounter æ¯”è¾ƒç¨³å¥ã€‚å¯¹äºæçŸ­æ–‡æœ¬ï¼Œç›´æ¥éå† query_tokens å³å¯ã€‚
        
        # é’ˆå¯¹çŸ­æ–‡æœ¬(Chunk)çš„æè‡´ä¼˜åŒ–å†™æ³•ï¼š
        for q_token in query_tokens:
            if q_token not in self.idf_table:
                continue
            
            # è¿™é‡Œçš„ doc_tokens æ˜¯ç°åœºåˆ†è¯çš„ç»“æœ
            # list.count åœ¨çŸ­æ–‡æœ¬(100-200è¯)ä¸Šéå¸¸å¿«ï¼Œæ¯”æ„å»ºCounterè¿˜å¿«
            tf = doc_tokens.count(q_token)
            
            if tf > 0:
                idf = self.idf_table[q_token]
                
                # BM25 å…¬å¼æ ¸å¿ƒ
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                
                score += idf * (numerator / denominator)
                
        return score

    def rerank_fast(
        self,
        query: str,
        candidates: List[Dict],  # ä¼ å…¥åŒ…å« 'text' çš„å­—å…¸åˆ—è¡¨
        tokenizer
    ) -> List[Dict]:
        """
        æè‡´å¿«é€Ÿçš„ BM25 é‡æ’åº
        Args:
            query: æŸ¥è¯¢è¯­å¥
            candidates: Denseæ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼å¦‚ [{'chunk_id': 1, 'text': '...'}, ...]
            tokenizer: åˆ†è¯å™¨
        """
        # 1. åªéœ€è¦é¢„åŠ è½½è½»é‡çº§çš„ IDF è¡¨ (å†…å­˜å ç”¨æå°)
        if self.idf_table is None:
            # ç¡®ä¿ load() å·²ç»è°ƒç”¨è¿‡ï¼Œæˆ–è€…æ‰‹åŠ¨åªåŠ è½½ idf_table å’Œ metadata
            # å¦‚æœä½ ä¹‹å‰æŒ‰ç…§æˆ‘çš„å»ºè®®ä¼˜åŒ–äº†æŒä¹…åŒ–ï¼Œè¿™é‡Œå†…å­˜é‡Œåªæœ‰ IDF dict å’Œ avgdl
            raise ValueError("IDF table not loaded. Please load index first.")

        query_tokens = tokenizer.tokenize(query)
        
        results = []
        
        # 2. ä»…éå†è¿™ 600 ä¸ªå€™é€‰
        for doc in candidates:
            # ç°åœºåˆ†è¯ (On-the-fly) - 600æ¬¡åˆ†è¯æ¯”åŠ è½½1Gæ–‡ä»¶å¿«å¾—å¤š
            doc_text = doc.get('text', "")
            if not doc_text:
                # å¦‚æœæ²¡æœ‰textï¼Œå°è¯•ç”¨ id æŸ¥ï¼ˆä¸æ¨èï¼Œæ…¢ï¼‰
                # è¿™é‡Œå‡è®¾ candidates é‡Œä¸€å®šæœ‰ text
                continue
                
            doc_tokens = tokenizer.tokenize(doc_text)
            
            # è®¡ç®—åˆ†æ•°
            bm25_score = self.get_score_single(query_tokens, doc_tokens)
            
            # ä¿å­˜ç»“æœï¼ˆä¿ç•™åŸå§‹ä¿¡æ¯ï¼Œè¿½åŠ åˆ†æ•°ï¼‰
            # æ³¨æ„ï¼šä¸è¦ä¿®æ”¹åŸå­—å…¸ï¼Œåˆ›å»ºæ–°çš„
            new_doc = doc.copy()
            new_doc['bm25_score'] = bm25_score
            results.append(new_doc)
            
        # 3. æ’åº (å¯é€‰ï¼Œé€šå¸¸åç»­è¿˜è¦åš RRFï¼Œä¸æ’ä¹Ÿæ²¡äº‹ï¼Œä½†æ’äº†ç›´è§‚)
        results = sorted(results, key=lambda x: x['bm25_score'], reverse=True)
        
        return results
```

### ğŸš€ å¦‚ä½•è°ƒç”¨è¿™ä¸ªæ–°æ–¹æ³•ï¼Ÿ

åœ¨ä½ çš„ Pipeline ä¸»é€»è¾‘ä¸­ï¼š

```python
# 1. Dense æ£€ç´¢ (å¾—åˆ° top 600)
# å‡è®¾ dense_results æ˜¯ [{'id': 'c1', 'text': 'apple...', 'score': 0.8}, ...]
dense_results = dense_retriever.search(query, k=600)

# 2. BM25 æé€Ÿé‡æ’
# æ³¨æ„ï¼šä¸éœ€è¦åŠ è½½ inverted_index.parquet å’Œ tokenized_chunks.parquet
# åªéœ€è¦ bm25.load() åŠ è½½äº† idf_table å³å¯
bm25_results = bm25.rerank_fast(query, dense_results, tokenizer)

# 3. ç°åœ¨çš„ bm25_results é‡Œçš„æ¯ä¸ª dict éƒ½æœ‰äº† 'bm25_score'
# æ¥ç€å»åš RRF...
```

### ğŸ’¡ æ ¸å¿ƒæ”¹åŠ¨æ€»ç»“

1.  **æŠ›å¼ƒ `get_scores`**: ä¸è¦è®¡ç®—å…¨åº“åˆ†æ•°ã€‚
2.  **æŠ›å¼ƒ `_load_tokenized_chunks`**: ä¸è¦åŠ è½½é‚£ä¸ªå·¨å¤§çš„ Parquet æ–‡ä»¶ã€‚
3.  **ç°åœºåˆ†è¯**: åˆ©ç”¨ Dense ç»“æœä¸­å·²æœ‰çš„ `text` å­—æ®µï¼Œç°åœºå¯¹è¿™ 600 ä¸ªæ–‡æœ¬åˆ†è¯ã€‚
    *   *æ€§èƒ½å¯¹æ¯”*ï¼šåŠ è½½ 500MB æ–‡ä»¶éœ€è¦ 2-5 ç§’ï¼›å¯¹ 600 ä¸ªçŸ­æ–‡æœ¬åˆ†è¯åªéœ€è¦ 0.05 ç§’ã€‚
4.  **æŒ‰éœ€è®¡ç®—**: åªç®—è¿™ 600 ä¸ªã€‚

è¿™æ ·ä¿®æ”¹åï¼Œä½ çš„ BM25 Rerank æ­¥éª¤è€—æ—¶å°†ä» **ç§’çº§** é™ä½åˆ° **æ¯«ç§’çº§**ã€‚