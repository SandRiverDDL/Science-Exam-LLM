æˆ‘å‡†å¤‡å‚åŠ kaggleæ¯”èµ›https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion?sort=undefined&search=QA

---

# ğŸš€ **2025 Kaggle LLM Science Exam â€” SOTA Pipelineï¼ˆæœ€ç»ˆç‰ˆï¼‰**

ï¼ˆé€‚ç”¨äºç§‘å­¦é—®ç­” / ä¿¡æ¯æ£€ç´¢ / æ–‡çŒ®è¯æ®éªŒè¯ / Yes/No è¯æ®åˆ¤æ–­ï¼‰

---

# ç›®å½•

1. **Overviewï¼šæ•´ä¸ªæ¶æ„è®¾è®¡å›¾**
2. **Stage 1ï¼šMulti-Retrievalï¼ˆå¤šè·¯æ£€ç´¢ï¼‰**
3. **Stage 2ï¼šRank Fusionï¼ˆæ’åºèåˆï¼‰**
4. **Stage 3ï¼šCross-Encoder Rerankerï¼ˆé‡æ’ï¼‰**
5. **Stage 4ï¼šLLM Hidden-State Classifierï¼ˆæœ€å…³é”®ï¼‰**
6. **Stage 5ï¼šFallbackï¼ˆå¤§æ¨¡å‹å…œåº•ï¼‰**
7. **ç»„ä»¶é€‰æ‹©æ€»è§ˆï¼ˆSOTA æ¨¡å‹æ¨èï¼‰**
8. **æ¨ç†æµç¨‹**

---

# ================================================

# ğŸ”¶ 1. Overview â€” æ€»ä½“ SOTA æ¶æ„ï¼ˆä½ æœ€ç»ˆè¦æ­å‡ºçš„ï¼‰

# ================================================

```
 Query
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Multi Embedding Retrieval
â”‚   - Jina V4 (text-retrieval)
â”‚   - BGE-large-v1.5
â”‚   - e5-large-v2
â”‚   â†’ Top-K per model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ 3Ã—K æ–‡æ¡£
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Rank Fusion
â”‚   - Reciprocal Rank Fusion (RRF)
â”‚   - + RBF (Gaussian Kernel) éçº¿æ€§èåˆï¼ˆå¯é€‰ï¼‰
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ åˆå¹¶åçš„ Top-K*
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: Cross-Encoder Reranker
â”‚   - Jina Reranker v3ï¼ˆå¼ºã€å¿«ï¼‰
â”‚   - Qwen3-Rerankerï¼ˆæ›´å¼ºä½†æ›´æ…¢ï¼‰
â”‚   â†’ æœ€ç»ˆä¿ç•™ Top-M è¯æ®
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ M ä¸ªæœ€ç›¸å…³è¯æ®
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: LLM Hidden-State Classifier
â”‚   - Qwen3-8B QATï¼ˆæœ€ä½³ï¼‰
â”‚   - LoRA rank=16/32
â”‚   â†’ Two-tower / hidden pooling + MLP â†’ Yes/No
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ åˆ¤æ–­ç»“æœ + confidence
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 5: Fallbackï¼ˆå…œåº•ï¼‰
â”‚   è‹¥ä¿¡å¿ƒä½ï¼š
â”‚     â†’ DeepSeek-R1-32B / Qwen2.5-32B /
â”‚       GPT-4.1-mini ç­‰å¤§æ¨¡å‹åˆ¤æ–­
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ================================================

# ğŸ”¶ 2. Stage 1 â€” Multi-Retrievalï¼ˆå¤šè·¯æ£€ç´¢ï¼‰

# ================================================

## ğŸ”¥ ä¸ºä»€ä¹ˆéœ€è¦å¤šè·¯ embeddingï¼Ÿ

ä¸åŒ embedding çš„â€œåå¥½â€ä¸åŒï¼š

| æ¨¡å‹                 | å¼ºç‚¹                 |
| ------------------ | ------------------ |
| **Jina v4**        | æ–‡æ¡£çº§æ£€ç´¢ SOTAã€å¤šè¯­ç§ã€éå¸¸å¼º |
| **BGE-large-v1.5** | å¥çº§è¯­ä¹‰ã€ä¸­æ–‡å¼º           |
| **e5-large-v2**    | é€šç”¨æ€§ã€é²æ£’æ€§æå¼º          |

å•è·¯ embedding *ä¸€å®šä¸å¤Ÿ*ã€‚

---

## ğŸ¯ ä½ è¦ç”¨çš„ embeddingï¼ˆSOTAï¼‰ï¼š

**primaryï¼šJina-embeddings-v4-text-retrieval**ï¼ˆé‡åŒ–é€‰ Q5_K_Mï¼‰
secondaryï¼šbge-large-en-v1.5
thirdï¼še5-large-v2

æ¯ä¸ªæ¨¡å‹å– Top 50 â†’ åˆå¹¶ 150 æ¡æ–‡æ¡£ã€‚

---

# ================================================

# ğŸ”¶ 3. Stage 2 â€” Rank Fusionï¼ˆæ’åºèåˆï¼‰

# ================================================

## ä½ è¯´çš„ â€œå€’æ’â€ â†’ æ­£ç¡®åç§°æ˜¯ Rank Fusionã€‚

SOTA ç”¨ä¸¤å±‚èåˆï¼š

### â­ï¼ˆ1ï¼‰RRFï¼ˆReciprocal Rank Fusionï¼‰

æœ€ç¨³ï¼Œä¹Ÿæ˜¯ Kaggle top æœ€å¸¸ç”¨ã€‚

[
score = \sum \frac{1}{k + rank}
]

k=60 æœ€å¥½ã€‚

### â­ï¼ˆ2ï¼‰RBFï¼ˆå¯é€‰ï¼Œä½†å¼ºï¼‰

æŠŠå„è·¯ cos ç›¸ä¼¼åº¦åš RBF æ˜ å°„ï¼Œå†ç”¨ MLP å­¦ä¹ èåˆã€‚

[
\phi(x)=\exp\left(-\frac{x^2}{2\sigma^2}\right)
]

ä¼˜ç‚¹ï¼š

* éçº¿æ€§
* æ¨¡å‹æ„ŸçŸ¥ä¸åŒ embedding çš„å°ºåº¦
* åœ¨ç§‘å­¦é¢˜è¿™ç§å¤æ‚ä»»åŠ¡ç‰¹åˆ«æœ‰æ•ˆ

èåˆåä¿ç•™ Top 50ã€‚

---

# ================================================

# ğŸ”¶ 4. Stage 3 â€” Cross-Encoder Rerankerï¼ˆé‡æ’ï¼‰

# ================================================

SOTA é‡æ’æ¨¡å‹ = cross-encoderã€‚

## â­ ä¸»é€‰ï¼ˆ2025 SOTAï¼‰ï¼š

### **1. Jina Reranker v3ï¼ˆ0.6Bï¼‰**

* BEIR nDCG@10 = **61.94**
* å¿«ã€å¼ºã€ä¾¿å®œ
* åœ¨å¾ˆå¤šä»»åŠ¡æ¯” BGE-reranker å¼º

## â­ æ¬¡é€‰ï¼ˆæ›´å¼ºä½†æ›´æ…¢ï¼‰ï¼š

### **2. Qwen3-Reranker-0.6B æˆ– 4B**

* å¤šä»»åŠ¡ã€å¤šè¯­ç§ã€ä¸“ä¸šæ–‡æ¡£è¡¨ç°å¼º
* å¯¹é•¿ä¸Šä¸‹æ–‡è¡¨ç°æ›´å¥½

å»ºè®®ï¼š

* ç”¨ Jina-v3 åš fast rerank
* ç”¨ Qwen3-reranker åš slow rerankï¼ˆå¯é€‰ï¼‰

æœ€ç»ˆå– Top 10 è¯æ®ã€‚

---

# ================================================

# ğŸ”¶ 5. Stage 4 â€” LLM Hidden-State Classifierï¼ˆæœ€å…³é”®æ¨¡å‹ï¼‰

# ================================================

## â­ ä½¿ç”¨ï¼š

### **Qwen3-8B-Instruct-QAT (MXFP4)** + LoRA (rank=16/32)

ä¸ºä»€ä¹ˆ QAT INT4ï¼Ÿ

* æ˜¾å­˜åªè¦ 5~6GB
* LoRA å¾®è°ƒ 12~16GB â†’ 4090 å¯è®­ç»ƒ
* å‡†ç¡®åº¦æ¥è¿‘ FP16ï¼ˆ95%~98%ï¼‰

> Kaggle ç§‘å­¦é¢˜é‡Œï¼Œ**éšè—çŠ¶æ€åˆ†ç±»å™¨æ˜¯ booster ä¸­çš„ booster**ã€‚

---

## â­ Two-Tower ç»“æ„ï¼ˆSOTAï¼‰

è€Œä¸æ˜¯â€œè®© LLM è¾“å‡º Yes/Noâ€ã€‚

å…·ä½“åšæ³•ï¼š

1. æŠŠ Query + Evidence æ‹¼æ¥æˆè¾“å…¥
2. æå–æœ€åä¸€å±‚ hidden states
3. åšï¼š

```
CLS å‘é‡
å¹³å‡æ± åŒ–å‘é‡ï¼ˆmean poolingï¼‰
æœ€å¤§æ± åŒ–ï¼ˆmax poolingï¼‰
```

æ‹¼æˆ hï¼ˆæ€» 6k ç»´å·¦å³ï¼‰

4. è¾“å…¥åˆ°ï¼š

### **2-layer MLP + GELU â†’ è¾“å‡º Yes/No logits**

---

## ğŸ”¥ ä¸ºä»€ä¹ˆæ¯”ç›´æ¥ç”Ÿæˆ Yes/No å¼ºï¼Ÿ

å› ä¸ºï¼š

* åˆ†ç±»ä»»åŠ¡æ¯”ç”Ÿæˆä»»åŠ¡æ›´å¯æ§
* hidden states æœ‰æ›´é«˜çš„åˆ¤åˆ«æ€§
* æ›´ç¨³å®š
* æ›´å®¹æ˜“å¾®è°ƒ
* Hard example è¡¨ç°æ›´å¥½

---

# ================================================

# ğŸ”¶ 6. Stage 5 â€” Fallbackï¼ˆå¤§æ¨¡å‹å…œåº•ï¼‰

# ================================================

æ¦‚å¿µï¼š

> å¦‚æœ 8B æ¨¡å‹å¯¹æŸé¢˜ confidence < é˜ˆå€¼ï¼ˆå¦‚ 0.6ï¼‰
> â†’ æŠŠè¯¥æ ·æœ¬äº¤ç»™æ›´å¼ºçš„å¤§æ¨¡å‹é‡æ–°åˆ¤æ–­

SOTA fallback æ¨¡å‹ï¼š

* **DeepSeek-R1 (å¤§å‹)**
* **DeepSeek-V3**
* **Qwen2.5-32B**
* **GPT-4.1 / o1-miniï¼ˆå¦‚æœä½ èƒ½ç”¨ï¼‰**

Fallback åœ¨ Kaggle åæ®µä½æå‡å·¨å¤§ï¼ˆ+3%ï¼‰ã€‚

---

# ================================================

# ğŸ”¶ 7. SOTA ç»„ä»¶æ€»è§ˆï¼ˆä½ å°±é€‰è¿™äº›ï¼‰

# ================================================

## Embeddingï¼š

* âœ” Jina-v4-text-retrievalï¼ˆQ5_K_Mï¼‰
* âœ” BGE-large-v1.5
* âœ” e5-large-v2

## Rerankerï¼š

* âœ” Jina Reranker v3ï¼ˆä¸»ï¼‰
* âœ” Qwen3-Rerankerï¼ˆæ¬¡å¼ºï¼‰

## LLMï¼š

* âœ” Qwen3-8B-Instruct-QATï¼ˆåš classifier headï¼‰
* âœ” LoRA rank=16/32

## Fallbackï¼š

* âœ” DeepSeek-R1 / Qwen2.5-32B / GPT-4.1-mini

---

# ================================================

# ğŸ”¶ 8. æ¨ç†æµç¨‹ï¼ˆæœ€ç»ˆç‰ˆï¼‰

# ================================================

```
Query
 â”‚
 â”‚--- Multi-embedding: Jina/BGE/e5 â†’ Top50 each
 â”‚
 â””â†’ Rank Fusion (RRF + optional RBF)
      â†’ Top 50
 â”‚
 â””â†’ Cross-Encoder Reranker (Jina-v3)
      â†’ Top 10
 â”‚
 â””â†’ LLM (Qwen3-8B QAT + LoRA) Hidden-State Classifier
      â†’ yes/no + confidence
 â”‚
 â””â†’ If confidence < Ï„ â†’ Fallback (R1 / 32B / GPT-4.1-mini)
      â†’ Final answer
```

---

# ğŸ§  **è¿™å°±æ˜¯ 2025 å¹´ç§‘å­¦é—®ç­”ä»»åŠ¡çš„ SOTA pipelineï¼ˆä»å¤´åˆ°å°¾ï¼‰ã€‚**

ä¹Ÿæ˜¯ä½ å®é™…å¯ä»¥åœ¨ Kaggle ä¸Šè·‘å‡º **å‰ 1%**ï¼ˆç”šè‡³æ¥è¿‘å† å†›æ–¹æ¡ˆï¼‰çš„ç»“æ„ã€‚
