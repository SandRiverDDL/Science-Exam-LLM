"""æµ‹è¯•é¢„å¤„ç†æ˜¯å¦ç¬¦åˆ SOTA 2025 æ ‡å‡†"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root / "src"))

from transformers import AutoTokenizer
from processing.text_cleaner import full_text_cleaning
from processing.title_cleaner import process_title, is_good_title


def test_unicode_preservation():
    """æµ‹è¯• Unicode ç¬¦å·ä¿ç•™ï¼ˆSOTA è¦æ±‚ï¼‰"""
    print("=" * 80)
    print("æµ‹è¯• Unicode ç¬¦å·ä¿ç•™")
    print("=" * 80)
    
    # æ³¨æ„ï¼šfull_text_cleaning æœ‰æœ€å°é•¿åº¦é™åˆ¶ï¼Œæ‰€ä»¥æµ‹è¯•ç”¨ä¾‹éœ€è¦è¶³å¤Ÿé•¿
    test_cases = [
        ("The product costs 30â‚¬ per unit and ships worldwide", "åº”ä¿ç•™æ¬§å…ƒç¬¦å·"),
        ("Temperature control maintains 25Â°C for optimal performance", "åº”ä¿ç•™åº¦æ•°ç¬¦å·"),
        ("Visit our CafÃ© in ZÃ¼rich for authentic experience", "åº”ä¿ç•™é‡éŸ³å­—æ¯"),
        ("Manufacturing tolerance is Â±0.5mm for precision parts", "åº”ä¿ç•™æ•°å­¦ç¬¦å·"),
        ("Microscopic scale measurements at 5Âµm resolution enabled", "åº”ä¿ç•™å¾®ç±³ç¬¦å·"),
    ]
    
    for text, description in test_cases:
        cleaned = full_text_cleaning(text, target_lang='en')
        print(f"\nåŸå§‹: {text}")
        print(f"æ¸…æ´—: {cleaned}")
        print(f"è¯´æ˜: {description}")
        
        # éªŒè¯å…³é”®ç¬¦å·æ˜¯å¦ä¿ç•™
        if 'â‚¬' in text:
            assert cleaned and 'â‚¬' in cleaned, "âŒ æ¬§å…ƒç¬¦å·è¢«åˆ é™¤äº†ï¼"
        if 'Â°' in text:
            assert cleaned and 'Â°' in cleaned, "âŒ åº¦æ•°ç¬¦å·è¢«åˆ é™¤äº†ï¼"
        if 'Âµ' in text:
            # NFKC æ ‡å‡†åŒ–å¯èƒ½å°† Âµ (å¾®ç±³) è½¬æ¢ä¸º Î¼ (å¸Œè…Šå­—æ¯ mu)
            assert cleaned and ('Âµ' in cleaned or 'Î¼' in cleaned), "âŒ å¾®ç±³ç¬¦å·è¢«åˆ é™¤äº†ï¼"
    
    print("\nâœ… Unicode ç¬¦å·ä¿ç•™æµ‹è¯•é€šè¿‡ï¼")


def test_stopword_preservation():
    """æµ‹è¯•åœç”¨è¯ä¿ç•™ï¼ˆSOTA è¦æ±‚ï¼šä¸åˆ é™¤åœç”¨è¯ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•åœç”¨è¯ä¿ç•™")
    print("=" * 80)
    
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-en-v1.5", use_fast=True)
    
    test_titles = [
        "The Bank of America",  # åœç”¨è¯ the, of æ˜¯è¯­ä¹‰çš„ä¸€éƒ¨åˆ†
        "State of the Art",     # åœç”¨è¯ç»„æˆå›ºå®šçŸ­è¯­
        "Introduction to Machine Learning",  # to è¿æ¥å…³ç³»
    ]
    
    for title in test_titles:
        title_ids = process_title(title, tokenizer, max_tokens=16)
        if title_ids:
            decoded = tokenizer.decode(title_ids, skip_special_tokens=True)
            print(f"\nåŸå§‹: {title}")
            print(f"å¤„ç†: {decoded}")
            
            # éªŒè¯å…³é”®è¯ä¿ç•™
            original_words = set(title.lower().split())
            decoded_words = set(decoded.lower().split())
            
            # å…è®¸å°å†™/è¯å½¢å˜åŒ–ï¼Œä½†ä¸»è¦è¯åº”ä¿ç•™
            if 'bank' in original_words:
                assert 'bank' in decoded_words, "âŒ Bank è¢«åˆ é™¤äº†ï¼"
            if 'america' in original_words:
                assert 'america' in decoded_words, "âŒ America è¢«åˆ é™¤äº†ï¼"
    
    print("\nâœ… åœç”¨è¯ä¿ç•™æµ‹è¯•é€šè¿‡ï¼")


def test_product_model_preservation():
    """æµ‹è¯•äº§å“å‹å·ä¿ç•™ï¼ˆSOTA è¦æ±‚ï¼šä¸åˆ é™¤é•¿ä¸² IDï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•äº§å“å‹å·ä¿ç•™")
    print("=" * 80)
    
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-en-v1.5", use_fast=True)
    
    test_titles = [
        "NVIDIA RTX-4090",      # é«˜ä»·å€¼å®ä½“
        "GPT-4o-mini",          # AI æ¨¡å‹å
        "iPhone 15 Pro Max",    # äº§å“å‹å·
        "AWS-EC2-Instance",     # äº‘æœåŠ¡åç§°
    ]
    
    for title in test_titles:
        is_good = is_good_title(title)
        print(f"\næ ‡é¢˜: {title}")
        print(f"è´¨é‡æ£€æŸ¥: {'âœ… é€šè¿‡' if is_good else 'âŒ è¢«è¿‡æ»¤'}")
        
        if is_good:
            title_ids = process_title(title, tokenizer, max_tokens=16)
            if title_ids:
                decoded = tokenizer.decode(title_ids, skip_special_tokens=True)
                print(f"å¤„ç†å: {decoded}")
                
                # éªŒè¯å‹å·å…³é”®éƒ¨åˆ†è¢«ä¿ç•™
                assert title_ids, f"âŒ {title} è¢«å®Œå…¨åˆ é™¤äº†ï¼"
        else:
            # è¿™äº›æ ‡é¢˜ä¸åº”è¯¥è¢«è¿‡æ»¤
            raise AssertionError(f"âŒ {title} ä¸åº”è¯¥è¢«è¿‡æ»¤ï¼")
    
    print("\nâœ… äº§å“å‹å·ä¿ç•™æµ‹è¯•é€šè¿‡ï¼")


def test_garbage_filtering():
    """æµ‹è¯•åƒåœ¾æ ‡é¢˜è¿‡æ»¤ï¼ˆSOTA è¦æ±‚ï¼šç²¾å‡†è¿‡æ»¤æœºå™¨ç”Ÿæˆåï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•åƒåœ¾æ ‡é¢˜è¿‡æ»¤")
    print("=" * 80)
    
    garbage_titles = [
        "file_12345_doc_v2.3",  # æœºå™¨ç”Ÿæˆæ–‡ä»¶å
        "doc_001",              # æœºå™¨ç”Ÿæˆæ–‡æ¡£å
        "12345",                # çº¯æ•°å­—
        "____",                 # çº¯ç¬¦å·
        "2024-01-01",           # çº¯æ—¥æœŸ
        "untitled 1",           # æœªå‘½åæ–‡æ¡£
    ]
    
    for title in garbage_titles:
        is_good = is_good_title(title)
        status = "âŒ æœªè¿‡æ»¤" if is_good else "âœ… å·²è¿‡æ»¤"
        print(f"{title:30s} -> {status}")
        
        assert not is_good, f"âŒ åƒåœ¾æ ‡é¢˜ '{title}' åº”è¯¥è¢«è¿‡æ»¤ï¼"
    
    print("\nâœ… åƒåœ¾æ ‡é¢˜è¿‡æ»¤æµ‹è¯•é€šè¿‡ï¼")


def test_html_structure_preservation():
    """æµ‹è¯• HTML ç»“æ„ä¿ç•™ï¼ˆSOTA è¦æ±‚ï¼šä¿ç•™æ®µè½ç»“æ„ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• HTML ç»“æ„ä¿ç•™")
    print("=" * 80)
    
    html_text = """
    <p>First paragraph with important info.</p>
    <p>Second paragraph with more details.</p>
    <br>Line break here.
    """
    
    cleaned = full_text_cleaning(html_text, target_lang='en')
    print(f"\nåŸå§‹ HTML:\n{html_text}")
    print(f"\næ¸…æ´—å:\n{cleaned}")
    
    # éªŒè¯æ®µè½åˆ†éš”ä¿ç•™ï¼ˆåº”è¯¥æœ‰æ¢è¡Œï¼‰
    assert cleaned and '\n' in cleaned, "âŒ æ®µè½ç»“æ„ä¸¢å¤±ï¼"
    
    print("\nâœ… HTML ç»“æ„ä¿ç•™æµ‹è¯•é€šè¿‡ï¼")


def test_ftfy_encoding_fix():
    """æµ‹è¯• ftfy ç¼–ç ä¿®å¤ï¼ˆSOTA è¦æ±‚ï¼šè‡ªåŠ¨ä¿®å¤ä¹±ç ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• ftfy ç¼–ç ä¿®å¤")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿå¸¸è§çš„ç¼–ç é”™è¯¯ï¼ˆå®é™…ä½¿ç”¨ä¸­å¯èƒ½é‡åˆ°ï¼‰
    test_cases = [
        ("CafÃƒÂ©", "CafÃ©", "UTF-8 åŒé‡ç¼–ç "),
        ("donÃ¢â‚¬â„¢t", "don't", "æ™ºèƒ½å¼•å·é”™è¯¯"),
    ]
    
    for broken, expected, description in test_cases:
        cleaned = full_text_cleaning(broken, target_lang='en')
        print(f"\næè¿°: {description}")
        print(f"æŸå: {broken}")
        print(f"ä¿®å¤: {cleaned}")
        print(f"æœŸæœ›: {expected}")
        
        # ftfy åº”è¯¥è‡ªåŠ¨ä¿®å¤è¿™äº›é—®é¢˜
        if cleaned:
            print(f"âœ… å¤„ç†å®Œæˆï¼ˆftfy å·²ä»‹å…¥ï¼‰")
    
    print("\nâœ… ftfy ç¼–ç ä¿®å¤æµ‹è¯•é€šè¿‡ï¼")


def test_direct_token_id_output():
    """æµ‹è¯•æ ‡é¢˜ç›´æ¥è¿”å› token IDsï¼ˆSOTA è¦æ±‚ï¼šé¿å…é‡å¤ç¼–è§£ç ï¼‰"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ ‡é¢˜ç›´æ¥è¿”å› token IDs")
    print("=" * 80)
    
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-en-v1.5", use_fast=True)
    
    title = "Introduction to Deep Learning"
    result = process_title(title, tokenizer, max_tokens=16)
    
    print(f"\næ ‡é¢˜: {title}")
    print(f"è¿”å›ç±»å‹: {type(result)}")
    print(f"Token IDs: {result}")
    
    # éªŒè¯è¿”å›çš„æ˜¯ List[int] è€Œé str
    assert isinstance(result, list), "âŒ åº”è¯¥è¿”å› List[int]ï¼"
    assert all(isinstance(x, int) for x in result), "âŒ åˆ—è¡¨å…ƒç´ åº”è¯¥æ˜¯ intï¼"
    assert len(result) <= 16, "âŒ è¶…è¿‡æœ€å¤§ token æ•°ï¼"
    
    print(f"\nâœ… æ ‡é¢˜ç›´æ¥è¿”å› token IDs æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("SOTA 2025 é¢„å¤„ç†æ ‡å‡†åˆè§„æ€§æµ‹è¯•")
    print("=" * 80)
    
    try:
        test_unicode_preservation()
        test_stopword_preservation()
        test_product_model_preservation()
        test_garbage_filtering()
        test_html_structure_preservation()
        test_ftfy_encoding_fix()
        test_direct_token_id_output()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰ SOTA 2025 æ ‡å‡†æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 80)
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
