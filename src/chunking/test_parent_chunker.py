"""æµ‹è¯•çˆ¶æ–‡æ¡£ç´¢å¼• chunking"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root / "src"))

from transformers import AutoTokenizer
from chunking.parent_chunker import ParentDocumentChunker


def test_basic_chunking():
    """æµ‹è¯•åŸºæœ¬çš„ chunking åŠŸèƒ½"""
    print("=" * 80)
    print("æµ‹è¯•çˆ¶æ–‡æ¡£ç´¢å¼• Chunking")
    print("=" * 80)
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-en-v1.5", use_fast=True)
    
    # åˆ›å»º chunker
    chunker = ParentDocumentChunker(
        tokenizer=tokenizer,
        child_size=128,
        parent_size=512,
        min_chunk_tokens=32
    )
    
    # æµ‹è¯•æ–‡æ¡£
    title = "Introduction to Machine Learning"
    text = """
    Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. 
    It focuses on the development of computer programs that can access data and use it to learn for themselves.
    
    The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data 
    and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without 
    human intervention or assistance and adjust actions accordingly.
    
    Machine learning algorithms are categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.
    Supervised learning algorithms learn from labeled training data, helping predict outcomes for unforeseen data.
    Unsupervised learning algorithms work with unlabeled data to discover hidden patterns or intrinsic structures.
    Reinforcement learning is about taking suitable action to maximize reward in a particular situation.
    """ * 3  # é‡å¤3æ¬¡ç¡®ä¿è¶³å¤Ÿé•¿
    
    # Encode
    title_ids = tokenizer.encode(title, add_special_tokens=False)
    doc_ids = tokenizer.encode(text, add_special_tokens=False)
    
    print(f"\næ–‡æ¡£ä¿¡æ¯:")
    print(f"  æ ‡é¢˜: {title}")
    print(f"  æ ‡é¢˜tokenæ•°: {len(title_ids)}")
    print(f"  æ­£æ–‡tokenæ•°: {len(doc_ids)}")
    
    # Chunking
    chunks = chunker.chunk_document(
        doc_id="test_doc_001",
        title_ids=title_ids,
        doc_ids=doc_ids,
        title_text=title
    )
    
    print(f"\nChunkingç»“æœ:")
    print(f"  ç”Ÿæˆchunkæ•°: {len(chunks)}")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\n  Chunk {i}:")
        print(f"    - chunk_id: {chunk['chunk_id']}")
        print(f"    - å­chunké•¿åº¦: {chunk['chunk_len']}")
        print(f"    - çˆ¶chunkèŒƒå›´: [{chunk['parent_start']}, {chunk['parent_end']})")
        print(f"    - çˆ¶chunké•¿åº¦: {chunk['parent_end'] - chunk['parent_start']}")
        print(f"    - rerank_texté•¿åº¦: {len(chunk['rerank_text'])} chars")
        print(f"    - rerank_textå‰100å­—ç¬¦: {chunk['rerank_text'][:100]}...")
    
    # éªŒè¯çº¦æŸ
    print(f"\néªŒè¯:")
    for i, chunk in enumerate(chunks):
        child_len = len(chunk['child_ids'])
        parent_len = chunk['parent_end'] - chunk['parent_start']
        
        # å­chunkåº”è¯¥ <= 128
        assert child_len <= 128, f"Chunk {i}: å­chunkå¤ªé•¿ ({child_len})"
        
        # çˆ¶chunkåº”è¯¥å°½å¯èƒ½æ¥è¿‘512
        # ï¼ˆæœ€åä¸€ä¸ªchunkå¯èƒ½è¾ƒçŸ­ï¼‰
        if i < len(chunks) - 1:
            assert parent_len >= 400, f"Chunk {i}: çˆ¶chunkå¤ªçŸ­ ({parent_len})"
        
        # çˆ¶chunkåº”è¯¥åŒ…å«å­chunk
        child_start_in_doc = chunk['parent_start']  # ç®€åŒ–å‡è®¾
        assert chunk['parent_start'] <= child_start_in_doc, f"Chunk {i}: çˆ¶chunkä¸åŒ…å«å­chunk"
        
        print(f"  âœ… Chunk {i}: å­chunk={child_len}, çˆ¶chunk={parent_len}")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


def test_embedding_input():
    """æµ‹è¯• embedding è¾“å…¥å‡†å¤‡"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• Embedding è¾“å…¥å‡†å¤‡")
    print("=" * 80)
    
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-small-en-v1.5", use_fast=True)
    chunker = ParentDocumentChunker(tokenizer=tokenizer, child_size=128)
    
    title_ids = [101, 102, 103]
    child_ids = [201, 202, 203, 204, 205]
    
    # å‡†å¤‡è¾“å…¥
    input_ids = chunker.prepare_embedding_inputs(title_ids, child_ids, add_special_tokens=True)
    
    print(f"\nè¾“å…¥å‡†å¤‡:")
    print(f"  æ ‡é¢˜IDs: {title_ids}")
    print(f"  å­chunk IDs: {child_ids}")
    print(f"  å®Œæ•´è¾“å…¥IDs: {input_ids}")
    print(f"  å®Œæ•´è¾“å…¥é•¿åº¦: {len(input_ids)}")
    
    # éªŒè¯ç»“æ„
    assert input_ids[0] == tokenizer.cls_token_id, "åº”è¯¥ä»¥[CLS]å¼€å¤´"
    assert tokenizer.sep_token_id in input_ids, "åº”è¯¥åŒ…å«[SEP]"
    
    print("\nâœ… Embedding è¾“å…¥å‡†å¤‡æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    try:
        test_basic_chunking()
        test_embedding_input()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¶æ–‡æ¡£ç´¢å¼• chunking æ­£å¸¸å·¥ä½œï¼")
        print("=" * 80)
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
