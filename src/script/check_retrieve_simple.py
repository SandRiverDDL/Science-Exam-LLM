"""ç®€åŒ–ç‰ˆæ£€ç´¢æµ‹è¯•è„šæœ¬ï¼šä¸éœ€è¦å®Œæ•´è§£å‹ç´¢å¼•

æµç¨‹ï¼š
1. ä»chunks.parquetéšæœºè¯»å–5ä¸ªchunk
2. ä»documents_cleaned.parquetè·å–è¿™äº›chunkçš„åŸå§‹æ–‡æœ¬
3. ä½¿ç”¨Qwen3 embeddingè¿›è¡Œå‘é‡åŒ–
4. ä¸ç›¸åŒæ–‡æ¡£çš„å…¶ä»–chunksè®¡ç®—ç›¸ä¼¼åº¦
5. éªŒè¯åŒæ–‡æ¡£chunksçš„ç›¸ä¼¼æ€§
"""
import sys
import random
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src'))

try:
    from retrieval.embedding_qwen import Qwen3EmbeddingModel
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥Qwen3EmbeddingModel")
    exit(1)


class SimpleRetrievalChecker:
    """ç®€åŒ–ç‰ˆæ£€ç´¢æ£€éªŒå™¨ - ä¸ä¾èµ–FAISSç´¢å¼•"""
    
    def __init__(self):
        self.chunks_path = project_root / 'data' / 'processed' / 'chunks.parquet'
        self.docs_path = project_root / 'data' / 'processed' / 'documents_cleaned.parquet'
        
        self.df_chunks = None
        self.df_docs = None
        self.embedding_model = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\n[1] åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            print("  ğŸ“¥ åŠ è½½ chunks.parquet...", end='', flush=True)
            self.df_chunks = pd.read_parquet(self.chunks_path)
            print(f" âœ… ({len(self.df_chunks):,}æ¡)")
            
            print("  ğŸ“¥ åŠ è½½ documents_cleaned.parquet...", end='', flush=True)
            self.df_docs = pd.read_parquet(self.docs_path)
            print(f" âœ… ({len(self.df_docs):,}æ¡)")
            
        except Exception as e:
            print(f"\n  âŒ åŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        print("\n[2] åˆå§‹åŒ–Qwen3 Embeddingæ¨¡å‹...")
        try:
            import os
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ä¸‹è½½
            os.environ['HF_DATASETS_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            self.embedding_model = Qwen3EmbeddingModel(
                model_id="Qwen/Qwen3-Embedding-0.6B",
                device="cuda",
                dtype="float16"
            )
            print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"\n  ğŸ’¡ æç¤º: å¦‚æœæ˜¯ä¸‹è½½å¡ä½ï¼Œè¯·å°è¯•ï¼š")
            print(f"     1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"     2. è®¾ç½®ä»£ç†: export HF_ENDPOINT=https://huggingface-mirror.com")
            print(f"     3. é¢„ä¸‹è½½æ¨¡å‹: huggingface-cli download Alibaba-NLP/gte-Qwen2-1.5B-instruct")
            return False
    
    def get_chunk_text(self, chunk_id: str) -> str:
        """è·å–chunkæ–‡æœ¬"""
        try:
            chunk_row = self.df_chunks[self.df_chunks['chunk_id'] == chunk_id].iloc[0]
            doc_id = chunk_row['doc_id']
            doc_row = self.df_docs[self.df_docs['doc_id'] == doc_id]
            
            if len(doc_row) == 0:
                return f"[æœªæ‰¾åˆ°doc_id: {doc_id}]"
            
            doc_text = doc_row.iloc[0]['text']
            title = chunk_row['title']
            child_start = chunk_row['child_start']
            child_end = chunk_row['child_end']
            chunk_text = doc_text[child_start:child_end]
            
            return f"[{title}]\n{chunk_text}"
        
        except Exception as e:
            return f"[é”™è¯¯: {e}]"
    
    def get_doc_chunks(self, doc_id: str) -> List[str]:
        """è·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunk_id"""
        return self.df_chunks[self.df_chunks['doc_id'] == doc_id]['chunk_id'].tolist()
    
    def search_similar_in_doc(self, query_embedding: np.ndarray, chunk_ids: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """åœ¨ç»™å®šçš„chunkä¸­æœç´¢ç›¸ä¼¼çš„"""
        try:
            # å¯¹æ‰€æœ‰chunkè¿›è¡Œembedding
            chunk_texts = [self.get_chunk_text(cid).split('\n', 1)[1] if '\n' in self.get_chunk_text(cid) else self.get_chunk_text(cid) 
                          for cid in chunk_ids]
            chunk_embeddings = self.embedding_model.encode(chunk_texts, batch_size=32)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding.reshape(1, -1), chunk_embeddings)[0]
            
            # æ’åº
            sorted_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in sorted_indices:
                results.append((chunk_ids[idx], float(similarities[idx])))
            
            return results
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def run_test(self, num_samples: int = 5):
        """è¿è¡Œæµ‹è¯•"""
        print("\n" + "=" * 100)
        print("ç®€åŒ–ç‰ˆæ£€ç´¢æµ‹è¯• - åŒæ–‡æ¡£å†…æ£€ç´¢")
        print("=" * 100)
        
        if not self.load_data():
            return
        
        if not self.init_embedding_model():
            return
        
        print(f"\n[3] éšæœºé‡‡æ · {num_samples} ä¸ªchunks è¿›è¡Œæ£€ç´¢æµ‹è¯•...\n")
        
        # éšæœºé‡‡æ ·
        sample_indices = random.sample(range(len(self.df_chunks)), num_samples)
        
        for test_idx, chunk_idx in enumerate(sample_indices, 1):
            chunk_row = self.df_chunks.iloc[chunk_idx]
            chunk_id = chunk_row['chunk_id']
            doc_id = chunk_row['doc_id']
            
            print(f"\n{'='*100}")
            print(f"æµ‹è¯• {test_idx}/{num_samples}: {chunk_id}")
            print(f"{'='*100}")
            
            # è·å–chunkæ–‡æœ¬
            chunk_text = self.get_chunk_text(chunk_id)
            text_only = chunk_text.split('\n', 1)[1] if '\n' in chunk_text else chunk_text
            
            print(f"\nğŸ“„ æŸ¥è¯¢Chunk:")
            print("-" * 100)
            print(text_only[:300] + ("..." if len(text_only) > 300 else ""))
            print("-" * 100)
            
            # Embeddingè¯¥chunk
            print(f"\nğŸ”„ å¯¹æŸ¥è¯¢chunkè¿›è¡Œembedding...", end='', flush=True)
            query_embedding = self.embedding_model.encode([text_only], batch_size=1)[0]
            print(" âœ…")
            
            # è·å–åŒæ–‡æ¡£çš„å…¶ä»–chunks
            all_doc_chunks = self.get_doc_chunks(doc_id)
            print(f"\nğŸ“Š è¯¥æ–‡æ¡£å…±æœ‰ {len(all_doc_chunks)} ä¸ªchunks")
            
            # æœç´¢
            print(f"\nğŸ” åœ¨åŒæ–‡æ¡£å†…æœç´¢Top5ç›¸ä¼¼chunks...\n")
            results = self.search_similar_in_doc(query_embedding, all_doc_chunks, top_k=5)
            
            # æ˜¾ç¤ºç»“æœ
            for rank, (result_chunk_id, similarity) in enumerate(results, 1):
                is_self = "âœ… [æœ¬èº«]" if result_chunk_id == chunk_id else ""
                result_text = self.get_chunk_text(result_chunk_id)
                text_only_result = result_text.split('\n', 1)[1] if '\n' in result_text else result_text
                
                print(f"\nã€Top {rank}ã€‘ç›¸ä¼¼åº¦: {similarity:.4f} {is_self}")
                print(f"ID: {result_chunk_id}")
                print(f"å†…å®¹: {text_only_result[:200]}...")
                print("-" * 100)
        
        print(f"\n\nâœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    checker = SimpleRetrievalChecker()
    checker.run_test(num_samples=5)
