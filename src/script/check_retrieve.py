"""æ£€ç´¢æµ‹è¯•è„šæœ¬ï¼šéªŒè¯embeddingç´¢å¼•çš„æ£€ç´¢æ•ˆæœ

æµç¨‹ï¼š
1. ä»chunks.parquetéšæœºè¯»å–5ä¸ªchunk
2. ä»documents_cleaned.parquetè·å–è¿™äº›chunkçš„åŸå§‹æ–‡æœ¬
3. ä½¿ç”¨Qwen3 embeddingè¿›è¡Œå‘é‡åŒ–
4. æœç´¢FAISSç´¢å¼•è·å–top5ç›¸ä¼¼chunks
5. å¯¹æ¯”ç»“æœï¼ŒéªŒè¯æ£€ç´¢å‡†ç¡®æ€§
"""
import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import faiss

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src'))

try:
    from retrieval.embedding_qwen import Qwen3EmbeddingModel
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥Qwen3EmbeddingModelï¼Œè¯·ç¡®ä¿æ¨¡å—æ­£ç¡®å®‰è£…")
    exit(1)


class RetrievalChecker:
    """æ£€ç´¢æ£€éªŒå™¨"""
    
    def __init__(self):
        self.chunks_path = project_root / 'data' / 'processed' / 'chunks.parquet'
        self.docs_path = project_root / 'data' / 'processed' / 'documents_cleaned.parquet'
        self.index_path = project_root / 'data' / 'faiss' / 'qwen3_fp16_ip.faiss'
        self.chunk_ids_path = project_root / 'data' / 'faiss' / 'qwen3_fp16_ip_chunk_ids.json'
        
        self.df_chunks = None
        self.df_docs = None
        self.index = None
        self.chunk_ids = None
        self.embedding_model = None
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®"""
        print("\n[1] åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            # åŠ è½½chunks
            print("  ğŸ“¥ åŠ è½½ chunks.parquet...", end='', flush=True)
            self.df_chunks = pd.read_parquet(self.chunks_path)
            print(f" âœ… ({len(self.df_chunks):,}æ¡)")
            
            # åŠ è½½æ–‡æ¡£
            print("  ğŸ“¥ åŠ è½½ documents_cleaned.parquet...", end='', flush=True)
            self.df_docs = pd.read_parquet(self.docs_path)
            print(f" âœ… ({len(self.df_docs):,}æ¡)")
            
            # æ£€æŸ¥FAISSç´¢å¼•
            print("  ğŸ“¥ åŠ è½½ FAISSç´¢å¼•...", end='', flush=True)
            
            # å¦‚æœ.faissä¸å­˜åœ¨ä½†.lz4å­˜åœ¨ï¼Œå…ˆè§£å‹
            if not self.index_path.exists() and Path(str(self.index_path) + '.lz4').exists():
                print("\n     æ­£åœ¨è§£å‹LZ4æ–‡ä»¶...", end='', flush=True)
                import lz4.frame
                lz4_path = str(self.index_path) + '.lz4'
                with lz4.frame.open(lz4_path, 'rb') as f_in:
                    data = f_in.read()
                with open(self.index_path, 'wb') as f_out:
                    f_out.write(data)
                print(" å®Œæˆ")
                print("  ğŸ“¥ åŠ è½½ FAISSç´¢å¼•...", end='', flush=True)
            
            self.index = faiss.read_index(str(self.index_path))
            print(f" âœ… (å‘é‡æ•°: {self.index.ntotal:,})")
            
            # åŠ è½½chunk_idæ˜ å°„
            print("  ğŸ“¥ åŠ è½½ chunk_idæ˜ å°„...", end='', flush=True)
            with open(self.chunk_ids_path, 'r', encoding='utf-8') as f:
                self.chunk_ids = json.load(f)
            print(f" âœ… ({len(self.chunk_ids):,}æ¡)")
            
        except Exception as e:
            print(f"\n  âŒ åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        return True
    
    def init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        print("\n[2] åˆå§‹åŒ–Qwen3 Embeddingæ¨¡å‹...")
        try:
            self.embedding_model = Qwen3EmbeddingModel(
                model_id="Alibaba-NLP/gte-Qwen2-1.5B-instruct",
                device="cuda",
                dtype="float16"
            )
            print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_chunk_text(self, chunk_id: str) -> str:
        """æ ¹æ®chunk_idè·å–chunkçš„å®Œæ•´æ–‡æœ¬
        
        Args:
            chunk_id: chunkæ ‡è¯†ï¼Œæ ¼å¼ "file:row:N:chunk:M"
        
        Returns:
            chunkçš„æ–‡æœ¬å†…å®¹
        """
        try:
            # æ‰¾åˆ°è¯¥chunkåœ¨chunks.parquetä¸­çš„è¡Œ
            chunk_row = self.df_chunks[self.df_chunks['chunk_id'] == chunk_id].iloc[0]
            
            # è·å–doc_id
            doc_id = chunk_row['doc_id']
            
            # ä»documents_cleanedæ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£
            doc_row = self.df_docs[self.df_docs['doc_id'] == doc_id]
            if len(doc_row) == 0:
                return f"[æœªæ‰¾åˆ°doc_id: {doc_id}]"
            
            doc_text = doc_row.iloc[0]['text']
            title = chunk_row['title']
            
            # æå–chunkå¯¹åº”çš„æ–‡æœ¬
            child_start = chunk_row['child_start']
            child_end = chunk_row['child_end']
            
            chunk_text = doc_text[child_start:child_end]
            
            return f"[{title}]\n{chunk_text}"
        
        except Exception as e:
            return f"[é”™è¯¯: {e}]"
    
    def search_similar(self, query_text: str, top_k: int = 5) -> List[Tuple[str, float, int]]:
        """æœç´¢ç›¸ä¼¼chunks
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›top_kä¸ªç»“æœ
        
        Returns:
            [(chunk_id, distance, faiss_idx), ...] çš„åˆ—è¡¨
        """
        try:
            # EmbeddingæŸ¥è¯¢æ–‡æœ¬
            query_embedding = self.embedding_model.encode([query_text], batch_size=1)[0]
            query_embedding = query_embedding.astype(np.float32).reshape(1, -1)
            
            # æœç´¢
            distances, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                chunk_id = self.chunk_ids[int(idx)]
                results.append((chunk_id, float(dist), int(idx)))
            
            return results
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def run_test(self, num_samples: int = 5):
        """è¿è¡Œæ£€ç´¢æµ‹è¯•
        
        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°
        """
        print("\n" + "=" * 100)
        print("æ£€ç´¢æµ‹è¯•")
        print("=" * 100)
        
        # åŠ è½½æ•°æ®
        if not self.load_data():
            return
        
        # åˆå§‹åŒ–æ¨¡å‹
        if not self.init_embedding_model():
            return
        
        print(f"\n[3] éšæœºé‡‡æ · {num_samples} ä¸ªchunks è¿›è¡Œæ£€ç´¢æµ‹è¯•...\n")
        
        # éšæœºé‡‡æ ·chunk
        sample_indices = random.sample(range(len(self.df_chunks)), num_samples)
        
        for test_idx, chunk_idx in enumerate(sample_indices, 1):
            chunk_row = self.df_chunks.iloc[chunk_idx]
            chunk_id = chunk_row['chunk_id']
            
            print(f"\n{'='*100}")
            print(f"æµ‹è¯• {test_idx}/{num_samples}: {chunk_id}")
            print(f"{'='*100}")
            
            # è·å–è¯¥chunkçš„æ–‡æœ¬
            chunk_text = self.get_chunk_text(chunk_id)
            print(f"\nğŸ“„ æŸ¥è¯¢Chunkå†…å®¹:")
            print("-" * 100)
            print(chunk_text[:500] + ("..." if len(chunk_text) > 500 else ""))
            print("-" * 100)
            
            # æœç´¢ç›¸ä¼¼chunks
            print(f"\nğŸ” æ£€ç´¢Top5ç›¸ä¼¼chunks...\n")
            results = self.search_similar(chunk_text.split('\n', 1)[1] if '\n' in chunk_text else chunk_text, top_k=5)
            
            # æ˜¾ç¤ºç»“æœ
            for rank, (result_chunk_id, distance, faiss_idx) in enumerate(results, 1):
                result_text = self.get_chunk_text(result_chunk_id)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªå·±
                is_self = "âœ… [æœ¬èº«]" if result_chunk_id == chunk_id else ""
                
                # åˆ¤æ–­æ˜¯å¦åŒdoc
                query_doc_id = chunk_row['doc_id']
                result_doc_id = self.df_chunks[self.df_chunks['chunk_id'] == result_chunk_id].iloc[0]['doc_id']
                is_same_doc = "âœ… [åŒæ–‡æ¡£]" if result_doc_id == query_doc_id else "âš ï¸  [ä¸åŒæ–‡æ¡£]"
                
                print(f"\nã€Top {rank}ã€‘è·ç¦»: {distance:.4f} {is_self} {is_same_doc}")
                print(f"ID: {result_chunk_id}")
                print(f"å†…å®¹: {result_text[:300]}...")
                print("-" * 100)
        
        print(f"\n\nâœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    checker = RetrievalChecker()
    checker.run_test(num_samples=5)
