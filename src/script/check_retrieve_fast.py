"""å¿«é€Ÿæ£€ç´¢æµ‹è¯•è„šæœ¬ - ä½¿ç”¨HFé•œåƒåŠ é€Ÿæ¨¡å‹ä¸‹è½½

å¦‚æœè¿˜æ˜¯å¡ä½ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š
1. æ‰‹åŠ¨é¢„ä¸‹è½½æ¨¡å‹
2. ä½¿ç”¨ä»£ç†æˆ–é•œåƒ
3. ç›´æ¥ä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹
"""
import sys
import os
import random
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src'))

# è®¾ç½®HuggingFaceé•œåƒï¼ˆåŠ é€Ÿä¸‹è½½ï¼‰
# å°è¯•å¤šä¸ªé•œåƒæº
MIRROR_SOURCES = [
    "https://huggingface-mirror.com",  # å›½å†…é•œåƒ1
    "https://hf-mirror.com",           # å›½å†…é•œåƒ2
    "https://huggingface.co",          # å®˜æ–¹æº
]

# å°è¯•è®¾ç½®é•œåƒ
for mirror in MIRROR_SOURCES[:2]:
    os.environ['HF_ENDPOINT'] = mirror
    print(f"[mirror] å°è¯•ä½¿ç”¨é•œåƒ: {mirror}")
    break

try:
    from retrieval.embedding_qwen import Qwen3EmbeddingModel
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥Qwen3EmbeddingModel")
    exit(1)


class FastRetrievalChecker:
    """å¿«é€Ÿæ£€ç´¢æ£€éªŒå™¨"""
    
    def __init__(self):
        self.chunks_path = project_root / 'data' / 'processed' / 'chunks.parquet'
        self.docs_path = project_root / 'data' / 'processed' / 'documents_cleaned.parquet'
        
        self.df_chunks = None
        self.df_docs = None
        self.embedding_model = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\n[1] åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            print("  ğŸ“¥ åŠ è½½ chunks.parquet...", end='', flush=True)
            self.df_chunks = pd.read_parquet(self.chunks_path)
            print(f" âœ… ({len(self.df_chunks):,}æ¡)")
            
            print("  ğŸ“¥ åŠ è½½ documents_cleaned.parquet...", end='', flush=True)
            self.df_docs = pd.read_parquet(self.docs_path)
            print(f" âœ… ({len(self.df_docs):,}æ¡)")
            
        except Exception as e:
            print(f"\n  âŒ åŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        print("\n[2] åˆå§‹åŒ–Qwen3 Embeddingæ¨¡å‹...")
        print("   â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ¥ä¸‹è½½æ¨¡å‹ï¼ˆçº¦3GBï¼‰...")
        
        try:
            self.embedding_model = Qwen3EmbeddingModel(
                model_id="Alibaba-NLP/gte-Qwen2-1.5B-instruct",
                device="cuda",
                dtype="float16"
            )
            print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"\n  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {type(e).__name__}: {e}")
            print(f"\n  ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            print(f"     æ–¹æ¡ˆA - æ›´æ¢é•œåƒæºï¼ˆå¿«é€Ÿï¼‰ï¼š")
            print(f"       export HF_ENDPOINT=https://hf-mirror.com")
            print(f"       python src/script/check_retrieve_fast.py")
            print(f"")
            print(f"     æ–¹æ¡ˆB - æ‰‹åŠ¨é¢„ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰ï¼š")
            print(f"       huggingface-cli download Alibaba-NLP/gte-Qwen2-1.5B-instruct --resume-download")
            print(f"")
            print(f"     æ–¹æ¡ˆC - æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼š")
            cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'
            print(f"       ç¼“å­˜ç›®å½•: {cache_dir}")
            if cache_dir.exists():
                print(f"       å·²æœ‰æ¨¡å‹: {list(cache_dir.glob('models--*'))}")
            return False
    
    def get_chunk_text(self, chunk_id: str) -> str:
        """è·å–chunkæ–‡æœ¬"""
        try:
            chunk_row = self.df_chunks[self.df_chunks['chunk_id'] == chunk_id].iloc[0]
            doc_id = chunk_row['doc_id']
            doc_row = self.df_docs[self.df_docs['doc_id'] == doc_id]
            
            if len(doc_row) == 0:
                return f"[æœªæ‰¾åˆ°doc_id: {doc_id}]"
            
            doc_text = doc_row.iloc[0]['text']
            title = chunk_row['title']
            child_start = chunk_row['child_start']
            child_end = chunk_row['child_end']
            chunk_text = doc_text[child_start:child_end]
            
            return f"[{title}]\n{chunk_text}"
        
        except Exception as e:
            return f"[é”™è¯¯: {e}]"
    
    def get_doc_chunks(self, doc_id: str) -> List[str]:
        """è·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunk_id"""
        return self.df_chunks[self.df_chunks['doc_id'] == doc_id]['chunk_id'].tolist()
    
    def search_similar_in_doc(self, query_embedding: np.ndarray, chunk_ids: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
        """åœ¨ç»™å®šçš„chunkä¸­æœç´¢ç›¸ä¼¼çš„"""
        try:
            # å¯¹æ‰€æœ‰chunkè¿›è¡Œembedding
            chunk_texts = [self.get_chunk_text(cid).split('\n', 1)[1] if '\n' in self.get_chunk_text(cid) else self.get_chunk_text(cid) 
                          for cid in chunk_ids]
            chunk_embeddings = self.embedding_model.encode(chunk_texts, batch_size=32)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding.reshape(1, -1), chunk_embeddings)[0]
            
            # æ’åº
            sorted_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in sorted_indices:
                results.append((chunk_ids[idx], float(similarities[idx])))
            
            return results
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def run_test(self, num_samples: int = 5):
        """è¿è¡Œæµ‹è¯•"""
        print("\n" + "=" * 100)
        print("å¿«é€Ÿæ£€ç´¢æµ‹è¯• - åŒæ–‡æ¡£å†…æ£€ç´¢")
        print("=" * 100)
        
        if not self.load_data():
            return
        
        if not self.init_embedding_model():
            return
        
        print(f"\n[3] éšæœºé‡‡æ · {num_samples} ä¸ªchunks è¿›è¡Œæ£€ç´¢æµ‹è¯•...\n")
        
        # éšæœºé‡‡æ ·
        sample_indices = random.sample(range(len(self.df_chunks)), num_samples)
        
        for test_idx, chunk_idx in enumerate(sample_indices, 1):
            chunk_row = self.df_chunks.iloc[chunk_idx]
            chunk_id = chunk_row['chunk_id']
            doc_id = chunk_row['doc_id']
            
            print(f"\n{'='*100}")
            print(f"æµ‹è¯• {test_idx}/{num_samples}: {chunk_id}")
            print(f"{'='*100}")
            
            # è·å–chunkæ–‡æœ¬
            chunk_text = self.get_chunk_text(chunk_id)
            text_only = chunk_text.split('\n', 1)[1] if '\n' in chunk_text else chunk_text
            
            print(f"\nğŸ“„ æŸ¥è¯¢Chunk:")
            print("-" * 100)
            print(text_only[:300] + ("..." if len(text_only) > 300 else ""))
            print("-" * 100)
            
            # Embeddingè¯¥chunk
            print(f"\nğŸ”„ å¯¹æŸ¥è¯¢chunkè¿›è¡Œembedding...", end='', flush=True)
            query_embedding = self.embedding_model.encode([text_only], batch_size=1)[0]
            print(" âœ…")
            
            # è·å–åŒæ–‡æ¡£çš„å…¶ä»–chunks
            all_doc_chunks = self.get_doc_chunks(doc_id)
            print(f"\nğŸ“Š è¯¥æ–‡æ¡£å…±æœ‰ {len(all_doc_chunks)} ä¸ªchunks")
            
            # æœç´¢
            print(f"\nğŸ” åœ¨åŒæ–‡æ¡£å†…æœç´¢Top5ç›¸ä¼¼chunks...\n")
            results = self.search_similar_in_doc(query_embedding, all_doc_chunks, top_k=5)
            
            # æ˜¾ç¤ºç»“æœ
            for rank, (result_chunk_id, similarity) in enumerate(results, 1):
                is_self = "âœ… [æœ¬èº«]" if result_chunk_id == chunk_id else ""
                result_text = self.get_chunk_text(result_chunk_id)
                text_only_result = result_text.split('\n', 1)[1] if '\n' in result_text else result_text
                
                print(f"\nã€Top {rank}ã€‘ç›¸ä¼¼åº¦: {similarity:.4f} {is_self}")
                print(f"ID: {result_chunk_id}")
                print(f"å†…å®¹: {text_only_result[:200]}...")
                print("-" * 100)
        
        print(f"\n\nâœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    print("\n" + "=" * 100)
    print("HuggingFace æ£€ç´¢æµ‹è¯•å·¥å…·")
    print("=" * 100)
    print(f"\nğŸ”— å½“å‰ä½¿ç”¨é•œåƒ: {os.environ.get('HF_ENDPOINT', 'https://huggingface.co')}")
    print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {Path.home() / '.cache' / 'huggingface'}")
    
    checker = FastRetrievalChecker()
    checker.run_test(num_samples=5)
